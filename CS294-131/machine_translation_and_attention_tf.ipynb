{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation and Attention\n",
    "In this notebook, we will implement a model for neural machine translation (NMT) with attention. This notebook is adapted from the [TensorFlow tutorial on NMT](https://www.tensorflow.org/tutorials/seq2seq) at  as well as the [TensorFlow NMT package](https://github.com/tensorflow/nmt/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import collections\n",
    "from functools import partial\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper TensorFlow functions\n",
    "from utils import maybe_download\n",
    "\n",
    "# The encoder-decoder architecture\n",
    "from nmt.model import AttentionalModel, LSTMCell\n",
    "from nmt.utils import vocab_utils\n",
    "from nmt.train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We'll train our model on a small-scale dataset: an English-Vietnamese parallel corpus of TED talks (133K sentence pairs) provided by the IWSLT Evaluation Campaign (https://sites.google.com/site/iwsltevaluation2015/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified datasets/nmt_data_vi/train.en\n",
      "Found and verified datasets/nmt_data_vi/train.vi\n",
      "Found and verified datasets/nmt_data_vi/tst2012.en\n",
      "Found and verified datasets/nmt_data_vi/tst2012.vi\n",
      "Found and verified datasets/nmt_data_vi/tst2013.en\n",
      "Found and verified datasets/nmt_data_vi/tst2013.vi\n",
      "Found and verified datasets/nmt_data_vi/vocab.en\n",
      "Found and verified datasets/nmt_data_vi/vocab.vi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'vocab.vi'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "site_prefix = \"https://nlp.stanford.edu/projects/nmt/data/\"\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/train.en', out_dir, 13603614)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/train.vi', out_dir, 18074646)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2012.en', out_dir, 140250)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2012.vi', out_dir, 188396)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2013.en', out_dir, 132264)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/tst2013.vi', out_dir, 183855)\n",
    "\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/vocab.en', out_dir, 139741)\n",
    "maybe_download(site_prefix + 'iwslt15.en-vi/vocab.vi', out_dir, 46767)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to NMT\n",
    "\n",
    "<figure>\n",
    "    <img src='images/encdec.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 1.** Example of a general, *encoder-decoder* approach to NMT. An encoder converts a source sentence into a representation which is passed through a decoder to produce a translation</figcaption>\n",
    "</figure>\n",
    "\n",
    "A neural machine translation (NMT) system reads in a source sentence using an *encoder*, and then uses a *decoder* to emit a translation. NMT models vary in terms of their exact architectures. A natural choice for sequential data is the recurrent neural network (RNN). Usually an RNN is used for both the encoder and decoder. The RNN models, however, differ in terms of: (a) directionality – unidirectional or bidirectional (whether they read the source sentence in forwards or forwards and backwards); (b) depth – single- or multi-layer; and (c) type – often either a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit (GRU).\n",
    "\n",
    "We will consider a deep multi-layer RNN which is bi-directional (it reads the input sequence both forwards and backwards) and uses LSTM units with attention. At a high level, the NMT model consists of two recurrent neural networks: the encoder recurrent network simply consumes the input source words without making any prediction; the decoder, on the other hand, processes the target sentence while predicting the next words.\n",
    "\n",
    "<figure>\n",
    "    <img src='images/seq2seq.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 2.** Example of a neural machine translation system for translating a source sentence \"I am a student\" into a target sentence \"Je suis étudiant\".  Here, $<s>$ marks the start of the decoding process while $</s>$ tells the decoder to stop.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "At the bottom layer, the encoder and decoder recurrent networks receive as input the following: first, the source sentence, then a boundary marker $</s>$ which indicates the transition from the encoding to the decoding mode, and the target sentence. We now go into the details of how the model deals with source and target sentences.\n",
    "\n",
    "### Embedding\n",
    "Given the categorical nature of words, the model must first look up the source and target embeddings to retrieve the corresponding word representations. For this embedding layer to work, a vocabulary is first chosen for each language. Usually, a vocabulary size $V$ is selected, and only the most frequent $V$ words in the corpus are treated as unique. All other words are converted to an \"unknown\" token $<$UNK$>$ and all get the same embedding. The embedding weights, one set per language, are usually learned during training (but pretrained word embeddings may be used instead).\n",
    "\n",
    "### Encoder\n",
    "Once retrieved, the word embeddings are then fed as input into the main network, which consists of two multi-layer recurrent neural networks -- an encoder for the source language and a decoder for the target language. These two networks, in principle, can share the same weights; however, in practice, we often use two different sets of parameters (such models do a better job when fitting large training datasets). The encoder uses zero vectors as its starting states (before it sees the source sequence). In TensorFlow:\n",
    "\n",
    "    # Build RNN cell\n",
    "    encoder_cell = YourEncoderRNNCell(num_units)\n",
    "\n",
    "    # Run Dynamic RNN\n",
    "    #   encoder_outputs: [max_time, batch_size, num_units]\n",
    "    #   encoder_state: [batch_size, num_units]\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "        encoder_cell, encoder_emb_inp,\n",
    "        sequence_length=source_sequence_length, time_major=True)\n",
    "\n",
    "### Decoder\n",
    "The decoder also needs to have access to the source information, and one simple way to achieve that is to initialize it with the last hidden state of the encoder, `encoder_state`. In Figure 2, we pass the hidden state at the source word \"student\" to the decoder side.\n",
    "\n",
    "    # Build RNN cell\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    \n",
    "    # Helper\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        decoder_emb_inp, decoder_lengths, time_major=True)\n",
    "\n",
    "    # Decoder\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, encoder_state, output_layer=projection_layer)\n",
    "    \n",
    "    # Dynamic decoding\n",
    "    outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)\n",
    "    logits = outputs.rnn_output\n",
    "\n",
    "### Loss\n",
    "Given the logits above, we are now ready to compute the training loss:\n",
    "\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=decoder_outputs, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) / batch_size)\n",
    "\n",
    "Here, target_weights is a zero-one matrix of the same size as decoder_outputs. It masks padding positions outside of the target sequence lengths with values 0.\n",
    "\n",
    "Important note: It's worth pointing out that we should divide the loss by `batch_size`, so our hyperparameters are \"invariant\" to `batch_size`. Some people divide the loss by (`batch_size * num_time_steps`), which plays down the errors made on short sentences. More subtly, the same hyperparameters (applied to the former way) can't be used for the latter way. For example, if both approaches use SGD with a learning of `1.0`, the latter approach effectively uses a much smaller learning rate of `1 / num_time_steps`.\n",
    "\n",
    "### How to generate translations at test time\n",
    "\n",
    "While you're training your NMT models (and once you have trained models), you can obtain translations given previously unseen source sentences. At test time, we only have access to the source sentence; i.e., `encoder_inputs`. There are many ways to perform decoding given those inputs. Decoding methods include greedy, sampling, and beam-search decoding. Here, we will discuss the greedy decoding strategy.\n",
    "\n",
    "The idea is simple and illustrated in Figure 3:\n",
    "\n",
    "1. We still encode the source sentence in the same way as during training to obtain an `encoder_state`, and this `encoder_state` is used to initialize the decoder.\n",
    "\n",
    "2. The decoding (translation) process is started as soon as the decoder receives a starting symbol $<$/s$>$.\n",
    "\n",
    "3. For each timestep on the decoder side, we treat the recurrent network's output as a set of logits. We choose the most likely word, the id associated with the maximum logit value, as the emitted word (this is the \"greedy\" behavior). For example in Figure 3, the word \"moi\" has the highest translation probability in the first decoding step. We then feed this word as input to the next timestep. (At training time, however, we may feed in the true target as input to the next timestep in a process called *teacher forcing*.)\n",
    "\n",
    "4. The process continues until the end-of-sentence marker $<$/s$>$ is produced as an output symbol.\n",
    "\n",
    "<figure>\n",
    "    <img src='images/greedy_dec.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 3.** Example of how a trained NMT model produces a translation for a source sentence \"Je suis étudiant\" using greedy search.\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Attention\n",
    "\n",
    "The attention mechanism was first introduced by Bahdanau et al., 2015 [1] and then later refined by Luong et al., 2015 [2] and others. The key idea of the attention mechanism is to establish direct short-cut connections between the target and the source by paying \"attention\" to relevant source content as we translate (produce output tokens). A nice byproduct of the attention mechanism is an easy-to-visualize alignment matrix between the source and target sentences that we will visualize at the end of this notebook.\n",
    " \n",
    "Remember that in a vanilla seq2seq model, we pass the last source state $h_{s_{T_s}}$ from the encoder to the decoder when starting the decoding process. This works well for short and medium-length sentences; however, for long sentences, the single fixed-size hidden state becomes an information bottleneck. Instead of discarding all of the hidden states computed in the source RNN, the attention mechanism provides an approach that allows the decoder to peek at them (treating them as a dynamic memory of the source information). By doing so, the attention mechanism improves the translation of longer sentences. Nowadays, attention mechanisms are the *de facto* standard and have been successfully applied to many other tasks (including image caption generation, speech recognition, and text summarization).\n",
    "\n",
    "<figure>\n",
    "    <img src='images/att.jpg' alt='missing' />\n",
    "    <figcaption>**Figure 4.** Example of an attention-based NMT system with the first step of the attention computation in detail. For clarity, the embedding and projection layers are omitted.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "### How do we actually attend over the input sequence?\n",
    "\n",
    "There are many different ways of formalizing attention. These variants depend on the form of a *scoring* function and an *attention* function (and on whether the previous state of the decoder $h_{t_{i-1}}$ is used instead of $h_{t_{i}}$ in the scoring function as originally suggested in Bahdanau et al. (2015); **we will stick to using $h_{t_{i}}$** in this notebook). Luong et al. (2015) demonstrate that only a few choices actually matter:\n",
    "\n",
    "1. First, the basic form of attention, i.e., **direct connections between target and source**, needs to be present. \n",
    "\n",
    "2. Second, it's important to **feed the attention vector to the next timestep** to inform the network about past attention decisions.\n",
    "\n",
    "3. Lastly, **choices of the scoring function** can often result in different performance. See Luong et al. (2015) for further details.\n",
    "\n",
    "### A general framework for computing attention\n",
    "\n",
    "The attention computation happens at every decoder time step. It consists of the following stages:\n",
    "\n",
    "1. The current target (encoder) hidden state $h_{t_i}$ is compared with all source (decoder) states $h_{s_j}$ to derive *attention weights* $\\alpha_{ij}$.\n",
    "2. Based on the attention weights we compute a *context vector* $c_{i}$ as the weighted average of the source states.\n",
    "3. We combine the context vector $c_{i}$ with the current target hidden state $h_{s_j}$ to yield the final *attention vector* $a_t$.\n",
    "4. The attention vector $a_i$ is fed as an input to the next time step (*input feeding*). \n",
    "\n",
    "The first three steps can be summarized by the equations below:\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\alpha_{ij} &= \\frac{\n",
    "    \\exp(\\text{score}(h_{t_i}, h_{s_j}))\n",
    "}{\n",
    "    \\sum_{k=1}^{T_s}{\\exp(\\text{score}(h_{t_i}, h_{s_k}))}\n",
    "} \\tag{attention weights} \\\\\\\\\n",
    "c_{i} &= \\sum_{j=1}^{T_s} \\alpha_{ij} h_{s_j} \\tag{context vector} \\\\\\\\\n",
    "a_{i} &= f(c_{i}, h_{t_i}) \\tag{attention vector} \\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Here, the function `score` is used to compare the target hidden state $h_{t_i}$ with each of the source hidden states $h_{s_j}$, and the result is normalized over the source timesteps $j = 1, \\dots, T_s$ to produce attention weights $\\alpha_{ij}$ (which define a distribution over source positions $j$ for a given source timestep $i$). (There are various choices of the scoring function; we will consider three below.) Note that we make use of the current decoder (or *target*) hidden state $h_{t_i}$, which is computed as a function of the previous hidden state $h_{t_{i-1}}$, the embedding of the input token $x_{i}$ (which is either the emission or the ground truth token from the previous timestep) using the standard formula for a recurrent cell. Optionally, in the case of *input feeding*, we combine $h_{t_{i-1}}$ with the context vector from the previous timestep, $c_{t_{i-1}}$ (which may require a change in the size of the kernel matrix, depending on how the combination is implemented). The encoder (or *source*) hidden states $h_{s_j}$ for $j=1, \\dots T_s$ are similarly the standard hidden state for a recurrent cell.\n",
    "\n",
    "We can also vectorize the computation of the context vector $c_i$ for every target timestep as follows: Given the source hidden states $h_{s_1}, \\dots, h_{s_{T_s}}$, we construct a matrix $H_s$ of size `hidden_size` $\\times$ `input_seq_len` by stacking the source hidden states into columns. Attention allows us to dynamically weight certain timesteps of the input sequence in a fixed size vector $c_i$ by taking a convex combination of the columns of $H_s$. In particular, we calculate a nonzero and normalized attention weight vector $\\vec{\\alpha}_i = [\\alpha_{i1}, \\dots, \\alpha_{iT_s}]^T$ that weights the source hidden states in the computation\n",
    "\n",
    "$$\\large c_i = H_s\\vec{\\alpha}_i~.$$\n",
    "\n",
    "\n",
    "\n",
    "The attention vector $a_i$ is used to derive the softmax logits and thereafter the loss by transformation under a function $f$.The function $f$ is commonly the a concatenation followed by $\\tanh$ layer:\n",
    "\n",
    "$$\\large a_{i} = \\tanh(W_a[c_i; h_{t_i}])$$\n",
    "\n",
    "but could take other forms. We then compute the predictive distribution over output tokens as\n",
    "\n",
    "$$\\large p(y_i \\mid y_1, \\dots y_{i-1}, x_i) = \\text{softmax}(W_s a_{i})~.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. LSTM cell with attention (8 pts)\n",
    "\n",
    "In the block below, you will implement the method `call`, which computes a single step of an LSTM cell using a method `attention` that computes an attention vector with some score function, as described above. **Complete the skeleton below**; assume inputs is already the input embedding (i.e., there is no need to construct an embedding matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithAttention(LSTMCell):\n",
    "    \n",
    "    def __init__(self, num_units, memory):\n",
    "        super(LSTMCellWithAttention, self).__init__(num_units)\n",
    "        self.memory = memory\n",
    "        \n",
    "    def attention(self):\n",
    "        raise NotImplementedError(\"The subclass must implement this method!\")\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Run this LSTM cell with attention on inputs, conditional on state.\"\"\"\n",
    "        \n",
    "        # Cell and hidden states of the LSTM\n",
    "        c, h = state\n",
    "        \n",
    "        # Source (encoder) states to attend over\n",
    "        source_states = self.memory\n",
    "        \n",
    "        # Cell activation (e.g., tanh, relu, etc.)\n",
    "        activation = self._activation\n",
    "        \n",
    "        # LSTM cell parameters\n",
    "        kernel = self._kernel\n",
    "        bias = self._bias\n",
    "        forget_bias = self._forget_bias\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # shapes of tensors\n",
    "        # input [batch, state_size] or [batch, num_units] \n",
    "        # source_states [batch, input_length, state_size]\n",
    "        # c [batch, state_size]\n",
    "        # h [batch, state_size]\n",
    "        \n",
    "        lstm_matrix = tf.matmul(tf.concat([inputs, h], 1), kernel) # [batch, 4*state_size]\n",
    "        lstm_matrix = tf.add(lstm_matrix, bias)\n",
    "        i, g, f, o = tf.split(lstm_matrix, 4, 1) # each size [batch, state_size]\n",
    "        new_c = tf.sigmoid(f + forget_bias) * c + tf.sigmoid(i) * activation(g)\n",
    "        new_h = tf.sigmoid(o) * activation(new_c)\n",
    "        \n",
    "        attention_vector = self.attention(new_h, source_states) # what's target_state here?      \n",
    "        ### END YOUR CODE\n",
    "        ### Your code should compute attention vector, new_c and new_h\n",
    "\n",
    "        # Adhering to convention\n",
    "        new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "    \n",
    "        return attention_vector, new_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a \"dummy\" version of attention in order to test that the LSTM cell step function is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithDummyAttention(LSTMCellWithAttention):\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Just return the target state so that the update becomes the vanilla\n",
    "        LSTM update.\"\"\"\n",
    "        return target_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2A. Dot-product Attention (8 pts)\n",
    "\n",
    "We first consider the simplest version of attention, which simply calculates the similarity between $h_{t_i}$ and $h_{s_j}$ by computing their dot product:\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=h_{t_i}^\\mathrm{\\,T}\\, h_{s_j}~.\n",
    "\\end{align*}$$\n",
    "\n",
    "This computation has no additional parameters, but it limits the expressivity of the model since its forces the input and output encodings to be close in order to have high score.\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using dot-product attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn` or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithDotProductAttention(LSTMCellWithAttention):\n",
    "        \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithDotProductAttention, self).build(inputs_shape)\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        #raise NotImplementedError(\"Need to implement dot-product attention.\")\n",
    "        \n",
    "        # shapes of tensors\n",
    "        # source_states [batch, input_length, state_size]\n",
    "        # target_state [batch, state_size]\n",
    "\n",
    "        scores = tf.matmul(source_states, tf.expand_dims(target_state, -1)) # [batch, input_length, 1]\n",
    "        scores = scores - tf.reduce_max(scores, 1, keepdims=True)\n",
    "        scores_exp = tf.exp(scores)\n",
    "        scores = scores_exp/tf.reduce_sum(scores_exp, 1, keepdims=True)\n",
    "        c = tf.squeeze(tf.matmul(source_states, scores, transpose_a=True), -1) # [batch, state_size]\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "        \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2B. Bilinear Attention (8 pts)\n",
    "\n",
    "To make the score function more expressive, we may consider using a bilinear function of the form\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=h_{t_i}^\\mathrm{\\,T} W_\\text{att} h_{s_j}~,\n",
    "\\end{align*}$$\n",
    "\n",
    "which transforms the source encoding $h_{s_j}$ by a linear transformation parameterized by $W_\\text{att}$ before taking the dot product. This formulation adds additional parameters that must be learned, but increases expressivity and also allows the source and target encodings to be of different dimensionality (if we so wish).\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using bilinear attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn`or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithBilinearAttention(LSTMCellWithAttention):\n",
    "    \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithBilinearAttention, self).build(inputs_shape)\n",
    "        self._W_att = self.add_variable(\"W_att\", \n",
    "                                        shape=[self._num_units, \n",
    "                                               self._num_units])\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "\n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        # shapes of tensors\n",
    "        # source_states [batch, input_length, state_size]\n",
    "        # target_state [batch, state_size]\n",
    "        \n",
    "        batch_size = tf.shape(target_state)[0]\n",
    "        W_att_batch = tf.tile(tf.expand_dims(self._W_att, 0), [batch_size, 1, 1]) # [batch, state_size, state_size]\n",
    "        target_state_W_att = tf.matmul(W_att_batch,  tf.expand_dims(target_state, -1)) # [batch, state_size, 1]\n",
    "        scores = tf.matmul(source_states, target_state_W_att) # [batch, input_length, 1]\n",
    "        scores = scores - tf.reduce_max(scores, 1, keepdims=True)\n",
    "        scores_exp = tf.exp(scores)\n",
    "        scores = scores_exp/tf.reduce_sum(scores_exp, 1, keepdims=True)\n",
    "        c = tf.squeeze(tf.matmul(source_states, scores, transpose_a=True), -1) # [batch, state_size]\n",
    "        \n",
    "       \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "        \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2C. Feedforward Attention (8 pts)\n",
    "\n",
    "Instead of simply using a linear transformation, why don't we use an even more expressive feedforward neural network to compute the score?\n",
    "\n",
    "$$\\large\\begin{align*}\n",
    "\\text{score}(h_{t_i}, h_{s_j})&=W_{\\text{att}_2} \\tanh( W_{\\text{att}_1} [h_{t_i}; h_{s_j}])~,\n",
    "\\end{align*}$$\n",
    "\n",
    "where $[v_1; v_2]$ denotes a concatenation of the vectors $v_1$ and $v_2$, and $W_{\\text{att}_1}$ and $W_{\\text{att}_2}$ are learned parameter matrices. The feedforward approach typically has fewer parameters (depending on the size of the hidden layer) than the bilinear attention mechanism (which requires `source_embedding_dim` $\\times$ `target_embedding_dim` parameters).\n",
    "\n",
    "For this question, **implement the __call__ function of the following LSTM cell using feedforward attention.** Your code should be less than ten lines and *not* make use of any higher-level primitives from `tf.nn` or `tf.layers`, etc. (6 pts). As a further step, **vectorize the operation** so that you can compute $\\text{score}(\\cdot, h_{s_j})$ for every word in the source sentence in parallel (2 pts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellWithFeedForwardAttention(LSTMCellWithAttention):\n",
    "    \n",
    "    def build(self, inputs_shape):\n",
    "        super(LSTMCellWithFeedForwardAttention, self).build(inputs_shape)\n",
    "\n",
    "        self._W_att_1 = self.add_variable(\"W_att_1\", \n",
    "                                          shape=[self._num_units + self._num_units, \n",
    "                                                 self._num_units])\n",
    "        self._W_att_2 = self.add_variable(\"W_att_2\", \n",
    "                                          shape=[self._num_units, 1])\n",
    "        self._W_c = self.add_variable(\"W_c\", \n",
    "                                      shape=[self._num_units + self._num_units, \n",
    "                                             256])\n",
    "        \n",
    "    def attention(self, target_state, source_states):\n",
    "        \"\"\"Return the attention vector computed from attending over\n",
    "        source_states using a function of target_state and source_states.\"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        # shapes of tensors\n",
    "        # source_states [batch, input_length, state_size]\n",
    "        # target_state [batch, state_size]\n",
    "        # W_att_1 [2*state_size, state_size]\n",
    "        # W_att_2 [state_size, 1]\n",
    "        \n",
    "        input_length = tf.shape(source_states)[1]\n",
    "        batch_size = tf.shape(source_states)[0]\n",
    "        target_state_tile = tf.tile(tf.expand_dims(target_state, 1), [1, input_length, 1]) # [batch, input_length, state_size]\n",
    "        state_concat = tf.concat([source_states, target_state_tile], 2) # [batch, input_length, 2*state_size]\n",
    "        W_att_1_batch = tf.tile(tf.expand_dims(self._W_att_1, 0), [batch_size, 1, 1]) # [batch_size, 2*state_size, state_size])\n",
    "        W_att_2_batch = tf.tile(tf.expand_dims(self._W_att_2, 0), [batch_size, 1, 1]) # [batch_size, state_size, 1])\n",
    "        temp = tf.tanh(tf.matmul(state_concat, W_att_1_batch)) # [batch_size, input_length, state_size]\n",
    "        scores = tf.matmul(temp, W_att_2_batch) # [batch_size, input_length, 1]\n",
    "                                \n",
    "        scores = scores - tf.reduce_max(scores, 1, keepdims=True)\n",
    "        scores_exp = tf.exp(scores)\n",
    "        scores = scores_exp/tf.reduce_sum(scores_exp, 1, keepdims=True)\n",
    "        c = tf.squeeze(tf.matmul(source_states, scores, transpose_a=True), -1) # [batch, state_size]\n",
    "        \n",
    "                                \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        ### Your code should compute the context vector c\n",
    "        attention_vector = tf.tanh(tf.matmul(tf.concat([c, target_state], -1), self._W_c))\n",
    "        \n",
    "        return attention_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter settings\n",
    "\n",
    "You may find it useful to tune some of these parameters (but not necessarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_standard_hparams(data_path, out_dir):\n",
    "    \n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        \n",
    "        # Data\n",
    "        src=\"vi\",\n",
    "        tgt=\"en\",\n",
    "        train_prefix=os.path.join(data_path, \"train\"),\n",
    "        dev_prefix=os.path.join(data_path, \"tst2012\"),\n",
    "        test_prefix=os.path.join(data_path, \"tst2013\"),\n",
    "        vocab_prefix=\"\",\n",
    "        embed_prefix=\"\",\n",
    "        out_dir=out_dir,\n",
    "        src_vocab_file=os.path.join(data_path, \"vocab.vi\"),\n",
    "        tgt_vocab_file=os.path.join(data_path, \"vocab.en\"),\n",
    "        src_embed_file=\"\",\n",
    "        tgt_embed_file=\"\",\n",
    "        src_file=os.path.join(data_path, \"train.vi\"),\n",
    "        tgt_file=os.path.join(data_path, \"train.en\"),\n",
    "        dev_src_file=os.path.join(data_path, \"tst2012.vi\"),\n",
    "        dev_tgt_file=os.path.join(data_path, \"tst2012.en\"),\n",
    "        test_src_file=os.path.join(data_path, \"tst2013.vi\"),\n",
    "        test_tgt_file=os.path.join(data_path, \"tst2013.en\"),\n",
    "\n",
    "        # Networks\n",
    "        num_units=512,\n",
    "        num_layers=1,\n",
    "        num_encoder_layers=1,\n",
    "        num_decoder_layers=1,\n",
    "        num_encoder_residual_layers=0,\n",
    "        num_decoder_residual_layers=0,\n",
    "        dropout=0.2,\n",
    "        unit_type=\"lstm\",\n",
    "        encoder_type=\"uni\",\n",
    "        residual=False,\n",
    "        time_major=True,\n",
    "        num_embeddings_partitions=0,\n",
    "\n",
    "        # Train\n",
    "        optimizer=\"adam\",\n",
    "        batch_size=128,\n",
    "        init_op=\"uniform\",\n",
    "        init_weight=0.1,\n",
    "        max_gradient_norm=100.0,\n",
    "        learning_rate=0.001,\n",
    "        warmup_steps=0,\n",
    "        warmup_scheme=\"t2t\",\n",
    "        decay_scheme=\"luong234\",\n",
    "        colocate_gradients_with_ops=True,\n",
    "        num_train_steps=12000,\n",
    "\n",
    "        # Data constraints\n",
    "        num_buckets=5,\n",
    "        max_train=0,\n",
    "        src_max_len=25,\n",
    "        tgt_max_len=25,\n",
    "        src_max_len_infer=0,\n",
    "        tgt_max_len_infer=0,\n",
    "\n",
    "        # Data format\n",
    "        sos=\"<s>\",\n",
    "        eos=\"</s>\",\n",
    "        subword_option=\"\",\n",
    "        check_special_token=True,\n",
    "\n",
    "        # Misc\n",
    "        forget_bias=1.0,\n",
    "        num_gpus=1,\n",
    "        epoch_step=0,  # record where we were within an epoch.\n",
    "        steps_per_stats=100,\n",
    "        steps_per_external_eval=0,\n",
    "        share_vocab=False,\n",
    "        metrics=[\"bleu\"],\n",
    "        log_device_placement=False,\n",
    "        random_seed=None,\n",
    "        # only enable beam search during inference when beam_width > 0.\n",
    "        beam_width=0,\n",
    "        length_penalty_weight=0.0,\n",
    "        override_loaded_hparams=True,\n",
    "        num_keep_ckpts=5,\n",
    "        avg_ckpts=False,\n",
    "        num_intra_threads=0,\n",
    "        num_inter_threads=0,\n",
    "\n",
    "        # For inference\n",
    "        inference_indices=None,\n",
    "        infer_batch_size=32,\n",
    "        sampling_temperature=0.0,\n",
    "        num_translations_per_input=1,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    src_vocab_size, _ = vocab_utils.check_vocab(hparams.src_vocab_file, hparams.out_dir)\n",
    "    tgt_vocab_size, _ = vocab_utils.check_vocab(hparams.tgt_vocab_file, hparams.out_dir)\n",
    "    hparams.add_hparam('src_vocab_size', src_vocab_size)\n",
    "    hparams.add_hparam('tgt_vocab_size', tgt_vocab_size)\n",
    "    \n",
    "    out_dir = hparams.out_dir\n",
    "    if not tf.gfile.Exists(out_dir):\n",
    "        tf.gfile.MakeDirs(out_dir)\n",
    "         \n",
    "    for metric in hparams.metrics:\n",
    "        hparams.add_hparam(\"best_\" + metric, 0)  # larger is better\n",
    "        best_metric_dir = os.path.join(hparams.out_dir, \"best_\" + metric)\n",
    "        hparams.add_hparam(\"best_\" + metric + \"_dir\", best_metric_dir)\n",
    "        tf.gfile.MakeDirs(best_metric_dir)\n",
    "\n",
    "        if hparams.avg_ckpts:\n",
    "            hparams.add_hparam(\"avg_best_\" + metric, 0)  # larger is better\n",
    "            best_metric_dir = os.path.join(hparams.out_dir, \"avg_best_\" + metric)\n",
    "            hparams.add_hparam(\"avg_best_\" + metric + \"_dir\", best_metric_dir)\n",
    "            tf.gfile.MakeDirs(best_metric_dir)\n",
    "\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Training (8 pts)\n",
    "\n",
    "For this question, **train at least two of the models that use the attention modules you defined above**. Did you notice any difference in the training or evaluation of the different models? **Provide a brief written answer below.**\n",
    "\n",
    "*Note*: Make sure you **remove the model checkpoints** in the appropriate folders (`nmt_model_dotprod_att`, `nmt_model_binlinear_att` or `nmt_model_feedforward_att`)  if you would like to start training from scratch. (It's safe to delete all the files saved in the directory, or move them elsewhere.) Otherwise, the saved parameters will automatically be reloaded from the latest checkpoint and training will resume where it left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your written answer here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab file datasets/nmt_data_vi/vocab.vi exists\n",
      "# Vocab file datasets/nmt_data_vi/vocab.en exists\n",
      "# creating train graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0  DropoutWrapper, dropout=0.2   DropoutWrapper  DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=luong234, start_decay_step=8000, decay_steps 1000, decay_factor 0.5\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 17191), \n",
      "# creating eval graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTMCellWithDummyAttention, dropout=0   LSTMCellWithDummyAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 17191), \n",
      "# creating infer graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTMCellWithDummyAttention, dropout=0   LSTMCellWithDummyAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dummy_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 17191), \n",
      "  created train model with fresh parameters, time 0.52s\n",
      "  created infer model with fresh parameters, time 0.07s\n",
      "  # 1434\n",
      "    src: LEGO đã lấy khối gạch bê tông , những khối gạch xây nên thế giới , và làm nó thành những viên gạch của trí tưởng tượng .\n",
      "    ref: LEGO has essentially taken the concrete block , the building block of the world , and made it into the building block of our imagination .\n",
      "    nmt: marks clinical squares 2 leaf bureaucratic sons bureaucratic bodies politicians politicians politicians politicians politicians politicians politicians politicians Secrets Secrets Ray depict changes Justin pretzels schedule schedule assured assured assured descended descended descended descended descended extrinsic extrinsic tore tore tore ideological ideological elemental ideological ideological ideological ideological ideological blueprint ideological ideological blueprint broadcasting ideological broadcasting staying broadcasting\n",
      "  created eval model with fresh parameters, time 0.13s\n",
      "  eval dev: perplexity 17263.17, time 1s, Mon Apr  2 05:19:15 2018.\n",
      "  eval test: perplexity 17263.24, time 1s, Mon Apr  2 05:19:16 2018.\n",
      "  created infer model with fresh parameters, time 0.05s\n",
      "# Start step 0, lr 0.001, Mon Apr  2 05:19:16 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 0.001 step-time 0.36s wps 12.82K ppl 498.97 gN 13.67 bleu 0.00, Mon Apr  2 05:19:52 2018\n",
      "  step 200 lr 0.001 step-time 0.22s wps 20.80K ppl 195.94 gN 7.91 bleu 0.00, Mon Apr  2 05:20:14 2018\n",
      "  step 300 lr 0.001 step-time 0.22s wps 20.86K ppl 145.11 gN 6.80 bleu 0.00, Mon Apr  2 05:20:37 2018\n",
      "  step 400 lr 0.001 step-time 0.22s wps 20.80K ppl 121.26 gN 6.29 bleu 0.00, Mon Apr  2 05:20:59 2018\n",
      "  step 500 lr 0.001 step-time 0.22s wps 20.84K ppl 105.58 gN 6.25 bleu 0.00, Mon Apr  2 05:21:21 2018\n",
      "  step 600 lr 0.001 step-time 0.22s wps 20.66K ppl 93.15 gN 6.37 bleu 0.00, Mon Apr  2 05:21:43 2018\n",
      "  step 700 lr 0.001 step-time 0.22s wps 20.65K ppl 85.57 gN 6.07 bleu 0.00, Mon Apr  2 05:22:05 2018\n",
      "  step 800 lr 0.001 step-time 0.22s wps 20.55K ppl 78.52 gN 5.95 bleu 0.00, Mon Apr  2 05:22:28 2018\n",
      "  step 900 lr 0.001 step-time 0.22s wps 20.64K ppl 74.67 gN 6.20 bleu 0.00, Mon Apr  2 05:22:50 2018\n",
      "  step 1000 lr 0.001 step-time 0.22s wps 20.74K ppl 67.88 gN 6.08 bleu 0.00, Mon Apr  2 05:23:13 2018\n",
      "# Save eval, global step 1000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-1000, time 0.09s\n",
      "  # 20\n",
      "    src: Tất cả những người lớn đều biết các rủi ro .\n",
      "    ref: All the adults knew the risks .\n",
      "    nmt: All of these are the things that we are .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-1000, time 0.10s\n",
      "  eval dev: perplexity 63.33, time 1s, Mon Apr  2 05:23:15 2018.\n",
      "  eval test: perplexity 73.10, time 1s, Mon Apr  2 05:23:16 2018.\n",
      "# Finished an epoch, step 1043. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-1000, time 0.09s\n",
      "  # 1481\n",
      "    src: Mục đích của việc này là để chính quyền có thể biết được nguồn gốc của những tài liệu đó\n",
      "    ref: And this was done so the government could track where text was coming from .\n",
      "    nmt: The first thing that is that the way of the world is that the way of the world is not\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-1000, time 0.08s\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:23:35 2018.\n",
      "  bleu dev: 3.1\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 05:23:44 2018.\n",
      "  bleu test: 2.6\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 1100 lr 0.001 step-time 0.36s wps 12.57K ppl 61.16 gN 6.23 bleu 3.13, Mon Apr  2 05:24:11 2018\n",
      "  step 1200 lr 0.001 step-time 0.22s wps 20.46K ppl 56.16 gN 5.94 bleu 3.13, Mon Apr  2 05:24:34 2018\n",
      "  step 1300 lr 0.001 step-time 0.22s wps 20.64K ppl 54.51 gN 6.02 bleu 3.13, Mon Apr  2 05:24:56 2018\n",
      "  step 1400 lr 0.001 step-time 0.23s wps 20.64K ppl 53.03 gN 6.13 bleu 3.13, Mon Apr  2 05:25:19 2018\n",
      "  step 1500 lr 0.001 step-time 0.22s wps 20.65K ppl 49.71 gN 5.82 bleu 3.13, Mon Apr  2 05:25:41 2018\n",
      "  step 1600 lr 0.001 step-time 0.22s wps 20.67K ppl 48.51 gN 5.84 bleu 3.13, Mon Apr  2 05:26:03 2018\n",
      "  step 1700 lr 0.001 step-time 0.23s wps 20.64K ppl 46.80 gN 6.01 bleu 3.13, Mon Apr  2 05:26:26 2018\n",
      "  step 1800 lr 0.001 step-time 0.23s wps 20.63K ppl 45.67 gN 5.89 bleu 3.13, Mon Apr  2 05:26:48 2018\n",
      "  step 1900 lr 0.001 step-time 0.22s wps 20.48K ppl 43.40 gN 5.59 bleu 3.13, Mon Apr  2 05:27:10 2018\n",
      "  step 2000 lr 0.001 step-time 0.22s wps 20.53K ppl 42.58 gN 5.63 bleu 3.13, Mon Apr  2 05:27:33 2018\n",
      "# Save eval, global step 2000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-2000, time 0.09s\n",
      "  # 59\n",
      "    src: Tôi không biết các qui trình .\n",
      "    ref: I didn &apos;t know the protocols .\n",
      "    nmt: I don &apos;t know .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-2000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-2000, time 0.09s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval dev: perplexity 45.10, time 1s, Mon Apr  2 05:27:35 2018.\n",
      "  eval test: perplexity 50.96, time 1s, Mon Apr  2 05:27:36 2018.\n",
      "# Finished an epoch, step 2086. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-2000, time 0.09s\n",
      "  # 157\n",
      "    src: Bà nói : &quot; nào , hãy chắc chắn là con sẽ không làm thế chứ &quot; . Tôi nói &quot; chắc chắn ạ &quot;\n",
      "    ref: She said , &quot; Now you make sure you don &apos;t do that . &quot; I said , &quot; Sure . &quot;\n",
      "    nmt: And she said , &quot; I &apos;m not going to say , &quot; I &apos;m not going to be a <unk> . &quot;\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-2000, time 0.09s\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:28:05 2018.\n",
      "  bleu dev: 4.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 05:28:15 2018.\n",
      "  bleu test: 4.0\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 2100 lr 0.001 step-time 0.33s wps 13.38K ppl 39.78 gN 5.68 bleu 4.94, Mon Apr  2 05:28:30 2018\n",
      "  step 2200 lr 0.001 step-time 0.24s wps 19.21K ppl 34.03 gN 5.94 bleu 4.94, Mon Apr  2 05:28:54 2018\n",
      "  step 2300 lr 0.001 step-time 0.22s wps 20.57K ppl 33.18 gN 5.82 bleu 4.94, Mon Apr  2 05:29:16 2018\n",
      "  step 2400 lr 0.001 step-time 0.22s wps 20.79K ppl 33.28 gN 5.89 bleu 4.94, Mon Apr  2 05:29:38 2018\n",
      "  step 2500 lr 0.001 step-time 0.22s wps 20.67K ppl 32.69 gN 5.88 bleu 4.94, Mon Apr  2 05:30:01 2018\n",
      "  step 2600 lr 0.001 step-time 0.22s wps 20.77K ppl 32.37 gN 5.82 bleu 4.94, Mon Apr  2 05:30:23 2018\n",
      "  step 2700 lr 0.001 step-time 0.23s wps 20.67K ppl 32.14 gN 5.84 bleu 4.94, Mon Apr  2 05:30:45 2018\n",
      "  step 2800 lr 0.001 step-time 0.22s wps 20.68K ppl 31.07 gN 5.83 bleu 4.94, Mon Apr  2 05:31:07 2018\n",
      "  step 2900 lr 0.001 step-time 0.22s wps 20.96K ppl 31.31 gN 5.72 bleu 4.94, Mon Apr  2 05:31:30 2018\n",
      "  step 3000 lr 0.001 step-time 0.22s wps 20.73K ppl 30.40 gN 5.72 bleu 4.94, Mon Apr  2 05:31:52 2018\n",
      "# Save eval, global step 3000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-3000, time 0.09s\n",
      "  # 677\n",
      "    src: Vâng , giả dụ bạn sống ở một vùng xa xôi hẻo lánh nào đó và bạn có một người thân bị tắc hai động mạch vành và bác sĩ gia đình chuyển người thân đó lên một bác sĩ chuyên khoa tim có chỉ số nong rộng động mạch vành thành công là 200 .\n",
      "    ref: Now suppose you live in a certain part of a certain remote place and you have a loved one who has blockages in two coronary arteries and your family doctor refers that loved one to a cardiologist who &apos;s batting 200 on angioplasties .\n",
      "    nmt: Well , you know , in the U.S. and a half of the <unk> and a half of the <unk> of the <unk> and the <unk> of the <unk> , you have a <unk> , and I &apos;m going to tell you a little bit about what they &apos;re doing ,\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-3000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-3000, time 0.10s\n",
      "  eval dev: perplexity 36.36, time 1s, Mon Apr  2 05:31:54 2018.\n",
      "  eval test: perplexity 41.11, time 1s, Mon Apr  2 05:31:55 2018.\n",
      "  step 3100 lr 0.001 step-time 0.22s wps 20.81K ppl 29.79 gN 5.69 bleu 4.94, Mon Apr  2 05:32:17 2018\n",
      "# Finished an epoch, step 3129. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-3000, time 0.08s\n",
      "  # 507\n",
      "    src: Gần đây , tôi đã khảo sát với hơn 2.000 người Mỹ , và trung bình số lựa chọn mà người châu mỹ điển hình đã làm là khoảng 70 lần trong 1 ngày\n",
      "    ref: I recently did a survey with over 2,000 Americans , and the average number of choices that the typical American reports making is about 70 in a typical day .\n",
      "    nmt: I &apos;ve been lucky , in many years , and the <unk> of the <unk> <unk> , the <unk> of the <unk> and the <unk> of the world , and the other <unk>\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-3000, time 0.08s\n",
      "# External evaluation, global step 3000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:32:33 2018.\n",
      "  bleu dev: 6.0\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 3000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 05:32:42 2018.\n",
      "  bleu test: 5.2\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 3200 lr 0.001 step-time 0.35s wps 12.75K ppl 24.85 gN 5.90 bleu 6.04, Mon Apr  2 05:33:12 2018\n",
      "  step 3300 lr 0.001 step-time 0.22s wps 20.88K ppl 23.75 gN 6.03 bleu 6.04, Mon Apr  2 05:33:34 2018\n",
      "  step 3400 lr 0.001 step-time 0.22s wps 20.83K ppl 23.31 gN 5.96 bleu 6.04, Mon Apr  2 05:33:56 2018\n",
      "  step 3500 lr 0.001 step-time 0.22s wps 20.90K ppl 24.00 gN 6.05 bleu 6.04, Mon Apr  2 05:34:18 2018\n",
      "  step 3600 lr 0.001 step-time 0.22s wps 20.69K ppl 23.32 gN 5.89 bleu 6.04, Mon Apr  2 05:34:40 2018\n",
      "  step 3700 lr 0.001 step-time 0.22s wps 20.78K ppl 23.46 gN 5.94 bleu 6.04, Mon Apr  2 05:35:03 2018\n",
      "  step 3800 lr 0.001 step-time 0.22s wps 20.75K ppl 22.98 gN 5.88 bleu 6.04, Mon Apr  2 05:35:25 2018\n",
      "  step 3900 lr 0.001 step-time 0.22s wps 20.88K ppl 23.39 gN 5.98 bleu 6.04, Mon Apr  2 05:35:47 2018\n",
      "  step 4000 lr 0.001 step-time 0.22s wps 20.79K ppl 23.12 gN 5.99 bleu 6.04, Mon Apr  2 05:36:09 2018\n",
      "# Save eval, global step 4000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-4000, time 0.09s\n",
      "  # 1151\n",
      "    src: Tôi nói với máy tính , &quot; Hãy lặp lại quá trình . &quot;\n",
      "    ref: Say , &quot; Please repeat that process . &quot;\n",
      "    nmt: I said , &quot; Let &apos;s take my own . &quot;\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-4000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-4000, time 0.10s\n",
      "  eval dev: perplexity 32.69, time 1s, Mon Apr  2 05:36:11 2018.\n",
      "  eval test: perplexity 37.41, time 1s, Mon Apr  2 05:36:13 2018.\n",
      "  step 4100 lr 0.001 step-time 0.22s wps 20.84K ppl 22.83 gN 5.90 bleu 6.04, Mon Apr  2 05:36:35 2018\n",
      "# Finished an epoch, step 4172. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-4000, time 0.09s\n",
      "  # 781\n",
      "    src: Và hai ngày sau tôi đến ca trực cấp cứu tiếp theo , và đó là lúc cấp trên của tôi yêu cầu nói chuyện riêng với tôi trong phòng bà .\n",
      "    ref: And two days later I came to do my next emergency shift , and that &apos;s when my chief asked to speak to me quietly in her office .\n",
      "    nmt: And two days later , I went to the <unk> and the <unk> of my life , and I was in the world , and I asked him to be a\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-4000, time 0.09s\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:37:00 2018.\n",
      "  bleu dev: 6.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 05:37:09 2018.\n",
      "  bleu test: 5.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 4200 lr 0.001 step-time 0.34s wps 13.09K ppl 20.99 gN 5.90 bleu 6.90, Mon Apr  2 05:37:28 2018\n",
      "  step 4300 lr 0.001 step-time 0.23s wps 19.71K ppl 17.69 gN 6.15 bleu 6.90, Mon Apr  2 05:37:52 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 4400 lr 0.001 step-time 0.22s wps 20.81K ppl 18.01 gN 6.24 bleu 6.90, Mon Apr  2 05:38:14 2018\n",
      "  step 4500 lr 0.001 step-time 0.22s wps 20.70K ppl 17.78 gN 6.18 bleu 6.90, Mon Apr  2 05:38:36 2018\n",
      "  step 4600 lr 0.001 step-time 0.22s wps 20.83K ppl 18.30 gN 6.19 bleu 6.90, Mon Apr  2 05:38:58 2018\n",
      "  step 4700 lr 0.001 step-time 0.22s wps 20.79K ppl 18.15 gN 6.17 bleu 6.90, Mon Apr  2 05:39:20 2018\n",
      "  step 4800 lr 0.001 step-time 0.22s wps 20.82K ppl 18.37 gN 6.18 bleu 6.90, Mon Apr  2 05:39:43 2018\n",
      "  step 4900 lr 0.001 step-time 0.22s wps 20.75K ppl 18.03 gN 6.09 bleu 6.90, Mon Apr  2 05:40:05 2018\n",
      "  step 5000 lr 0.001 step-time 0.22s wps 20.78K ppl 18.23 gN 6.11 bleu 6.90, Mon Apr  2 05:40:27 2018\n",
      "# Save eval, global step 5000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-5000, time 0.09s\n",
      "  # 781\n",
      "    src: Và hai ngày sau tôi đến ca trực cấp cứu tiếp theo , và đó là lúc cấp trên của tôi yêu cầu nói chuyện riêng với tôi trong phòng bà .\n",
      "    ref: And two days later I came to do my next emergency shift , and that &apos;s when my chief asked to speak to me quietly in her office .\n",
      "    nmt: And then two days later , I was walking around the world and I was asked to be a child , and I was asked to be a woman who was\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-5000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-5000, time 0.10s\n",
      "  eval dev: perplexity 30.48, time 1s, Mon Apr  2 05:40:29 2018.\n",
      "  eval test: perplexity 34.65, time 1s, Mon Apr  2 05:40:31 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-5000, time 0.09s\n",
      "  # 1501\n",
      "    src: Bjorn Sundin .\n",
      "    ref: This is Bjorn Sundin .\n",
      "    nmt: <unk> <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-5000, time 0.09s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 05:40:55 2018.\n",
      "  bleu dev: 7.7\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 05:41:06 2018.\n",
      "  bleu test: 6.3\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 5100 lr 0.001 step-time 0.22s wps 20.74K ppl 18.03 gN 6.08 bleu 7.65, Mon Apr  2 05:41:28 2018\n",
      "  step 5200 lr 0.001 step-time 0.22s wps 20.68K ppl 18.10 gN 6.08 bleu 7.65, Mon Apr  2 05:41:51 2018\n",
      "# Finished an epoch, step 5215. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-5000, time 0.08s\n",
      "  # 729\n",
      "    src: Phần còn lại ngày hôm đó , chiều đó , Tôi có cảm giác nôn nao trong lòng .\n",
      "    ref: All the rest of that day , that afternoon , I had this kind of gnawing feeling inside my stomach .\n",
      "    nmt: The other day , I had the same feeling , I was in the <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-5000, time 0.08s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:42:03 2018.\n",
      "  bleu dev: 7.7\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 05:42:12 2018.\n",
      "  bleu test: 6.3\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 5300 lr 0.001 step-time 0.35s wps 12.78K ppl 14.31 gN 6.36 bleu 7.65, Mon Apr  2 05:42:45 2018\n",
      "  step 5400 lr 0.001 step-time 0.22s wps 20.75K ppl 13.89 gN 6.25 bleu 7.65, Mon Apr  2 05:43:07 2018\n",
      "  step 5500 lr 0.001 step-time 0.22s wps 20.64K ppl 14.16 gN 6.36 bleu 7.65, Mon Apr  2 05:43:29 2018\n",
      "  step 5600 lr 0.001 step-time 0.23s wps 20.84K ppl 14.50 gN 6.46 bleu 7.65, Mon Apr  2 05:43:52 2018\n",
      "  step 5700 lr 0.001 step-time 0.22s wps 20.64K ppl 14.44 gN 6.34 bleu 7.65, Mon Apr  2 05:44:14 2018\n",
      "  step 5800 lr 0.001 step-time 0.22s wps 20.63K ppl 14.64 gN 6.34 bleu 7.65, Mon Apr  2 05:44:36 2018\n",
      "  step 5900 lr 0.001 step-time 0.22s wps 20.69K ppl 14.77 gN 6.32 bleu 7.65, Mon Apr  2 05:44:59 2018\n",
      "  step 6000 lr 0.001 step-time 0.22s wps 20.74K ppl 14.90 gN 6.34 bleu 7.65, Mon Apr  2 05:45:21 2018\n",
      "# Save eval, global step 6000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-6000, time 0.09s\n",
      "  # 1536\n",
      "    src: Nhưng chính phủ phương Tây cũng thực hiện điều đó ngay tại quốc gia của mình\n",
      "    ref: But Western governments are doing it to themselves as well .\n",
      "    nmt: But the government of the West is that the rest of the world are .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-6000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-6000, time 0.10s\n",
      "  eval dev: perplexity 29.74, time 1s, Mon Apr  2 05:45:23 2018.\n",
      "  eval test: perplexity 33.73, time 1s, Mon Apr  2 05:45:24 2018.\n",
      "  step 6100 lr 0.001 step-time 0.22s wps 20.66K ppl 14.82 gN 6.28 bleu 7.65, Mon Apr  2 05:45:47 2018\n",
      "  step 6200 lr 0.001 step-time 0.22s wps 20.63K ppl 14.97 gN 6.28 bleu 7.65, Mon Apr  2 05:46:09 2018\n",
      "# Finished an epoch, step 6258. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-6000, time 0.09s\n",
      "  # 1087\n",
      "    src: Và vào thời điểm ấy , chúng ta có quần thể đa tế bào , quần thể có vô số loại tế bào khác nhau , làm việc cùng nhau như một cơ quan duy nhất .\n",
      "    ref: And at that stage , we have multi-cellular communities , communities of lots of different types of cells , working together as a single organism .\n",
      "    nmt: And at the same time , we have a <unk> cell , which is different from the genetic variation , which is a different kind of cell .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-6000, time 0.09s\n",
      "# External evaluation, global step 6000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:46:31 2018.\n",
      "  bleu dev: 7.7\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 6000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 05:46:40 2018.\n",
      "  bleu test: 6.6\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 6300 lr 0.001 step-time 0.35s wps 12.86K ppl 13.04 gN 6.31 bleu 7.70, Mon Apr  2 05:47:03 2018\n",
      "  step 6400 lr 0.001 step-time 0.23s wps 20.30K ppl 11.41 gN 6.46 bleu 7.70, Mon Apr  2 05:47:26 2018\n",
      "  step 6500 lr 0.001 step-time 0.22s wps 20.80K ppl 11.58 gN 6.52 bleu 7.70, Mon Apr  2 05:47:48 2018\n",
      "  step 6600 lr 0.001 step-time 0.22s wps 20.68K ppl 11.80 gN 6.51 bleu 7.70, Mon Apr  2 05:48:10 2018\n",
      "  step 6700 lr 0.001 step-time 0.22s wps 20.82K ppl 11.94 gN 6.55 bleu 7.70, Mon Apr  2 05:48:32 2018\n",
      "  step 6800 lr 0.001 step-time 0.22s wps 20.72K ppl 12.11 gN 6.52 bleu 7.70, Mon Apr  2 05:48:55 2018\n",
      "  step 6900 lr 0.001 step-time 0.22s wps 20.71K ppl 12.17 gN 6.57 bleu 7.70, Mon Apr  2 05:49:17 2018\n",
      "  step 7000 lr 0.001 step-time 0.22s wps 20.72K ppl 12.47 gN 6.49 bleu 7.70, Mon Apr  2 05:49:39 2018\n",
      "# Save eval, global step 7000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-7000, time 0.09s\n",
      "  # 423\n",
      "    src: Một ngành ngư nghiệp quy mô lớn đang khai thác loại cá này cho đến thập niên 80 .\n",
      "    ref: A big fishery was run on it until the &apos; 80s .\n",
      "    nmt: A huge <unk> of the population of the <unk> that I &apos;ve been used for about 60 to 1,000 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-7000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-7000, time 0.10s\n",
      "  eval dev: perplexity 29.69, time 1s, Mon Apr  2 05:49:41 2018.\n",
      "  eval test: perplexity 34.04, time 1s, Mon Apr  2 05:49:43 2018.\n",
      "  step 7100 lr 0.001 step-time 0.22s wps 20.58K ppl 12.40 gN 6.56 bleu 7.70, Mon Apr  2 05:50:05 2018\n",
      "  step 7200 lr 0.001 step-time 0.22s wps 20.62K ppl 12.54 gN 6.46 bleu 7.70, Mon Apr  2 05:50:28 2018\n",
      "  step 7300 lr 0.001 step-time 0.22s wps 20.36K ppl 12.48 gN 6.46 bleu 7.70, Mon Apr  2 05:50:49 2018\n",
      "# Finished an epoch, step 7301. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-7000, time 0.09s\n",
      "  # 825\n",
      "    src: Ta không thể tống khứ vấn đề này được .\n",
      "    ref: We can &apos;t get rid of it .\n",
      "    nmt: We can &apos;t have to be the same thing .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-7000, time 0.09s\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:50:59 2018.\n",
      "  bleu dev: 7.8\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 05:51:08 2018.\n",
      "  bleu test: 6.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 7400 lr 0.001 step-time 0.36s wps 13.16K ppl 9.59 gN 6.61 bleu 7.76, Mon Apr  2 05:51:44 2018\n",
      "  step 7500 lr 0.001 step-time 0.22s wps 20.62K ppl 9.57 gN 6.57 bleu 7.76, Mon Apr  2 05:52:06 2018\n",
      "  step 7600 lr 0.001 step-time 0.22s wps 20.68K ppl 9.99 gN 6.66 bleu 7.76, Mon Apr  2 05:52:29 2018\n",
      "  step 7700 lr 0.001 step-time 0.22s wps 20.73K ppl 10.10 gN 6.68 bleu 7.76, Mon Apr  2 05:52:51 2018\n",
      "  step 7800 lr 0.001 step-time 0.22s wps 20.82K ppl 10.27 gN 6.80 bleu 7.76, Mon Apr  2 05:53:13 2018\n",
      "  step 7900 lr 0.001 step-time 0.22s wps 20.78K ppl 10.46 gN 6.68 bleu 7.76, Mon Apr  2 05:53:35 2018\n",
      "  step 8000 lr 0.001 step-time 0.22s wps 20.73K ppl 10.56 gN 6.74 bleu 7.76, Mon Apr  2 05:53:57 2018\n",
      "# Save eval, global step 8000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-8000, time 0.09s\n",
      "  # 54\n",
      "    src: Trong thế giới kia , tôi vướng mắc trong những mảnh đời bấp bênh , bị tổn thương bi thảm vì bạo lực , nghiện ngập và cô quạnh .\n",
      "    ref: In the other , I was enmeshed in lives that were precarious , tragically scarred by violence , drug abuse and isolation .\n",
      "    nmt: In the world , I found myself in the <unk> of <unk> , <unk> , <unk> , <unk> , <unk> , and I was <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-8000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-8000, time 0.10s\n",
      "  eval dev: perplexity 30.48, time 1s, Mon Apr  2 05:54:00 2018.\n",
      "  eval test: perplexity 34.68, time 1s, Mon Apr  2 05:54:01 2018.\n",
      "  step 8100 lr 0.001 step-time 0.22s wps 20.42K ppl 10.56 gN 6.62 bleu 7.76, Mon Apr  2 05:54:23 2018\n",
      "  step 8200 lr 0.001 step-time 0.23s wps 20.58K ppl 10.72 gN 6.79 bleu 7.76, Mon Apr  2 05:54:46 2018\n",
      "  step 8300 lr 0.001 step-time 0.23s wps 20.43K ppl 10.70 gN 6.69 bleu 7.76, Mon Apr  2 05:55:09 2018\n",
      "# Finished an epoch, step 8344. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-8000, time 0.09s\n",
      "  # 505\n",
      "    src: Bạn có biết bạn thực hiện bao nhiêu sự lựa chọn trong 1 ngày ?\n",
      "    ref: Do you know how many choices you make in a typical day ?\n",
      "    nmt: Do you know how many of you have to do ?\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-8000, time 0.09s\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:55:28 2018.\n",
      "  bleu dev: 7.7\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 05:55:37 2018.\n",
      "  bleu test: 6.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 8400 lr 0.001 step-time 0.36s wps 12.64K ppl 9.20 gN 6.62 bleu 7.76, Mon Apr  2 05:56:04 2018\n",
      "  step 8500 lr 0.001 step-time 0.22s wps 20.64K ppl 8.27 gN 6.70 bleu 7.76, Mon Apr  2 05:56:26 2018\n",
      "  step 8600 lr 0.001 step-time 0.22s wps 20.67K ppl 8.59 gN 6.86 bleu 7.76, Mon Apr  2 05:56:49 2018\n",
      "  step 8700 lr 0.001 step-time 0.22s wps 20.66K ppl 8.77 gN 6.89 bleu 7.76, Mon Apr  2 05:57:11 2018\n",
      "  step 8800 lr 0.001 step-time 0.22s wps 20.61K ppl 8.77 gN 6.76 bleu 7.76, Mon Apr  2 05:57:33 2018\n",
      "  step 8900 lr 0.001 step-time 0.22s wps 20.65K ppl 8.99 gN 6.85 bleu 7.76, Mon Apr  2 05:57:56 2018\n",
      "  step 9000 lr 0.001 step-time 0.22s wps 20.64K ppl 9.15 gN 6.88 bleu 7.76, Mon Apr  2 05:58:18 2018\n",
      "# Save eval, global step 9000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-9000, time 0.09s\n",
      "  # 1213\n",
      "    src: Hiện tại có chín thành phố đang lên kế hoạch sử dụng ứng dụng này .\n",
      "    ref: So we now know of nine cities that are planning to use this .\n",
      "    nmt: Today in the city will be building up to the building industry .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-9000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-9000, time 0.10s\n",
      "  eval dev: perplexity 31.18, time 1s, Mon Apr  2 05:58:20 2018.\n",
      "  eval test: perplexity 35.75, time 1s, Mon Apr  2 05:58:22 2018.\n",
      "  step 9100 lr 0.0005 step-time 0.22s wps 20.51K ppl 8.90 gN 6.66 bleu 7.76, Mon Apr  2 05:58:44 2018\n",
      "  step 9200 lr 0.0005 step-time 0.22s wps 20.50K ppl 8.78 gN 6.64 bleu 7.76, Mon Apr  2 05:59:06 2018\n",
      "  step 9300 lr 0.0005 step-time 0.22s wps 20.55K ppl 8.87 gN 6.67 bleu 7.76, Mon Apr  2 05:59:29 2018\n",
      "# Finished an epoch, step 9387. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-9000, time 0.09s\n",
      "  # 1453\n",
      "    src: Gắn thêm chiếc chuông điện này và giờ bạn đã tạo ra máy tạo tiếng động .\n",
      "    ref: Add this buzzer for some extra punch and you &apos;ve created a noise machine .\n",
      "    nmt: <unk> with the <unk> and you have a <unk> machine .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-9000, time 0.09s\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 05:59:57 2018.\n",
      "  bleu dev: 7.7\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 06:00:07 2018.\n",
      "  bleu test: 7.0\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 9400 lr 0.0005 step-time 0.33s wps 13.41K ppl 8.56 gN 6.74 bleu 7.76, Mon Apr  2 06:00:21 2018\n",
      "  step 9500 lr 0.0005 step-time 0.24s wps 19.06K ppl 6.93 gN 6.58 bleu 7.76, Mon Apr  2 06:00:46 2018\n",
      "  step 9600 lr 0.0005 step-time 0.22s wps 20.72K ppl 7.01 gN 6.72 bleu 7.76, Mon Apr  2 06:01:08 2018\n",
      "  step 9700 lr 0.0005 step-time 0.22s wps 20.67K ppl 7.16 gN 6.70 bleu 7.76, Mon Apr  2 06:01:30 2018\n",
      "  step 9800 lr 0.0005 step-time 0.22s wps 20.64K ppl 7.13 gN 6.77 bleu 7.76, Mon Apr  2 06:01:53 2018\n",
      "  step 9900 lr 0.0005 step-time 0.22s wps 20.73K ppl 7.30 gN 6.91 bleu 7.76, Mon Apr  2 06:02:15 2018\n",
      "  step 10000 lr 0.0005 step-time 0.22s wps 20.50K ppl 7.27 gN 6.83 bleu 7.76, Mon Apr  2 06:02:37 2018\n",
      "# Save eval, global step 10000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-10000, time 0.09s\n",
      "  # 101\n",
      "    src: Chúng tôi kể chuyện cho bà và cam đoan với bà là chúng tôi luôn ở bên bà .\n",
      "    ref: We told her stories and assured her that we were still with her .\n",
      "    nmt: We talked to her mother and she left her to her .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-10000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-10000, time 0.10s\n",
      "  eval dev: perplexity 31.50, time 1s, Mon Apr  2 06:02:40 2018.\n",
      "  eval test: perplexity 36.12, time 1s, Mon Apr  2 06:02:41 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-10000, time 0.09s\n",
      "  # 390\n",
      "    src: Thay vì chỉ biết quyên góp tiền , chúng tôi có thể giúp được gì ?\n",
      "    ref: Other than writing a check , what could we do ?\n",
      "    nmt: So instead of just knowing how much money we could do ?\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-10000, time 0.09s\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 06:03:02 2018.\n",
      "  bleu dev: 7.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 06:03:13 2018.\n",
      "  bleu test: 6.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 10100 lr 0.00025 step-time 0.22s wps 20.64K ppl 7.32 gN 6.78 bleu 7.94, Mon Apr  2 06:03:36 2018\n",
      "  step 10200 lr 0.00025 step-time 0.22s wps 20.65K ppl 7.22 gN 6.74 bleu 7.94, Mon Apr  2 06:03:58 2018\n",
      "  step 10300 lr 0.00025 step-time 0.22s wps 20.65K ppl 7.34 gN 6.78 bleu 7.94, Mon Apr  2 06:04:20 2018\n",
      "  step 10400 lr 0.00025 step-time 0.22s wps 20.65K ppl 7.29 gN 6.84 bleu 7.94, Mon Apr  2 06:04:43 2018\n",
      "# Finished an epoch, step 10430. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-10000, time 0.09s\n",
      "  # 677\n",
      "    src: Vâng , giả dụ bạn sống ở một vùng xa xôi hẻo lánh nào đó và bạn có một người thân bị tắc hai động mạch vành và bác sĩ gia đình chuyển người thân đó lên một bác sĩ chuyên khoa tim có chỉ số nong rộng động mạch vành thành công là 200 .\n",
      "    ref: Now suppose you live in a certain part of a certain remote place and you have a loved one who has blockages in two coronary arteries and your family doctor refers that loved one to a cardiologist who &apos;s batting 200 on angioplasties .\n",
      "    nmt: Well , if you have a <unk> , you &apos;re not born with a dead person , and you &apos;re a <unk> <unk> for your <unk> , and you can &apos;t do it .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-10000, time 0.09s\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 06:04:58 2018.\n",
      "  bleu dev: 7.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 06:05:08 2018.\n",
      "  bleu test: 6.9\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 10500 lr 0.00025 step-time 0.35s wps 12.60K ppl 6.53 gN 6.69 bleu 7.94, Mon Apr  2 06:05:37 2018\n",
      "  step 10600 lr 0.00025 step-time 0.22s wps 20.59K ppl 6.38 gN 6.65 bleu 7.94, Mon Apr  2 06:05:59 2018\n",
      "  step 10700 lr 0.00025 step-time 0.22s wps 20.59K ppl 6.35 gN 6.67 bleu 7.94, Mon Apr  2 06:06:22 2018\n",
      "  step 10800 lr 0.00025 step-time 0.23s wps 20.59K ppl 6.46 gN 6.84 bleu 7.94, Mon Apr  2 06:06:44 2018\n",
      "  step 10900 lr 0.00025 step-time 0.22s wps 20.49K ppl 6.44 gN 6.79 bleu 7.94, Mon Apr  2 06:07:06 2018\n",
      "  step 11000 lr 0.00025 step-time 0.22s wps 20.59K ppl 6.54 gN 6.84 bleu 7.94, Mon Apr  2 06:07:29 2018\n",
      "# Save eval, global step 11000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-11000, time 0.09s\n",
      "  # 385\n",
      "    src: Rằng sự tồn tại của tất cả chúng ta gắn bó mật thiết với sự tồn tài của từng người .\n",
      "    ref: That all of our survival is tied to the survival of everyone .\n",
      "    nmt: That &apos;s the whole thing for all of us to do with the restorative benefits .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-11000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-11000, time 0.10s\n",
      "  eval dev: perplexity 32.04, time 1s, Mon Apr  2 06:07:31 2018.\n",
      "  eval test: perplexity 36.65, time 1s, Mon Apr  2 06:07:32 2018.\n",
      "  step 11100 lr 0.000125 step-time 0.23s wps 20.64K ppl 6.57 gN 6.88 bleu 7.94, Mon Apr  2 06:07:55 2018\n",
      "  step 11200 lr 0.000125 step-time 0.22s wps 20.53K ppl 6.51 gN 6.75 bleu 7.94, Mon Apr  2 06:08:17 2018\n",
      "  step 11300 lr 0.000125 step-time 0.23s wps 20.56K ppl 6.60 gN 6.92 bleu 7.94, Mon Apr  2 06:08:40 2018\n",
      "  step 11400 lr 0.000125 step-time 0.22s wps 20.48K ppl 6.55 gN 6.83 bleu 7.94, Mon Apr  2 06:09:03 2018\n",
      "# Finished an epoch, step 11473. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-11000, time 0.09s\n",
      "  # 90\n",
      "    src: Cuộc sống đã là như vậy trong hàng thế kỷ rồi .\n",
      "    ref: Life hadn &apos;t changed for centuries .\n",
      "    nmt: Life is like the next century .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-11000, time 0.09s\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 06:09:28 2018.\n",
      "  bleu dev: 8.0\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 06:09:37 2018.\n",
      "  bleu test: 7.3\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "  step 11500 lr 0.000125 step-time 0.34s wps 12.99K ppl 6.33 gN 6.79 bleu 8.02, Mon Apr  2 06:09:56 2018\n",
      "  step 11600 lr 0.000125 step-time 0.24s wps 19.53K ppl 6.09 gN 6.78 bleu 8.02, Mon Apr  2 06:10:20 2018\n",
      "  step 11700 lr 0.000125 step-time 0.22s wps 20.74K ppl 6.13 gN 6.80 bleu 8.02, Mon Apr  2 06:10:42 2018\n",
      "  step 11800 lr 0.000125 step-time 0.22s wps 20.68K ppl 6.08 gN 6.81 bleu 8.02, Mon Apr  2 06:11:04 2018\n",
      "  step 11900 lr 0.000125 step-time 0.22s wps 20.61K ppl 6.21 gN 6.87 bleu 8.02, Mon Apr  2 06:11:27 2018\n",
      "  step 12000 lr 0.000125 step-time 0.22s wps 20.62K ppl 6.10 gN 6.82 bleu 8.02, Mon Apr  2 06:11:49 2018\n",
      "# Save eval, global step 12000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-12000, time 0.09s\n",
      "  # 239\n",
      "    src: cũng không phải điều mà chúng ta cần đấu tranh , nỗ lực .\n",
      "    ref: It &apos;s not our struggle .\n",
      "    nmt: It &apos;s not that we need to fight , but to fight .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-12000\n",
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-12000, time 0.10s\n",
      "  eval dev: perplexity 32.45, time 1s, Mon Apr  2 06:11:51 2018.\n",
      "  eval test: perplexity 37.13, time 1s, Mon Apr  2 06:11:53 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-12000, time 0.09s\n",
      "  # 1108\n",
      "    src: Nếu bạn nghĩ về nó , bạn sẽ thấy đó là một phát minh hết sức vĩ đại .\n",
      "    ref: It &apos;s really a pretty amazing invention if you think about it .\n",
      "    nmt: If you think about it , it &apos;s probably going to be a very powerful thing .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loaded eval model parameters from nmt_model_noatt/translate.ckpt-12000, time 0.10s\n",
      "  eval dev: perplexity 32.45, time 1s, Mon Apr  2 06:12:05 2018.\n",
      "  eval test: perplexity 37.13, time 1s, Mon Apr  2 06:12:06 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_noatt/translate.ckpt-12000, time 0.09s\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 06:12:15 2018.\n",
      "  bleu dev: 8.1\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 06:12:25 2018.\n",
      "  bleu test: 7.3\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# Final, step 12000 lr 0.000125 step-time 0.22s wps 20.62K ppl 6.10 gN 6.82 dev ppl 32.45, dev bleu 8.1, test ppl 37.13, test bleu 7.3, Mon Apr  2 06:12:25 2018\n",
      "# Done training!, time 3189s, Mon Apr  2 06:12:25 2018.\n",
      "# Start evaluating saved best models.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/best_bleu/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_noatt/best_bleu/translate.ckpt-12000, time 0.09s\n",
      "  # 438\n",
      "    src: Ngành thuỷ sản đánh bắt cá rô phi cằm đen này giúp ổn định số lượng cá và họ thực sự có thời đánh bắt khá thuận lợi họ kiếm được nhiều hơn mức thu nhập trung bình ở Ghana .\n",
      "    ref: And the fisheries for this tilapia sustained lots of fish and they had a good time and they earned more than average in Ghana .\n",
      "    nmt: The wasp that the fetus <unk> the <unk> and the <unk> <unk> of the <unk> <unk> that they used to be used in the <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/best_bleu/translate.ckpt-12000\n",
      "  loaded eval model parameters from nmt_model_noatt/best_bleu/translate.ckpt-12000, time 0.10s\n",
      "  eval dev: perplexity 32.45, time 1s, Mon Apr  2 06:12:27 2018.\n",
      "  eval test: perplexity 37.13, time 1s, Mon Apr  2 06:12:28 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_noatt/best_bleu/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_noatt/best_bleu/translate.ckpt-12000, time 0.09s\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_noatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 06:12:38 2018.\n",
      "  bleu dev: 8.1\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_noatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 06:12:47 2018.\n",
      "  bleu test: 7.3\n",
      "  saving hparams to nmt_model_noatt/hparams\n",
      "# Best bleu, step 12000 lr 0.000125 step-time 0.22s wps 20.62K ppl 6.10 gN 6.82 dev ppl 32.45, dev bleu 8.1, test ppl 37.13, test bleu 7.3, Mon Apr  2 06:12:47 2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dev_ppl': 32.448353852135405,\n",
       "  'dev_scores': {'bleu': 8.058157810604447},\n",
       "  'test_ppl': 37.13180638014357,\n",
       "  'test_scores': {'bleu': 7.297478762553148}},\n",
       " 12000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If desired as a baseline, train a vanilla LSTM model without attention\n",
    "hparams = create_standard_hparams(\n",
    "    data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "    out_dir=\"nmt_model_noatt\"\n",
    ")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithDummyAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab file datasets/nmt_data_vi/vocab.vi exists\n",
      "# Vocab file datasets/nmt_data_vi/vocab.en exists\n",
      "# creating train graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0  DropoutWrapper, dropout=0.2   DropoutWrapper  DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=luong234, start_decay_step=8000, decay_steps 1000, decay_factor 0.5\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating eval graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTMCellWithDotProductAttention, dropout=0   LSTMCellWithDotProductAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating infer graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTMCellWithDotProductAttention, dropout=0   LSTMCellWithDotProductAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_dot_product_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "  created train model with fresh parameters, time 0.40s\n",
      "  created infer model with fresh parameters, time 0.09s\n",
      "  # 705\n",
      "    src: Và khi tôi nghe bà thở , bà phát ra tiếng khò khè .\n",
      "    ref: And when I listened to her , she was making a wheezy sound .\n",
      "    nmt: consumes consumes Dance Dance dozens collapse anticipate demons demons blowing poking center Commandments harnessed harnessed harnessed harnessed harnessed Challenge linked linked youth youth airline airline airline airline airline\n",
      "  created eval model with fresh parameters, time 0.13s\n",
      "  eval dev: perplexity 17415.45, time 1s, Mon Apr  2 07:03:26 2018.\n",
      "  eval test: perplexity 17405.32, time 1s, Mon Apr  2 07:03:27 2018.\n",
      "  created infer model with fresh parameters, time 0.05s\n",
      "# Start step 0, lr 0.001, Mon Apr  2 07:03:27 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 0.001 step-time 0.38s wps 12.11K ppl 574.93 gN 15.53 bleu 0.00, Mon Apr  2 07:04:06 2018\n",
      "  step 200 lr 0.001 step-time 0.23s wps 19.75K ppl 261.42 gN 9.89 bleu 0.00, Mon Apr  2 07:04:29 2018\n",
      "  step 300 lr 0.001 step-time 0.23s wps 19.93K ppl 166.64 gN 8.95 bleu 0.00, Mon Apr  2 07:04:52 2018\n",
      "  step 400 lr 0.001 step-time 0.23s wps 19.62K ppl 133.89 gN 8.93 bleu 0.00, Mon Apr  2 07:05:15 2018\n",
      "  step 500 lr 0.001 step-time 0.23s wps 19.72K ppl 110.52 gN 9.09 bleu 0.00, Mon Apr  2 07:05:39 2018\n",
      "  step 600 lr 0.001 step-time 0.23s wps 19.70K ppl 87.91 gN 8.26 bleu 0.00, Mon Apr  2 07:06:02 2018\n",
      "  step 700 lr 0.001 step-time 0.23s wps 19.70K ppl 72.47 gN 7.63 bleu 0.00, Mon Apr  2 07:06:26 2018\n",
      "  step 800 lr 0.001 step-time 0.23s wps 19.71K ppl 61.41 gN 7.46 bleu 0.00, Mon Apr  2 07:06:49 2018\n",
      "  step 900 lr 0.001 step-time 0.23s wps 19.68K ppl 52.41 gN 7.11 bleu 0.00, Mon Apr  2 07:07:13 2018\n",
      "  step 1000 lr 0.001 step-time 0.23s wps 19.82K ppl 45.77 gN 7.00 bleu 0.00, Mon Apr  2 07:07:36 2018\n",
      "# Save eval, global step 1000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-1000, time 0.08s\n",
      "  # 1488\n",
      "    src: Đây chính là một ví dụ về cách mà chính phủ của chúng ta đang sử dụng công nghệ để chống lại chúng ta , những công dân của đất nước .\n",
      "    ref: And this is an example of the ways that our own governments are using technology against us , the citizens .\n",
      "    nmt: This is a example of the way that the <unk> of our technology is to create our technology , and the <unk> of the country .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-1000, time 0.10s\n",
      "  eval dev: perplexity 42.63, time 1s, Mon Apr  2 07:07:38 2018.\n",
      "  eval test: perplexity 45.56, time 1s, Mon Apr  2 07:07:40 2018.\n",
      "# Finished an epoch, step 1043. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-1000, time 0.08s\n",
      "  # 1289\n",
      "    src: Đó chính là những gì OccupytheSEC đã làm .\n",
      "    ref: So that &apos;s OccupytheSEC movement has done .\n",
      "    nmt: That &apos;s what &apos;s <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-1000, time 0.08s\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 07:08:00 2018.\n",
      "  bleu dev: 6.9\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:08:10 2018.\n",
      "  bleu test: 6.7\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 1100 lr 0.001 step-time 0.37s wps 12.24K ppl 39.13 gN 7.62 bleu 6.90, Mon Apr  2 07:08:37 2018\n",
      "  step 1200 lr 0.001 step-time 0.24s wps 19.42K ppl 35.48 gN 6.79 bleu 6.90, Mon Apr  2 07:09:01 2018\n",
      "  step 1300 lr 0.001 step-time 0.23s wps 19.69K ppl 31.92 gN 6.60 bleu 6.90, Mon Apr  2 07:09:24 2018\n",
      "  step 1400 lr 0.001 step-time 0.23s wps 19.75K ppl 30.87 gN 7.09 bleu 6.90, Mon Apr  2 07:09:48 2018\n",
      "  step 1500 lr 0.001 step-time 0.23s wps 19.75K ppl 28.39 gN 6.71 bleu 6.90, Mon Apr  2 07:10:11 2018\n",
      "  step 1600 lr 0.001 step-time 0.23s wps 19.64K ppl 26.82 gN 6.59 bleu 6.90, Mon Apr  2 07:10:35 2018\n",
      "  step 1700 lr 0.001 step-time 0.23s wps 19.67K ppl 25.31 gN 6.53 bleu 6.90, Mon Apr  2 07:10:58 2018\n",
      "  step 1800 lr 0.001 step-time 0.24s wps 19.62K ppl 24.33 gN 6.79 bleu 6.90, Mon Apr  2 07:11:21 2018\n",
      "  step 1900 lr 0.001 step-time 0.24s wps 19.74K ppl 23.47 gN 6.51 bleu 6.90, Mon Apr  2 07:11:45 2018\n",
      "  step 2000 lr 0.001 step-time 0.23s wps 19.64K ppl 22.03 gN 7.54 bleu 6.90, Mon Apr  2 07:12:08 2018\n",
      "# Save eval, global step 2000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-2000, time 0.08s\n",
      "  # 823\n",
      "    src: Trong một hệ thống bệnh viện nơi mà kiến thức y khoa tăng gấp đôi mỗi hai hay ba năm , ta không thể theo kịp .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ref: In a hospital system where medical knowledge is doubling every two or three years , we can &apos;t keep up with it .\n",
      "    nmt: In a hospital where the medical economy is more than two or three years , we can &apos;t even be able to go .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-2000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-2000, time 0.09s\n",
      "  eval dev: perplexity 23.57, time 1s, Mon Apr  2 07:12:10 2018.\n",
      "  eval test: perplexity 23.55, time 1s, Mon Apr  2 07:12:12 2018.\n",
      "# Finished an epoch, step 2086. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-2000, time 0.08s\n",
      "  # 538\n",
      "    src: Nhiều người đã dừng lại , khoảng 60 % khi chúng tôi đưa ra 24 loại mứt , Và khi chỉ có 6 loại , thì chỉ có 40 % .\n",
      "    ref: More people stopped when there were 24 , about 60 percent , than when there were six , about 40 percent .\n",
      "    nmt: Many people stop , about 60 percent when we put up in a half of the <unk> , and when we put six percent of them , and when you\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-2000, time 0.08s\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:12:42 2018.\n",
      "  bleu dev: 11.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:12:52 2018.\n",
      "  bleu test: 11.1\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 2100 lr 0.001 step-time 0.35s wps 12.91K ppl 20.89 gN 6.68 bleu 10.96, Mon Apr  2 07:13:07 2018\n",
      "  step 2200 lr 0.001 step-time 0.26s wps 17.96K ppl 17.14 gN 6.33 bleu 10.96, Mon Apr  2 07:13:33 2018\n",
      "  step 2300 lr 0.001 step-time 0.24s wps 19.77K ppl 16.95 gN 6.58 bleu 10.96, Mon Apr  2 07:13:57 2018\n",
      "  step 2400 lr 0.001 step-time 0.23s wps 19.59K ppl 16.89 gN 6.51 bleu 10.96, Mon Apr  2 07:14:20 2018\n",
      "  step 2500 lr 0.001 step-time 0.23s wps 19.73K ppl 16.33 gN 6.48 bleu 10.96, Mon Apr  2 07:14:43 2018\n",
      "  step 2600 lr 0.001 step-time 0.23s wps 19.68K ppl 16.14 gN 6.92 bleu 10.96, Mon Apr  2 07:15:07 2018\n",
      "  step 2700 lr 0.001 step-time 0.23s wps 19.79K ppl 15.96 gN 6.37 bleu 10.96, Mon Apr  2 07:15:30 2018\n",
      "  step 2800 lr 0.001 step-time 0.23s wps 19.74K ppl 15.63 gN 6.38 bleu 10.96, Mon Apr  2 07:15:54 2018\n",
      "  step 2900 lr 0.001 step-time 0.23s wps 19.66K ppl 15.44 gN 6.32 bleu 10.96, Mon Apr  2 07:16:17 2018\n",
      "  step 3000 lr 0.001 step-time 0.23s wps 19.61K ppl 14.86 gN 6.33 bleu 10.96, Mon Apr  2 07:16:40 2018\n",
      "# Save eval, global step 3000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-3000, time 0.08s\n",
      "  # 220\n",
      "    src: Mỹ là quốc gia duy nhất trên thế giới kết án đứa trẻ 13 tuổi tù chung thân .\n",
      "    ref: The United States is the only country in the world where we sentence 13-year-old children to die in prison .\n",
      "    nmt: The U.S. is the only country in the world &apos;s <unk> <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-3000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-3000, time 0.09s\n",
      "  eval dev: perplexity 18.62, time 1s, Mon Apr  2 07:16:42 2018.\n",
      "  eval test: perplexity 18.37, time 1s, Mon Apr  2 07:16:44 2018.\n",
      "  step 3100 lr 0.001 step-time 0.24s wps 19.59K ppl 15.09 gN 6.34 bleu 10.96, Mon Apr  2 07:17:08 2018\n",
      "# Finished an epoch, step 3129. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-3000, time 0.08s\n",
      "  # 805\n",
      "    src: Nếu tôi đi vào một căn phòng -- như lúc này , tôi hoàn toàn không biết các bạn sẽ nghĩ gì về tôi .\n",
      "    ref: If I were to walk into a room -- like right now , I have no idea what you think of me .\n",
      "    nmt: If I went into a room -- like this , I &apos;m not really sure you would think about me .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-3000, time 0.08s\n",
      "# External evaluation, global step 3000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:17:23 2018.\n",
      "  bleu dev: 13.6\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 3000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:17:33 2018.\n",
      "  bleu test: 14.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 3200 lr 0.001 step-time 0.37s wps 12.07K ppl 12.47 gN 6.34 bleu 13.62, Mon Apr  2 07:18:04 2018\n",
      "  step 3300 lr 0.001 step-time 0.24s wps 19.55K ppl 11.79 gN 6.28 bleu 13.62, Mon Apr  2 07:18:28 2018\n",
      "  step 3400 lr 0.001 step-time 0.24s wps 19.60K ppl 11.90 gN 6.38 bleu 13.62, Mon Apr  2 07:18:52 2018\n",
      "  step 3500 lr 0.001 step-time 0.24s wps 19.53K ppl 11.88 gN 6.32 bleu 13.62, Mon Apr  2 07:19:15 2018\n",
      "  step 3600 lr 0.001 step-time 0.23s wps 19.54K ppl 11.83 gN 6.41 bleu 13.62, Mon Apr  2 07:19:39 2018\n",
      "  step 3700 lr 0.001 step-time 0.24s wps 19.59K ppl 11.86 gN 6.30 bleu 13.62, Mon Apr  2 07:20:02 2018\n",
      "  step 3800 lr 0.001 step-time 0.24s wps 19.58K ppl 11.92 gN 6.30 bleu 13.62, Mon Apr  2 07:20:26 2018\n",
      "  step 3900 lr 0.001 step-time 0.24s wps 19.57K ppl 11.76 gN 6.33 bleu 13.62, Mon Apr  2 07:20:50 2018\n",
      "  step 4000 lr 0.001 step-time 0.24s wps 19.50K ppl 11.74 gN 6.28 bleu 13.62, Mon Apr  2 07:21:13 2018\n",
      "# Save eval, global step 4000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-4000, time 0.08s\n",
      "  # 504\n",
      "    src: Cám ơn rất nhiều\n",
      "    ref: Thank you very much .\n",
      "    nmt: Thank you very much .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-4000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-4000, time 0.09s\n",
      "  eval dev: perplexity 16.60, time 1s, Mon Apr  2 07:21:15 2018.\n",
      "  eval test: perplexity 16.12, time 1s, Mon Apr  2 07:21:17 2018.\n",
      "  step 4100 lr 0.001 step-time 0.23s wps 19.76K ppl 11.77 gN 6.18 bleu 13.62, Mon Apr  2 07:21:40 2018\n",
      "# Finished an epoch, step 4172. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-4000, time 0.08s\n",
      "  # 556\n",
      "    src: Càng nhiều sự lựa chọn Càng nhiều người gửi tất cả tiền của họ vào những tài khoản thị trường tài chính\n",
      "    ref: The more choices available , the more likely they were to put all their money in pure money market accounts .\n",
      "    nmt: More <unk> choices , people send them all their money into financial markets .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-4000, time 0.08s\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:22:07 2018.\n",
      "  bleu dev: 14.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:22:17 2018.\n",
      "  bleu test: 14.3\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 4200 lr 0.001 step-time 0.36s wps 12.56K ppl 10.87 gN 6.22 bleu 14.03, Mon Apr  2 07:22:36 2018\n",
      "  step 4300 lr 0.001 step-time 0.25s wps 18.30K ppl 9.14 gN 6.01 bleu 14.03, Mon Apr  2 07:23:01 2018\n",
      "  step 4400 lr 0.001 step-time 0.24s wps 19.69K ppl 9.44 gN 6.25 bleu 14.03, Mon Apr  2 07:23:25 2018\n",
      "  step 4500 lr 0.001 step-time 0.24s wps 19.67K ppl 9.63 gN 6.32 bleu 14.03, Mon Apr  2 07:23:49 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 4600 lr 0.001 step-time 0.23s wps 19.54K ppl 9.61 gN 6.30 bleu 14.03, Mon Apr  2 07:24:12 2018\n",
      "  step 4700 lr 0.001 step-time 0.24s wps 19.60K ppl 9.81 gN 6.53 bleu 14.03, Mon Apr  2 07:24:35 2018\n",
      "  step 4800 lr 0.001 step-time 0.24s wps 19.69K ppl 9.71 gN 6.34 bleu 14.03, Mon Apr  2 07:24:59 2018\n",
      "  step 4900 lr 0.001 step-time 0.24s wps 19.47K ppl 9.72 gN 6.31 bleu 14.03, Mon Apr  2 07:25:23 2018\n",
      "  step 5000 lr 0.001 step-time 0.24s wps 19.65K ppl 9.68 gN 6.21 bleu 14.03, Mon Apr  2 07:25:46 2018\n",
      "# Save eval, global step 5000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-5000, time 0.08s\n",
      "  # 535\n",
      "    src: Họ có hơn 348 loại mứt khác nhau .\n",
      "    ref: They had 348 different kinds of jam .\n",
      "    nmt: They have more than <unk> different <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-5000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-5000, time 0.09s\n",
      "  eval dev: perplexity 15.64, time 1s, Mon Apr  2 07:25:48 2018.\n",
      "  eval test: perplexity 15.18, time 1s, Mon Apr  2 07:25:50 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-5000, time 0.08s\n",
      "  # 367\n",
      "    src: và khi tôi bước vào , thẩm phán nhìn thấy tôi đến\n",
      "    ref: And as soon as I walked inside , the judge saw me coming in .\n",
      "    nmt: And when I walked into , the judge looked at me .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-5000, time 0.08s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 07:26:03 2018.\n",
      "  bleu dev: 14.6\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:26:13 2018.\n",
      "  bleu test: 14.8\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 5100 lr 0.001 step-time 0.23s wps 19.50K ppl 9.62 gN 6.13 bleu 14.61, Mon Apr  2 07:26:37 2018\n",
      "  step 5200 lr 0.001 step-time 0.24s wps 19.64K ppl 9.81 gN 6.15 bleu 14.61, Mon Apr  2 07:27:00 2018\n",
      "# Finished an epoch, step 5215. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-5000, time 0.08s\n",
      "  # 10\n",
      "    src: Ông là ông của tôi .\n",
      "    ref: He is my grandfather .\n",
      "    nmt: He was my grandfather .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-5000, time 0.08s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:27:13 2018.\n",
      "  bleu dev: 14.6\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:27:23 2018.\n",
      "  bleu test: 14.8\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 5300 lr 0.001 step-time 0.37s wps 12.10K ppl 8.10 gN 6.66 bleu 14.61, Mon Apr  2 07:27:58 2018\n",
      "  step 5400 lr 0.001 step-time 0.23s wps 19.55K ppl 7.87 gN 6.03 bleu 14.61, Mon Apr  2 07:28:21 2018\n",
      "  step 5500 lr 0.001 step-time 0.23s wps 19.69K ppl 8.10 gN 6.16 bleu 14.61, Mon Apr  2 07:28:45 2018\n",
      "  step 5600 lr 0.001 step-time 0.24s wps 19.64K ppl 8.03 gN 6.26 bleu 14.61, Mon Apr  2 07:29:08 2018\n",
      "  step 5700 lr 0.001 step-time 0.23s wps 19.52K ppl 8.03 gN 6.10 bleu 14.61, Mon Apr  2 07:29:32 2018\n",
      "  step 5800 lr 0.001 step-time 0.24s wps 19.62K ppl 8.31 gN 6.26 bleu 14.61, Mon Apr  2 07:29:55 2018\n",
      "  step 5900 lr 0.001 step-time 0.24s wps 19.63K ppl 8.40 gN 6.26 bleu 14.61, Mon Apr  2 07:30:19 2018\n",
      "  step 6000 lr 0.001 step-time 0.24s wps 19.61K ppl 8.45 gN 6.28 bleu 14.61, Mon Apr  2 07:30:42 2018\n",
      "# Save eval, global step 6000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-6000, time 0.08s\n",
      "  # 737\n",
      "    src: y tá kia hỏi đơn giản như thế .\n",
      "    ref: the other nurse asked matter-of-factly .\n",
      "    nmt: nurse the next nurse is a simple .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-6000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-6000, time 0.09s\n",
      "  eval dev: perplexity 15.32, time 1s, Mon Apr  2 07:30:45 2018.\n",
      "  eval test: perplexity 14.80, time 1s, Mon Apr  2 07:30:46 2018.\n",
      "  step 6100 lr 0.001 step-time 0.24s wps 19.54K ppl 8.51 gN 6.24 bleu 14.61, Mon Apr  2 07:31:10 2018\n",
      "  step 6200 lr 0.001 step-time 0.24s wps 19.44K ppl 8.52 gN 6.30 bleu 14.61, Mon Apr  2 07:31:33 2018\n",
      "# Finished an epoch, step 6258. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-6000, time 0.08s\n",
      "  # 786\n",
      "    src: Bạn có thể tra bệnh này trên Google , nhưng nó là bệnh nhiễm khuẩn , không phải cổ họng , mà là phần trên của khí quản , và nó có thể dẫn đến tắc khí quản .\n",
      "    ref: You can Google it , but it &apos;s an infection , not of the throat , but of the upper airway , and it can actually cause the airway to close .\n",
      "    nmt: You can check this in Google , but it &apos;s the infection infected , not the <unk> , the <unk> of the air , the <unk> of the air .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-6000, time 0.08s\n",
      "# External evaluation, global step 6000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:31:57 2018.\n",
      "  bleu dev: 14.3\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 6000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:32:07 2018.\n",
      "  bleu test: 14.9\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 6300 lr 0.001 step-time 0.37s wps 12.32K ppl 7.64 gN 6.10 bleu 14.61, Mon Apr  2 07:32:31 2018\n",
      "  step 6400 lr 0.001 step-time 0.25s wps 18.67K ppl 6.80 gN 5.99 bleu 14.61, Mon Apr  2 07:32:55 2018\n",
      "  step 6500 lr 0.001 step-time 0.24s wps 19.54K ppl 6.97 gN 6.24 bleu 14.61, Mon Apr  2 07:33:19 2018\n",
      "  step 6600 lr 0.001 step-time 0.23s wps 19.49K ppl 7.03 gN 6.19 bleu 14.61, Mon Apr  2 07:33:43 2018\n",
      "  step 6700 lr 0.001 step-time 0.24s wps 19.47K ppl 7.15 gN 6.30 bleu 14.61, Mon Apr  2 07:34:06 2018\n",
      "  step 6800 lr 0.001 step-time 0.24s wps 19.53K ppl 7.38 gN 6.30 bleu 14.61, Mon Apr  2 07:34:30 2018\n",
      "  step 6900 lr 0.001 step-time 0.24s wps 19.60K ppl 7.27 gN 6.30 bleu 14.61, Mon Apr  2 07:34:54 2018\n",
      "  step 7000 lr 0.001 step-time 0.24s wps 19.52K ppl 7.44 gN 6.30 bleu 14.61, Mon Apr  2 07:35:17 2018\n",
      "# Save eval, global step 7000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-7000, time 0.08s\n",
      "  # 1170\n",
      "    src: Vậy mà chúng ta vẫn không thực sự hiểu sự lựa chọn của nó .\n",
      "    ref: And yet , we don &apos;t quite understand the options of it .\n",
      "    nmt: So we don &apos;t really understand the choice of it .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-7000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-7000, time 0.09s\n",
      "  eval dev: perplexity 15.45, time 1s, Mon Apr  2 07:35:19 2018.\n",
      "  eval test: perplexity 15.02, time 1s, Mon Apr  2 07:35:21 2018.\n",
      "  step 7100 lr 0.001 step-time 0.24s wps 19.53K ppl 7.44 gN 6.32 bleu 14.61, Mon Apr  2 07:35:44 2018\n",
      "  step 7200 lr 0.001 step-time 0.23s wps 19.55K ppl 7.44 gN 6.32 bleu 14.61, Mon Apr  2 07:36:08 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 7300 lr 0.001 step-time 0.23s wps 19.25K ppl 7.51 gN 6.29 bleu 14.61, Mon Apr  2 07:36:31 2018\n",
      "# Finished an epoch, step 7301. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-7000, time 0.08s\n",
      "  # 140\n",
      "    src: bà ôm tôi chặt đến mức tôi thấy khó thở rồi sau đó bà để tôi đi\n",
      "    ref: And she &apos;d squeeze me so tight I could barely breathe and then she &apos;d let me go .\n",
      "    nmt: She was holding my breath down to me , and then she left me with breath after she left .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-7000, time 0.08s\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:36:41 2018.\n",
      "  bleu dev: 14.4\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:36:51 2018.\n",
      "  bleu test: 15.5\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 7400 lr 0.001 step-time 0.38s wps 12.36K ppl 6.11 gN 6.08 bleu 14.61, Mon Apr  2 07:37:29 2018\n",
      "  step 7500 lr 0.001 step-time 0.23s wps 19.54K ppl 6.15 gN 6.21 bleu 14.61, Mon Apr  2 07:37:52 2018\n",
      "  step 7600 lr 0.001 step-time 0.23s wps 19.64K ppl 6.20 gN 6.28 bleu 14.61, Mon Apr  2 07:38:16 2018\n",
      "  step 7700 lr 0.001 step-time 0.24s wps 19.62K ppl 6.36 gN 6.24 bleu 14.61, Mon Apr  2 07:38:39 2018\n",
      "  step 7800 lr 0.001 step-time 0.24s wps 19.63K ppl 6.45 gN 6.38 bleu 14.61, Mon Apr  2 07:39:03 2018\n",
      "  step 7900 lr 0.001 step-time 0.24s wps 19.58K ppl 6.57 gN 6.36 bleu 14.61, Mon Apr  2 07:39:27 2018\n",
      "  step 8000 lr 0.001 step-time 0.23s wps 19.60K ppl 6.53 gN 6.29 bleu 14.61, Mon Apr  2 07:39:50 2018\n",
      "# Save eval, global step 8000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-8000, time 0.08s\n",
      "  # 375\n",
      "    src: Ông luôn nhìn vào qua cánh cửa sổ , và ông có thể nghe hết tất cả những tiếng la hét .\n",
      "    ref: And he kept looking through the window , and he could hear all of this holler .\n",
      "    nmt: He always looked at the window , and he could hear it all the <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-8000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-8000, time 0.10s\n",
      "  eval dev: perplexity 15.83, time 1s, Mon Apr  2 07:39:52 2018.\n",
      "  eval test: perplexity 15.44, time 1s, Mon Apr  2 07:39:54 2018.\n",
      "  step 8100 lr 0.001 step-time 0.24s wps 19.55K ppl 6.62 gN 6.29 bleu 14.61, Mon Apr  2 07:40:17 2018\n",
      "  step 8200 lr 0.001 step-time 0.24s wps 19.45K ppl 6.74 gN 6.30 bleu 14.61, Mon Apr  2 07:40:41 2018\n",
      "  step 8300 lr 0.001 step-time 0.24s wps 19.56K ppl 6.79 gN 6.44 bleu 14.61, Mon Apr  2 07:41:05 2018\n",
      "# Finished an epoch, step 8344. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-8000, time 0.08s\n",
      "  # 909\n",
      "    src: Ai có thể dự đoán &quot; Double Rainbow &quot; hay Rebecca Black hay &quot; Nyan Cat &quot;\n",
      "    ref: Who could have predicted &quot; Double Rainbow &quot; or Rebecca Black or &quot; Nyan Cat ? &quot;\n",
      "    nmt: Who could predict &quot; <unk> <unk> &quot; or &quot; Rebecca <unk> . &quot;\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-8000, time 0.08s\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:41:24 2018.\n",
      "  bleu dev: 14.7\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:41:35 2018.\n",
      "  bleu test: 15.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 8400 lr 0.001 step-time 0.37s wps 12.16K ppl 5.88 gN 6.08 bleu 14.75, Mon Apr  2 07:42:02 2018\n",
      "  step 8500 lr 0.001 step-time 0.24s wps 19.31K ppl 5.52 gN 6.07 bleu 14.75, Mon Apr  2 07:42:26 2018\n",
      "  step 8600 lr 0.001 step-time 0.23s wps 19.57K ppl 5.56 gN 6.14 bleu 14.75, Mon Apr  2 07:42:49 2018\n",
      "  step 8700 lr 0.001 step-time 0.24s wps 19.68K ppl 5.77 gN 6.40 bleu 14.75, Mon Apr  2 07:43:13 2018\n",
      "  step 8800 lr 0.001 step-time 0.24s wps 19.56K ppl 5.75 gN 6.32 bleu 14.75, Mon Apr  2 07:43:36 2018\n",
      "  step 8900 lr 0.001 step-time 0.23s wps 19.55K ppl 5.90 gN 6.40 bleu 14.75, Mon Apr  2 07:44:00 2018\n",
      "  step 9000 lr 0.001 step-time 0.24s wps 19.59K ppl 6.02 gN 6.45 bleu 14.75, Mon Apr  2 07:44:23 2018\n",
      "# Save eval, global step 9000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-9000, time 0.08s\n",
      "  # 612\n",
      "    src: Một thứ được gọi là &quot; Jazz &quot; còn cái kia được gọi là &quot; Swing &quot;\n",
      "    ref: One is called &quot; Jazz &quot; and the other one is called &quot; Swing . &quot;\n",
      "    nmt: One of the things called &quot; <unk> &quot; that was called &quot; <unk> . &quot;\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-9000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-9000, time 0.09s\n",
      "  eval dev: perplexity 16.56, time 1s, Mon Apr  2 07:44:26 2018.\n",
      "  eval test: perplexity 15.83, time 1s, Mon Apr  2 07:44:27 2018.\n",
      "  step 9100 lr 0.0005 step-time 0.24s wps 19.58K ppl 5.84 gN 6.28 bleu 14.75, Mon Apr  2 07:44:51 2018\n",
      "  step 9200 lr 0.0005 step-time 0.23s wps 19.49K ppl 5.80 gN 6.13 bleu 14.75, Mon Apr  2 07:45:14 2018\n",
      "  step 9300 lr 0.0005 step-time 0.23s wps 19.54K ppl 5.71 gN 6.12 bleu 14.75, Mon Apr  2 07:45:38 2018\n",
      "# Finished an epoch, step 9387. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-9000, time 0.08s\n",
      "  # 1039\n",
      "    src: Trạng thái mới mà thế giới đang hướng tới là gì ?\n",
      "    ref: What &apos;s that new state that the world is heading toward ?\n",
      "    nmt: What is the new <unk> world ?\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-9000, time 0.08s\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 07:46:08 2018.\n",
      "  bleu dev: 14.2\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:46:18 2018.\n",
      "  bleu test: 14.2\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 9400 lr 0.0005 step-time 0.35s wps 12.89K ppl 5.64 gN 6.22 bleu 14.75, Mon Apr  2 07:46:34 2018\n",
      "  step 9500 lr 0.0005 step-time 0.26s wps 17.86K ppl 4.70 gN 5.85 bleu 14.75, Mon Apr  2 07:47:00 2018\n",
      "  step 9600 lr 0.0005 step-time 0.23s wps 19.53K ppl 4.74 gN 5.88 bleu 14.75, Mon Apr  2 07:47:23 2018\n",
      "  step 9700 lr 0.0005 step-time 0.23s wps 19.65K ppl 4.83 gN 6.01 bleu 14.75, Mon Apr  2 07:47:46 2018\n",
      "  step 9800 lr 0.0005 step-time 0.24s wps 19.59K ppl 4.91 gN 6.09 bleu 14.75, Mon Apr  2 07:48:10 2018\n",
      "  step 9900 lr 0.0005 step-time 0.24s wps 19.64K ppl 4.96 gN 6.13 bleu 14.75, Mon Apr  2 07:48:34 2018\n",
      "  step 10000 lr 0.0005 step-time 0.24s wps 19.51K ppl 4.99 gN 6.13 bleu 14.75, Mon Apr  2 07:48:57 2018\n",
      "# Save eval, global step 10000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-10000, time 0.08s\n",
      "  # 1342\n",
      "    src: Một thứ khác nữa là chúng ta biết tất cả các trạm xăng ở đâu .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ref: The other thing is we know where all the gas stations are .\n",
      "    nmt: Another thing is we know , all of the gas stations .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-10000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-10000, time 0.10s\n",
      "  eval dev: perplexity 16.40, time 1s, Mon Apr  2 07:49:00 2018.\n",
      "  eval test: perplexity 15.77, time 1s, Mon Apr  2 07:49:01 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-10000, time 0.08s\n",
      "  # 826\n",
      "    src: Chúng ta có những định kiến dựa vào kinh nghiệm sẵn có , ví dụ tôi có thể chấp nhận là một bệnh nhân đau ngực có tiền sử bệnh hoàn hảo .\n",
      "    ref: We have our cognitive biases , so that I can take a perfect history on a patient with chest pain .\n",
      "    nmt: We have these stereotypes that are available , for example , to accept that patient &apos;s <unk> is perfect , and I can tell you , I &apos;m a patient with a\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-10000, time 0.08s\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 07:49:14 2018.\n",
      "  bleu dev: 14.7\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:49:24 2018.\n",
      "  bleu test: 15.3\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 10100 lr 0.00025 step-time 0.23s wps 19.61K ppl 4.88 gN 6.06 bleu 14.75, Mon Apr  2 07:49:48 2018\n",
      "  step 10200 lr 0.00025 step-time 0.23s wps 19.61K ppl 4.91 gN 6.03 bleu 14.75, Mon Apr  2 07:50:11 2018\n",
      "  step 10300 lr 0.00025 step-time 0.23s wps 19.65K ppl 4.94 gN 6.10 bleu 14.75, Mon Apr  2 07:50:35 2018\n",
      "  step 10400 lr 0.00025 step-time 0.24s wps 19.61K ppl 4.90 gN 6.10 bleu 14.75, Mon Apr  2 07:50:58 2018\n",
      "# Finished an epoch, step 10430. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-10000, time 0.08s\n",
      "  # 615\n",
      "    src: Còn nếu bạn nghĩ bên trái là Jazz và bên phải là Swing thì xin mời bạn vỗ tay .\n",
      "    ref: If you think the one on the left is Jazz and the one on the right is Swing , clap your hands .\n",
      "    nmt: And if you think the left is a little bit , you &apos;re a <unk> and you &apos;re going to ask your hand .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-10000, time 0.08s\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:51:15 2018.\n",
      "  bleu dev: 14.7\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:51:25 2018.\n",
      "  bleu test: 15.3\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 10500 lr 0.00025 step-time 0.37s wps 12.06K ppl 4.56 gN 6.00 bleu 14.75, Mon Apr  2 07:51:56 2018\n",
      "  step 10600 lr 0.00025 step-time 0.24s wps 19.69K ppl 4.45 gN 5.92 bleu 14.75, Mon Apr  2 07:52:19 2018\n",
      "  step 10700 lr 0.00025 step-time 0.23s wps 19.43K ppl 4.38 gN 5.84 bleu 14.75, Mon Apr  2 07:52:43 2018\n",
      "  step 10800 lr 0.00025 step-time 0.24s wps 19.64K ppl 4.45 gN 6.01 bleu 14.75, Mon Apr  2 07:53:06 2018\n",
      "  step 10900 lr 0.00025 step-time 0.23s wps 19.64K ppl 4.48 gN 6.00 bleu 14.75, Mon Apr  2 07:53:30 2018\n",
      "  step 11000 lr 0.00025 step-time 0.24s wps 19.53K ppl 4.49 gN 6.01 bleu 14.75, Mon Apr  2 07:53:54 2018\n",
      "# Save eval, global step 11000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-11000, time 0.08s\n",
      "  # 1216\n",
      "    src: Việc cho ra đời một phần mềm thường mất vài năm .\n",
      "    ref: Procuring software usually takes a couple of years .\n",
      "    nmt: It &apos;s about a regular software that takes a few years .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-11000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-11000, time 0.10s\n",
      "  eval dev: perplexity 16.76, time 1s, Mon Apr  2 07:53:56 2018.\n",
      "  eval test: perplexity 16.10, time 1s, Mon Apr  2 07:53:57 2018.\n",
      "  step 11100 lr 0.000125 step-time 0.23s wps 19.43K ppl 4.47 gN 5.97 bleu 14.75, Mon Apr  2 07:54:21 2018\n",
      "  step 11200 lr 0.000125 step-time 0.24s wps 19.60K ppl 4.55 gN 6.05 bleu 14.75, Mon Apr  2 07:54:44 2018\n",
      "  step 11300 lr 0.000125 step-time 0.24s wps 19.53K ppl 4.50 gN 6.04 bleu 14.75, Mon Apr  2 07:55:08 2018\n",
      "  step 11400 lr 0.000125 step-time 0.24s wps 19.55K ppl 4.51 gN 6.03 bleu 14.75, Mon Apr  2 07:55:32 2018\n",
      "# Finished an epoch, step 11473. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-11000, time 0.08s\n",
      "  # 987\n",
      "    src: Và thế là chũng ta mang họ trở về một cuốn sách thiếu nhi hiện đại .\n",
      "    ref: And so we &apos;re bringing them back in a contemporary story for children .\n",
      "    nmt: And so we brought them back to a <unk> book .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-11000, time 0.08s\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:55:58 2018.\n",
      "  bleu dev: 14.7\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 07:56:08 2018.\n",
      "  bleu test: 15.1\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "  step 11500 lr 0.000125 step-time 0.36s wps 12.52K ppl 4.37 gN 6.01 bleu 14.75, Mon Apr  2 07:56:27 2018\n",
      "  step 11600 lr 0.000125 step-time 0.26s wps 18.23K ppl 4.21 gN 5.93 bleu 14.75, Mon Apr  2 07:56:53 2018\n",
      "  step 11700 lr 0.000125 step-time 0.23s wps 19.53K ppl 4.22 gN 5.88 bleu 14.75, Mon Apr  2 07:57:16 2018\n",
      "  step 11800 lr 0.000125 step-time 0.24s wps 19.61K ppl 4.25 gN 5.94 bleu 14.75, Mon Apr  2 07:57:40 2018\n",
      "  step 11900 lr 0.000125 step-time 0.23s wps 19.45K ppl 4.23 gN 5.90 bleu 14.75, Mon Apr  2 07:58:03 2018\n",
      "  step 12000 lr 0.000125 step-time 0.24s wps 19.59K ppl 4.33 gN 6.07 bleu 14.75, Mon Apr  2 07:58:27 2018\n",
      "# Save eval, global step 12000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-12000, time 0.08s\n",
      "  # 461\n",
      "    src: Một lần chúng tôi bắt được một con còn sống .\n",
      "    ref: And one time we caught a live one .\n",
      "    nmt: One once we were caught was a living .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-12000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-12000, time 0.10s\n",
      "  eval dev: perplexity 16.95, time 1s, Mon Apr  2 07:58:30 2018.\n",
      "  eval test: perplexity 16.34, time 1s, Mon Apr  2 07:58:31 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-12000, time 0.08s\n",
      "  # 310\n",
      "    src: cố gắng làm được việc gì đó về án tử hình ,\n",
      "    ref: We &apos;re trying to do something about the death penalty .\n",
      "    nmt: I was trying to do something about the death project .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-12000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/translate.ckpt-12000, time 0.10s\n",
      "  eval dev: perplexity 16.95, time 1s, Mon Apr  2 07:58:35 2018.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval test: perplexity 16.34, time 1s, Mon Apr  2 07:58:37 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/translate.ckpt-12000, time 0.08s\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:58:46 2018.\n",
      "  bleu dev: 14.6\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 07:58:55 2018.\n",
      "  bleu test: 15.2\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# Final, step 12000 lr 0.000125 step-time 0.24s wps 19.59K ppl 4.33 gN 6.07 dev ppl 16.95, dev bleu 14.6, test ppl 16.34, test bleu 15.2, Mon Apr  2 07:58:56 2018\n",
      "# Done training!, time 3328s, Mon Apr  2 07:58:56 2018.\n",
      "# Start evaluating saved best models.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-8000, time 0.08s\n",
      "  # 1027\n",
      "    src: Vậy , nếu công nghệ vận tải phát triển nhanh như công nghệ vi xử lý , thì ngày kia tôi có thể gọi một chiếc taxi và đến Tokyo trong vòng 30 giây .\n",
      "    ref: So if transportation technology was moving along as fast as microprocessor technology , then the day after tomorrow , I would be able to get in a taxi cab and be in Tokyo in 30 seconds .\n",
      "    nmt: So , if you take a rapid rate like the <unk> processor , I can call a taxi and I went to Tokyo , and I went to Tokyo .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-8000\n",
      "  loaded eval model parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-8000, time 0.09s\n",
      "  eval dev: perplexity 15.83, time 1s, Mon Apr  2 07:58:58 2018.\n",
      "  eval test: perplexity 15.44, time 1s, Mon Apr  2 07:58:59 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_dotprodatt/best_bleu/translate.ckpt-8000, time 0.08s\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_dotprodatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 9s, Mon Apr  2 07:59:09 2018.\n",
      "  bleu dev: 14.7\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_dotprodatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 8s, Mon Apr  2 07:59:18 2018.\n",
      "  bleu test: 15.0\n",
      "  saving hparams to nmt_model_dotprodatt/hparams\n",
      "# Best bleu, step 8000 lr 0.000125 step-time 0.24s wps 19.59K ppl 4.33 gN 6.07 dev ppl 15.83, dev bleu 14.7, test ppl 15.44, test bleu 15.0, Mon Apr  2 07:59:18 2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dev_ppl': 16.94916214271354,\n",
       "  'dev_scores': {'bleu': 14.60982733339068},\n",
       "  'test_ppl': 16.34162359243638,\n",
       "  'test_scores': {'bleu': 15.20867011933523}},\n",
       " 12000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an LSTM model with dot-product attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "                                  out_dir=\"nmt_model_dotprodatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithDotProductAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab file datasets/nmt_data_vi/vocab.vi exists\n",
      "# Vocab file datasets/nmt_data_vi/vocab.en exists\n",
      "# creating train graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0  DropoutWrapper, dropout=0.2   DropoutWrapper  DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=luong234, start_decay_step=8000, decay_steps 1000, decay_factor 0.5\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_att:0, (512, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating eval graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTMCellWithBilinearAttention, dropout=0   LSTMCellWithBilinearAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_att:0, (512, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating infer graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTMCellWithBilinearAttention, dropout=0   LSTMCellWithBilinearAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_att:0, (512, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_bilinear_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "  created train model with fresh parameters, time 0.42s\n",
      "  created infer model with fresh parameters, time 0.08s\n",
      "  # 728\n",
      "    src: Và tôi quay lại làm việc ở phòng khám .\n",
      "    ref: And I went back to my work on the wards .\n",
      "    nmt: fancy Southern Madagascar reporting careers chamber tucked sin 73 changer fitness explicit receptor receptor support receptor receptor support hits hits\n",
      "  created eval model with fresh parameters, time 0.14s\n",
      "  eval dev: perplexity 17379.42, time 2s, Mon Apr  2 08:42:38 2018.\n",
      "  eval test: perplexity 17389.10, time 2s, Mon Apr  2 08:42:40 2018.\n",
      "  created infer model with fresh parameters, time 0.06s\n",
      "# Start step 0, lr 0.001, Mon Apr  2 08:42:40 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 0.001 step-time 0.52s wps 8.86K ppl 595.00 gN 16.13 bleu 0.00, Mon Apr  2 08:43:32 2018\n",
      "  step 200 lr 0.001 step-time 0.39s wps 11.80K ppl 268.49 gN 10.22 bleu 0.00, Mon Apr  2 08:44:12 2018\n",
      "  step 300 lr 0.001 step-time 0.39s wps 11.76K ppl 174.19 gN 9.56 bleu 0.00, Mon Apr  2 08:44:51 2018\n",
      "  step 400 lr 0.001 step-time 0.39s wps 11.77K ppl 139.66 gN 8.22 bleu 0.00, Mon Apr  2 08:45:30 2018\n",
      "  step 500 lr 0.001 step-time 0.40s wps 11.77K ppl 113.93 gN 8.87 bleu 0.00, Mon Apr  2 08:46:09 2018\n",
      "  step 600 lr 0.001 step-time 0.39s wps 11.73K ppl 92.03 gN 8.36 bleu 0.00, Mon Apr  2 08:46:48 2018\n",
      "  step 700 lr 0.001 step-time 0.39s wps 11.69K ppl 76.45 gN 7.80 bleu 0.00, Mon Apr  2 08:47:28 2018\n",
      "  step 800 lr 0.001 step-time 0.39s wps 11.72K ppl 67.64 gN 7.89 bleu 0.00, Mon Apr  2 08:48:07 2018\n",
      "  step 900 lr 0.001 step-time 0.39s wps 11.76K ppl 57.55 gN 7.50 bleu 0.00, Mon Apr  2 08:48:46 2018\n",
      "  step 1000 lr 0.001 step-time 0.40s wps 11.75K ppl 52.00 gN 7.56 bleu 0.00, Mon Apr  2 08:49:26 2018\n",
      "# Save eval, global step 1000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-1000, time 0.08s\n",
      "  # 947\n",
      "    src: Kết luận chúng tôi rút ra là phép màu đã &apos; được &apos; thay thế bằng máy móc .\n",
      "    ref: The conclusion that we came to was that magic had been replaced by machinery .\n",
      "    nmt: The <unk> we &apos;ve been to be able to be <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-1000, time 0.10s\n",
      "  eval dev: perplexity 46.71, time 2s, Mon Apr  2 08:49:29 2018.\n",
      "  eval test: perplexity 50.98, time 2s, Mon Apr  2 08:49:32 2018.\n",
      "# Finished an epoch, step 1043. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-1000, time 0.08s\n",
      "  # 1038\n",
      "    src: Và vì vậy , điều mà tôi đang muốn hỏi , điều mà tôi vẫn đang tự hỏi mình , là trạng thái mới mà thế giới đang tồn tại là gì ?\n",
      "    ref: And so what I &apos;m trying to ask , what I &apos;ve been asking myself , is what &apos;s this new way that the world is ?\n",
      "    nmt: And so , what I &apos;m going to ask , what I &apos;m going to ask you , is that the world &apos;s really important ?\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-1000, time 0.08s\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 08:49:58 2018.\n",
      "  bleu dev: 6.8\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 08:50:09 2018.\n",
      "  bleu test: 6.3\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 1100 lr 0.001 step-time 0.51s wps 8.75K ppl 45.67 gN 8.22 bleu 6.79, Mon Apr  2 08:50:44 2018\n",
      "  step 1200 lr 0.001 step-time 0.39s wps 11.76K ppl 39.12 gN 7.02 bleu 6.79, Mon Apr  2 08:51:23 2018\n",
      "  step 1300 lr 0.001 step-time 0.39s wps 11.79K ppl 36.86 gN 7.38 bleu 6.79, Mon Apr  2 08:52:03 2018\n",
      "  step 1400 lr 0.001 step-time 0.39s wps 11.75K ppl 34.24 gN 7.18 bleu 6.79, Mon Apr  2 08:52:42 2018\n",
      "  step 1500 lr 0.001 step-time 0.39s wps 11.75K ppl 31.95 gN 7.14 bleu 6.79, Mon Apr  2 08:53:21 2018\n",
      "  step 1600 lr 0.001 step-time 0.39s wps 11.75K ppl 29.66 gN 7.01 bleu 6.79, Mon Apr  2 08:54:00 2018\n",
      "  step 1700 lr 0.001 step-time 0.40s wps 11.78K ppl 28.60 gN 7.10 bleu 6.79, Mon Apr  2 08:54:40 2018\n",
      "  step 1800 lr 0.001 step-time 0.39s wps 11.73K ppl 26.32 gN 7.15 bleu 6.79, Mon Apr  2 08:55:19 2018\n",
      "  step 1900 lr 0.001 step-time 0.39s wps 11.75K ppl 25.47 gN 7.99 bleu 6.79, Mon Apr  2 08:55:58 2018\n",
      "  step 2000 lr 0.001 step-time 0.39s wps 11.79K ppl 24.38 gN 7.12 bleu 6.79, Mon Apr  2 08:56:37 2018\n",
      "# Save eval, global step 2000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-2000, time 0.08s\n",
      "  # 1437\n",
      "    src: Giống như viên gạch bê tông , transistor cho phép bạn xây những mạch điện lớn và phức tạp hơn , từng viên gạch một .\n",
      "    ref: Like the concrete block , the transistor allows you to build much larger , more complex circuits , one brick at a time .\n",
      "    nmt: Like the <unk> , the <unk> for you , the <unk> you can build the <unk> and more complex .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-2000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-2000, time 0.09s\n",
      "  eval dev: perplexity 24.88, time 2s, Mon Apr  2 08:56:40 2018.\n",
      "  eval test: perplexity 25.35, time 2s, Mon Apr  2 08:56:43 2018.\n",
      "# Finished an epoch, step 2086. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-2000, time 0.08s\n",
      "  # 1378\n",
      "    src: Nên nó thực sự là việc nắm bắt ý tưởng hơn là nắm bắt một khoảnh khắc .\n",
      "    ref: So it &apos;s more about capturing an idea than about capturing a moment really .\n",
      "    nmt: So it &apos;s really a more <unk> thing that &apos;s going to be more like that .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-2000, time 0.08s\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 08:57:27 2018.\n",
      "  bleu dev: 11.2\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 08:57:37 2018.\n",
      "  bleu test: 11.2\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 2100 lr 0.001 step-time 0.50s wps 8.97K ppl 23.09 gN 7.10 bleu 11.25, Mon Apr  2 08:57:55 2018\n",
      "  step 2200 lr 0.001 step-time 0.40s wps 11.58K ppl 19.11 gN 7.47 bleu 11.25, Mon Apr  2 08:58:35 2018\n",
      "  step 2300 lr 0.001 step-time 0.39s wps 11.79K ppl 18.63 gN 7.12 bleu 11.25, Mon Apr  2 08:59:14 2018\n",
      "  step 2400 lr 0.001 step-time 0.39s wps 11.74K ppl 18.10 gN 7.26 bleu 11.25, Mon Apr  2 08:59:53 2018\n",
      "  step 2500 lr 0.001 step-time 0.39s wps 11.76K ppl 17.67 gN 7.24 bleu 11.25, Mon Apr  2 09:00:33 2018\n",
      "  step 2600 lr 0.001 step-time 0.40s wps 11.76K ppl 17.60 gN 7.11 bleu 11.25, Mon Apr  2 09:01:12 2018\n",
      "  step 2700 lr 0.001 step-time 0.39s wps 11.78K ppl 17.68 gN 7.88 bleu 11.25, Mon Apr  2 09:01:51 2018\n",
      "  step 2800 lr 0.001 step-time 0.39s wps 11.76K ppl 16.60 gN 7.02 bleu 11.25, Mon Apr  2 09:02:30 2018\n",
      "  step 2900 lr 0.001 step-time 0.39s wps 11.73K ppl 16.57 gN 6.99 bleu 11.25, Mon Apr  2 09:03:10 2018\n",
      "  step 3000 lr 0.001 step-time 0.39s wps 11.75K ppl 16.26 gN 7.05 bleu 11.25, Mon Apr  2 09:03:49 2018\n",
      "# Save eval, global step 3000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-3000, time 0.08s\n",
      "  # 1201\n",
      "    src: Mỗi lần làm như vậy , bạn phải đặt tên cho nó , và anh ấy đặt tên chiếc đầu tiên là Al .\n",
      "    ref: If you do , you get to name it , and he called the first one Al .\n",
      "    nmt: Every time , you have to put it , and he put the first one of the first Al name .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-3000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-3000, time 0.09s\n",
      "  eval dev: perplexity 19.15, time 2s, Mon Apr  2 09:03:52 2018.\n",
      "  eval test: perplexity 19.02, time 2s, Mon Apr  2 09:03:55 2018.\n",
      "  step 3100 lr 0.001 step-time 0.39s wps 11.75K ppl 15.74 gN 7.02 bleu 11.25, Mon Apr  2 09:04:34 2018\n",
      "# Finished an epoch, step 3129. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-3000, time 0.08s\n",
      "  # 744\n",
      "    src: Họ cho bà thuốc làm tăng huyết áp .\n",
      "    ref: They gave her medications to raise her blood pressure .\n",
      "    nmt: They <unk> her to the <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-3000, time 0.08s\n",
      "# External evaluation, global step 3000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:04:54 2018.\n",
      "  bleu dev: 13.3\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 3000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:05:04 2018.\n",
      "  bleu test: 14.0\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 3200 lr 0.001 step-time 0.51s wps 8.83K ppl 13.24 gN 6.95 bleu 13.28, Mon Apr  2 09:05:45 2018\n",
      "  step 3300 lr 0.001 step-time 0.39s wps 11.82K ppl 12.61 gN 7.07 bleu 13.28, Mon Apr  2 09:06:24 2018\n",
      "  step 3400 lr 0.001 step-time 0.39s wps 11.74K ppl 12.66 gN 7.22 bleu 13.28, Mon Apr  2 09:07:03 2018\n",
      "  step 3500 lr 0.001 step-time 0.39s wps 11.82K ppl 12.64 gN 7.21 bleu 13.28, Mon Apr  2 09:07:42 2018\n",
      "  step 3600 lr 0.001 step-time 0.39s wps 11.74K ppl 12.55 gN 7.07 bleu 13.28, Mon Apr  2 09:08:21 2018\n",
      "  step 3700 lr 0.001 step-time 0.39s wps 11.71K ppl 12.39 gN 6.96 bleu 13.28, Mon Apr  2 09:09:01 2018\n",
      "  step 3800 lr 0.001 step-time 0.39s wps 11.73K ppl 12.61 gN 7.09 bleu 13.28, Mon Apr  2 09:09:40 2018\n",
      "  step 3900 lr 0.001 step-time 0.40s wps 11.73K ppl 12.45 gN 7.06 bleu 13.28, Mon Apr  2 09:10:20 2018\n",
      "  step 4000 lr 0.001 step-time 0.39s wps 11.70K ppl 12.22 gN 6.96 bleu 13.28, Mon Apr  2 09:10:59 2018\n",
      "# Save eval, global step 4000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-4000, time 0.08s\n",
      "  # 1459\n",
      "    src: Chúng tôi muốn tích hợp mọi tương tác trên thế giới vào những &quot; viên gạch &quot; có thể sử dụng dễ dàng .\n",
      "    ref: We want to make every single interaction in the world into a ready-to-use brick .\n",
      "    nmt: We want to combine all the interactions on the world in the <unk> , and they can use it easy .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-4000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-4000, time 0.09s\n",
      "  eval dev: perplexity 17.56, time 2s, Mon Apr  2 09:11:02 2018.\n",
      "  eval test: perplexity 17.22, time 2s, Mon Apr  2 09:11:05 2018.\n",
      "  step 4100 lr 0.001 step-time 0.39s wps 11.72K ppl 12.13 gN 7.39 bleu 13.28, Mon Apr  2 09:11:44 2018\n",
      "# Finished an epoch, step 4172. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-4000, time 0.08s\n",
      "  # 447\n",
      "    src: trong những năm 70-- , à không , bắt đầu từ những năm 60-- Châu Âu thực hiện rất nhiều các dự án phát triển\n",
      "    ref: In the &apos; 70s -- well , beginning in the &apos; 60s -- Europe did lots of development projects .\n",
      "    nmt: In <unk> , the <unk> , starting from the <unk> of Europe , was so many of the projects that were going to have been\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-4000, time 0.08s\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:12:22 2018.\n",
      "  bleu dev: 14.8\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:12:32 2018.\n",
      "  bleu test: 15.7\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 4200 lr 0.001 step-time 0.51s wps 8.89K ppl 11.25 gN 7.00 bleu 14.81, Mon Apr  2 09:12:56 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 4300 lr 0.001 step-time 0.39s wps 11.75K ppl 9.76 gN 6.94 bleu 14.81, Mon Apr  2 09:13:35 2018\n",
      "  step 4400 lr 0.001 step-time 0.39s wps 11.74K ppl 9.72 gN 6.94 bleu 14.81, Mon Apr  2 09:14:14 2018\n",
      "  step 4500 lr 0.001 step-time 0.39s wps 11.78K ppl 9.92 gN 7.11 bleu 14.81, Mon Apr  2 09:14:53 2018\n",
      "  step 4600 lr 0.001 step-time 0.39s wps 11.77K ppl 10.01 gN 7.10 bleu 14.81, Mon Apr  2 09:15:32 2018\n",
      "  step 4700 lr 0.001 step-time 0.39s wps 11.77K ppl 9.94 gN 7.14 bleu 14.81, Mon Apr  2 09:16:11 2018\n",
      "  step 4800 lr 0.001 step-time 0.39s wps 11.75K ppl 10.16 gN 6.96 bleu 14.81, Mon Apr  2 09:16:51 2018\n",
      "  step 4900 lr 0.001 step-time 0.39s wps 11.72K ppl 9.95 gN 7.03 bleu 14.81, Mon Apr  2 09:17:30 2018\n",
      "  step 5000 lr 0.001 step-time 0.40s wps 11.63K ppl 10.25 gN 7.01 bleu 14.81, Mon Apr  2 09:18:10 2018\n",
      "# Save eval, global step 5000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-5000, time 0.08s\n",
      "  # 1153\n",
      "    src: Có thể đưa ra vài sự hoán đổi .\n",
      "    ref: Introduce some mutations perhaps .\n",
      "    nmt: Maybe some of the details .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-5000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-5000, time 0.09s\n",
      "  eval dev: perplexity 16.52, time 2s, Mon Apr  2 09:18:13 2018.\n",
      "  eval test: perplexity 16.27, time 2s, Mon Apr  2 09:18:16 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-5000, time 0.08s\n",
      "  # 741\n",
      "    src: Chừng một giờ sau khi bà về nhà , sau khi tôi cho bà về nhà , bà quỵ ngã và gia đình bà gọi cấp cứu và mọi người đưa bà quay lại phòng cấp cứu và huyết áp bà chỉ là 50 , trong ngưỡng sốc nghiêm trọng .\n",
      "    ref: About an hour after she had arrived home , after I &apos;d sent her home , she collapsed and her family called 911 and the paramedics brought her back to the emergency department where she had a blood pressure of 50 , which is in severe shock .\n",
      "    nmt: It was a hour after she was about home , after I gave her home , she was <unk> and she said , &quot; <unk> , I &apos;m going to give you 50 , in the <unk> , and I &apos;m not going to\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-5000, time 0.08s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:18:29 2018.\n",
      "  bleu dev: 15.2\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:18:39 2018.\n",
      "  bleu test: 16.3\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 5100 lr 0.001 step-time 0.39s wps 11.79K ppl 10.07 gN 6.91 bleu 15.16, Mon Apr  2 09:19:18 2018\n",
      "  step 5200 lr 0.001 step-time 0.40s wps 11.62K ppl 10.10 gN 6.98 bleu 15.16, Mon Apr  2 09:19:58 2018\n",
      "# Finished an epoch, step 5215. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-5000, time 0.08s\n",
      "  # 1082\n",
      "    src: Một số loại vi khuẩn tìm ra cách kháng thuốc penicillin , và nó lan toả ra xung quanh trao đổi thông tin ADN của chúng với các vi khuẩn khác , và ngày nay chúng ta có rất nhiều loại vi khuẩn có khả năng kháng lại penicillin , bởi vì vi khuẩn truyền đạt thông tin ADN .\n",
      "    ref: Some bacteria figured out how to stay away from penicillin , and it went around sort of creating its little DNA information with other bacteria , and now we have a lot of bacteria that are resistant to penicillin , because bacteria communicate .\n",
      "    nmt: Some bacteria find the way to antibiotics , and it spreads around the <unk> of the <unk> , and the bacteria that we can get to the <unk> of the <unk> , and we have a lot of different kinds of bacteria .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-5000, time 0.08s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:20:13 2018.\n",
      "  bleu dev: 15.2\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:20:23 2018.\n",
      "  bleu test: 16.3\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 5300 lr 0.001 step-time 0.51s wps 8.83K ppl 8.38 gN 7.07 bleu 15.16, Mon Apr  2 09:21:10 2018\n",
      "  step 5400 lr 0.001 step-time 0.39s wps 11.69K ppl 8.11 gN 6.87 bleu 15.16, Mon Apr  2 09:21:49 2018\n",
      "  step 5500 lr 0.001 step-time 0.39s wps 11.73K ppl 8.19 gN 6.90 bleu 15.16, Mon Apr  2 09:22:28 2018\n",
      "  step 5600 lr 0.001 step-time 0.39s wps 11.71K ppl 8.34 gN 6.99 bleu 15.16, Mon Apr  2 09:23:07 2018\n",
      "  step 5700 lr 0.001 step-time 0.40s wps 11.72K ppl 8.42 gN 7.07 bleu 15.16, Mon Apr  2 09:23:47 2018\n",
      "  step 5800 lr 0.001 step-time 0.39s wps 11.67K ppl 8.37 gN 7.00 bleu 15.16, Mon Apr  2 09:24:26 2018\n",
      "  step 5900 lr 0.001 step-time 0.39s wps 11.66K ppl 8.52 gN 6.98 bleu 15.16, Mon Apr  2 09:25:06 2018\n",
      "  step 6000 lr 0.001 step-time 0.39s wps 11.68K ppl 8.65 gN 7.03 bleu 15.16, Mon Apr  2 09:25:45 2018\n",
      "# Save eval, global step 6000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-6000, time 0.08s\n",
      "  # 657\n",
      "    src: Ồ , tại sao không ?\n",
      "    ref: Hey , why not ?\n",
      "    nmt: Oh , why not ?\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-6000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-6000, time 0.10s\n",
      "  eval dev: perplexity 15.97, time 2s, Mon Apr  2 09:25:49 2018.\n",
      "  eval test: perplexity 15.53, time 2s, Mon Apr  2 09:25:51 2018.\n",
      "  step 6100 lr 0.001 step-time 0.40s wps 11.69K ppl 8.73 gN 7.00 bleu 15.16, Mon Apr  2 09:26:31 2018\n",
      "  step 6200 lr 0.001 step-time 0.39s wps 11.71K ppl 8.79 gN 7.06 bleu 15.16, Mon Apr  2 09:27:10 2018\n",
      "# Finished an epoch, step 6258. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-6000, time 0.08s\n",
      "  # 1184\n",
      "    src: Một vài năm trước tôi bắt đầu một chương trình tìm kiếm những siêu sao công nghệ và tạo điều kiện cho họ nghỉ một năm và làm việc trong môi trường dường như họ sẽ rất ghét ; họ phải làm việc cho chính phủ .\n",
      "    ref: So a couple of years ago I started a program to try to get the rockstar tech and design people to take a year off and work in the one environment that represents pretty much everything they &apos;re supposed to hate ; we have them work in government .\n",
      "    nmt: A few years ago I started a program looking for these things , and they were making conditions for them to go for a year and do the same thing they were going to be very <unk> , and they were working on\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-6000, time 0.08s\n",
      "# External evaluation, global step 6000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:27:43 2018.\n",
      "  bleu dev: 15.5\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 6000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:27:53 2018.\n",
      "  bleu test: 16.4\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 6300 lr 0.001 step-time 0.51s wps 8.73K ppl 7.88 gN 6.92 bleu 15.54, Mon Apr  2 09:28:22 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 6400 lr 0.001 step-time 0.39s wps 11.73K ppl 6.86 gN 6.71 bleu 15.54, Mon Apr  2 09:29:02 2018\n",
      "  step 6500 lr 0.001 step-time 0.40s wps 11.72K ppl 7.14 gN 6.97 bleu 15.54, Mon Apr  2 09:29:41 2018\n",
      "  step 6600 lr 0.001 step-time 0.39s wps 11.67K ppl 7.22 gN 6.95 bleu 15.54, Mon Apr  2 09:30:21 2018\n",
      "  step 6700 lr 0.001 step-time 0.39s wps 11.72K ppl 7.39 gN 8.22 bleu 15.54, Mon Apr  2 09:31:00 2018\n",
      "  step 6800 lr 0.001 step-time 0.39s wps 11.68K ppl 7.36 gN 7.09 bleu 15.54, Mon Apr  2 09:31:39 2018\n",
      "  step 6900 lr 0.001 step-time 0.40s wps 11.66K ppl 7.47 gN 7.07 bleu 15.54, Mon Apr  2 09:32:19 2018\n",
      "  step 7000 lr 0.001 step-time 0.40s wps 11.69K ppl 7.48 gN 7.03 bleu 15.54, Mon Apr  2 09:32:59 2018\n",
      "# Save eval, global step 7000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-7000, time 0.08s\n",
      "  # 1404\n",
      "    src: Khó có thể nói làm thế nào hình ảnh được tạo ra\n",
      "    ref: Make it impossible to say how the image actually was composed .\n",
      "    nmt: So , there &apos;s a really difficult idea to do that .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-7000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-7000, time 0.10s\n",
      "  eval dev: perplexity 15.44, time 2s, Mon Apr  2 09:33:02 2018.\n",
      "  eval test: perplexity 14.94, time 2s, Mon Apr  2 09:33:04 2018.\n",
      "  step 7100 lr 0.001 step-time 0.39s wps 11.66K ppl 7.53 gN 7.01 bleu 15.54, Mon Apr  2 09:33:44 2018\n",
      "  step 7200 lr 0.001 step-time 0.40s wps 11.72K ppl 7.63 gN 7.04 bleu 15.54, Mon Apr  2 09:34:23 2018\n",
      "  step 7300 lr 0.001 step-time 0.38s wps 11.59K ppl 7.68 gN 6.96 bleu 15.54, Mon Apr  2 09:35:02 2018\n",
      "# Finished an epoch, step 7301. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-7000, time 0.08s\n",
      "  # 624\n",
      "    src: Nó chỉ ra rằng chúng ta thực sự có thể giải quyết rất nhiều thông tin hơn chúng ta có thể Chúng ta có thể làm chúng 1 cách dễ hơn\n",
      "    ref: It turns out we can actually handle a lot more information than we think we can , we &apos;ve just got to take it a little easier .\n",
      "    nmt: It shows that we can actually solve a lot more information we can do , and we can do it easier to do it .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-7000, time 0.08s\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:35:13 2018.\n",
      "  bleu dev: 15.1\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:35:24 2018.\n",
      "  bleu test: 15.4\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 7400 lr 0.001 step-time 0.52s wps 8.95K ppl 6.18 gN 6.75 bleu 15.54, Mon Apr  2 09:36:16 2018\n",
      "  step 7500 lr 0.001 step-time 0.39s wps 11.80K ppl 6.18 gN 6.84 bleu 15.54, Mon Apr  2 09:36:55 2018\n",
      "  step 7600 lr 0.001 step-time 0.39s wps 11.82K ppl 6.27 gN 6.96 bleu 15.54, Mon Apr  2 09:37:34 2018\n",
      "  step 7700 lr 0.001 step-time 0.39s wps 11.81K ppl 6.41 gN 7.06 bleu 15.54, Mon Apr  2 09:38:13 2018\n",
      "  step 7800 lr 0.001 step-time 0.39s wps 11.79K ppl 6.57 gN 7.02 bleu 15.54, Mon Apr  2 09:38:52 2018\n",
      "  step 7900 lr 0.001 step-time 0.39s wps 11.81K ppl 6.60 gN 7.04 bleu 15.54, Mon Apr  2 09:39:31 2018\n",
      "  step 8000 lr 0.001 step-time 0.39s wps 11.79K ppl 6.64 gN 7.03 bleu 15.54, Mon Apr  2 09:40:10 2018\n",
      "# Save eval, global step 8000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-8000, time 0.08s\n",
      "  # 663\n",
      "    src: Tôi sẽ tập trung vào một con số mà tôi hi vọng nhiều bạn đã nghe nói tới .\n",
      "    ref: I &apos;m going to focus on one stat that I hope a lot of you have heard of .\n",
      "    nmt: I &apos;m going to focus on a number that I hope you &apos;ve heard about .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-8000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-8000, time 0.09s\n",
      "  eval dev: perplexity 15.96, time 2s, Mon Apr  2 09:40:13 2018.\n",
      "  eval test: perplexity 15.32, time 2s, Mon Apr  2 09:40:16 2018.\n",
      "  step 8100 lr 0.001 step-time 0.39s wps 11.77K ppl 6.70 gN 7.08 bleu 15.54, Mon Apr  2 09:40:55 2018\n",
      "  step 8200 lr 0.001 step-time 0.40s wps 11.83K ppl 6.84 gN 7.22 bleu 15.54, Mon Apr  2 09:41:34 2018\n",
      "  step 8300 lr 0.001 step-time 0.39s wps 11.79K ppl 6.81 gN 7.06 bleu 15.54, Mon Apr  2 09:42:13 2018\n",
      "# Finished an epoch, step 8344. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-8000, time 0.08s\n",
      "  # 461\n",
      "    src: Một lần chúng tôi bắt được một con còn sống .\n",
      "    ref: And one time we caught a live one .\n",
      "    nmt: And once we started a living .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-8000, time 0.08s\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:42:40 2018.\n",
      "  bleu dev: 15.5\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:42:50 2018.\n",
      "  bleu test: 16.0\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 8400 lr 0.001 step-time 0.50s wps 8.85K ppl 5.96 gN 6.83 bleu 15.54, Mon Apr  2 09:43:25 2018\n",
      "  step 8500 lr 0.001 step-time 0.39s wps 11.82K ppl 5.48 gN 6.81 bleu 15.54, Mon Apr  2 09:44:04 2018\n",
      "  step 8600 lr 0.001 step-time 0.39s wps 11.80K ppl 5.60 gN 6.92 bleu 15.54, Mon Apr  2 09:44:43 2018\n",
      "  step 8700 lr 0.001 step-time 0.39s wps 11.84K ppl 5.70 gN 7.06 bleu 15.54, Mon Apr  2 09:45:22 2018\n",
      "  step 8800 lr 0.001 step-time 0.39s wps 11.77K ppl 5.83 gN 7.06 bleu 15.54, Mon Apr  2 09:46:01 2018\n",
      "  step 8900 lr 0.001 step-time 0.39s wps 11.79K ppl 5.92 gN 7.12 bleu 15.54, Mon Apr  2 09:46:40 2018\n",
      "  step 9000 lr 0.001 step-time 0.39s wps 11.77K ppl 6.02 gN 7.12 bleu 15.54, Mon Apr  2 09:47:19 2018\n",
      "# Save eval, global step 9000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-9000, time 0.08s\n",
      "  # 875\n",
      "    src: Trong khi video này được đăng lên vào tận tháng 1 .\n",
      "    ref: And this video had actually been posted all the way back in January .\n",
      "    nmt: While this video was posted in January .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-9000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-9000, time 0.09s\n",
      "  eval dev: perplexity 16.64, time 2s, Mon Apr  2 09:47:22 2018.\n",
      "  eval test: perplexity 15.69, time 2s, Mon Apr  2 09:47:25 2018.\n",
      "  step 9100 lr 0.0005 step-time 0.40s wps 11.78K ppl 5.91 gN 7.01 bleu 15.54, Mon Apr  2 09:48:04 2018\n",
      "  step 9200 lr 0.0005 step-time 0.39s wps 11.72K ppl 5.82 gN 6.84 bleu 15.54, Mon Apr  2 09:48:44 2018\n",
      "  step 9300 lr 0.0005 step-time 0.39s wps 11.73K ppl 5.76 gN 6.84 bleu 15.54, Mon Apr  2 09:49:23 2018\n",
      "# Finished an epoch, step 9387. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-9000, time 0.08s\n",
      "  # 203\n",
      "    src: Đối với những người nghèo , người da màu hệ luỵ của việc này đã khiến họ phải sống với nỗi thất vọng tràn trề .\n",
      "    ref: In poor communities , in communities of color there is this despair , there is this hopelessness , that is being shaped by these outcomes .\n",
      "    nmt: For the poor , the black people are the best benefit of this , which has to be living with the fear of <unk> .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-9000, time 0.08s\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:50:07 2018.\n",
      "  bleu dev: 15.5\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:50:17 2018.\n",
      "  bleu test: 16.0\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 9400 lr 0.0005 step-time 0.50s wps 8.93K ppl 5.60 gN 6.75 bleu 15.54, Mon Apr  2 09:50:34 2018\n",
      "  step 9500 lr 0.0005 step-time 0.40s wps 11.57K ppl 4.68 gN 6.35 bleu 15.54, Mon Apr  2 09:51:14 2018\n",
      "  step 9600 lr 0.0005 step-time 0.39s wps 11.78K ppl 4.79 gN 6.56 bleu 15.54, Mon Apr  2 09:51:53 2018\n",
      "  step 9700 lr 0.0005 step-time 0.39s wps 11.76K ppl 4.80 gN 6.53 bleu 15.54, Mon Apr  2 09:52:32 2018\n",
      "  step 9800 lr 0.0005 step-time 0.39s wps 11.78K ppl 4.81 gN 6.70 bleu 15.54, Mon Apr  2 09:53:12 2018\n",
      "  step 9900 lr 0.0005 step-time 0.39s wps 11.78K ppl 4.87 gN 6.74 bleu 15.54, Mon Apr  2 09:53:51 2018\n",
      "  step 10000 lr 0.0005 step-time 0.39s wps 11.78K ppl 4.91 gN 6.80 bleu 15.54, Mon Apr  2 09:54:30 2018\n",
      "# Save eval, global step 10000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-10000, time 0.08s\n",
      "  # 1542\n",
      "    src: Tuy nhiên sẽ có người khi xem xét vấn đề này sẽ phản ứng như sau &quot; Ok , nghe tệ thật đấy , nhưng nó không ảnh hưởng gì đến tôi vì tôi là một công dân gương mẫu\n",
      "    ref: Now when we think deeper about things like these , the obvious response from people should be that , &quot; Okay , that sounds bad , but that doesn &apos;t really affect me because I &apos;m a legal citizen .\n",
      "    nmt: But there &apos;s a man who &apos;s going to look at this problem , &quot; Okay , listen , it doesn &apos;t matter what I mean after , but it &apos;s not something I &apos;m really going to\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-10000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-10000, time 0.09s\n",
      "  eval dev: perplexity 16.88, time 2s, Mon Apr  2 09:54:34 2018.\n",
      "  eval test: perplexity 16.16, time 2s, Mon Apr  2 09:54:36 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-10000, time 0.08s\n",
      "  # 1061\n",
      "    src: Và vì vậy , tất cả các giọt đều khác nhau đôi chút .\n",
      "    ref: And so every drop was a little bit different .\n",
      "    nmt: And so , all of them are sometimes different .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-10000, time 0.08s\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:54:49 2018.\n",
      "  bleu dev: 15.7\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:54:59 2018.\n",
      "  bleu test: 16.1\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 10100 lr 0.00025 step-time 0.39s wps 11.78K ppl 4.87 gN 6.64 bleu 15.73, Mon Apr  2 09:55:38 2018\n",
      "  step 10200 lr 0.00025 step-time 0.39s wps 11.88K ppl 4.92 gN 6.74 bleu 15.73, Mon Apr  2 09:56:17 2018\n",
      "  step 10300 lr 0.00025 step-time 0.39s wps 11.77K ppl 4.84 gN 6.62 bleu 15.73, Mon Apr  2 09:56:56 2018\n",
      "  step 10400 lr 0.00025 step-time 0.40s wps 11.83K ppl 4.92 gN 6.78 bleu 15.73, Mon Apr  2 09:57:35 2018\n",
      "# Finished an epoch, step 10430. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-10000, time 0.08s\n",
      "  # 726\n",
      "    src: Tôi nhớ rõ mọi sự như ngày hôm qua vậy .\n",
      "    ref: I can remember that like it was yesterday .\n",
      "    nmt: I remember all the time that yesterday .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-10000, time 0.08s\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 09:57:56 2018.\n",
      "  bleu dev: 15.7\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 09:58:06 2018.\n",
      "  bleu test: 16.1\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 10500 lr 0.00025 step-time 0.51s wps 8.81K ppl 4.41 gN 6.41 bleu 15.73, Mon Apr  2 09:58:47 2018\n",
      "  step 10600 lr 0.00025 step-time 0.39s wps 11.78K ppl 4.30 gN 6.31 bleu 15.73, Mon Apr  2 09:59:25 2018\n",
      "  step 10700 lr 0.00025 step-time 0.39s wps 11.81K ppl 4.37 gN 6.50 bleu 15.73, Mon Apr  2 10:00:05 2018\n",
      "  step 10800 lr 0.00025 step-time 0.39s wps 11.80K ppl 4.36 gN 6.49 bleu 15.73, Mon Apr  2 10:00:44 2018\n",
      "  step 10900 lr 0.00025 step-time 0.39s wps 11.83K ppl 4.41 gN 6.55 bleu 15.73, Mon Apr  2 10:01:23 2018\n",
      "  step 11000 lr 0.00025 step-time 0.39s wps 11.82K ppl 4.42 gN 6.59 bleu 15.73, Mon Apr  2 10:02:02 2018\n",
      "# Save eval, global step 11000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-11000, time 0.08s\n",
      "  # 1389\n",
      "    src: Chúng ta có ở đây ba vật thể có hình dung hoàn hảo , những vật mà chúng ta đều có thể liên quan đến cuộc sống trong thế giới không gian ba chiều .\n",
      "    ref: Here we have three perfectly imaginable physical objects , something we all can relate to living in a three-dimensional world .\n",
      "    nmt: We have here three objects that have perfect shapes , things that we can relate to life in the space of three dimensions .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-11000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-11000, time 0.09s\n",
      "  eval dev: perplexity 17.08, time 2s, Mon Apr  2 10:02:05 2018.\n",
      "  eval test: perplexity 16.37, time 2s, Mon Apr  2 10:02:08 2018.\n",
      "  step 11100 lr 0.000125 step-time 0.39s wps 11.86K ppl 4.43 gN 6.60 bleu 15.73, Mon Apr  2 10:02:47 2018\n",
      "  step 11200 lr 0.000125 step-time 0.39s wps 11.83K ppl 4.42 gN 6.52 bleu 15.73, Mon Apr  2 10:03:26 2018\n",
      "  step 11300 lr 0.000125 step-time 0.39s wps 11.81K ppl 4.43 gN 6.59 bleu 15.73, Mon Apr  2 10:04:05 2018\n",
      "  step 11400 lr 0.000125 step-time 0.39s wps 11.87K ppl 4.40 gN 6.57 bleu 15.73, Mon Apr  2 10:04:44 2018\n",
      "# Finished an epoch, step 11473. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-11000, time 0.08s\n",
      "  # 547\n",
      "    src: Bây giờ , tôi sẽ kể cho bạn 1 bài học tôi đã có với Gur Huberman , Emir Kamenica , Wei Jang nơi chúng ta nhìn vào những quyết định tiết kiệm tiền hưu trí của gần 1 triệu người mỹ từ khoảng 650 kế hoạch ở mỹ .\n",
      "    ref: Now I &apos;m going to describe to you a study I did with Gur Huberman , Emir Kamenica , Wei Jang where we looked at the retirement savings decisions of nearly a million Americans from about 650 plans all in the U.S.\n",
      "    nmt: Now , I &apos;m going to tell you a lesson I &apos;ve had with <unk> <unk> , <unk> <unk> , where we see that in the <unk> of a million <unk> from the <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-11000, time 0.08s\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 10:05:21 2018.\n",
      "  bleu dev: 15.8\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 10:05:31 2018.\n",
      "  bleu test: 16.6\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "  step 11500 lr 0.000125 step-time 0.50s wps 8.89K ppl 4.33 gN 6.56 bleu 15.83, Mon Apr  2 10:05:55 2018\n",
      "  step 11600 lr 0.000125 step-time 0.39s wps 11.74K ppl 4.17 gN 6.32 bleu 15.83, Mon Apr  2 10:06:34 2018\n",
      "  step 11700 lr 0.000125 step-time 0.39s wps 11.85K ppl 4.23 gN 6.49 bleu 15.83, Mon Apr  2 10:07:13 2018\n",
      "  step 11800 lr 0.000125 step-time 0.39s wps 11.82K ppl 4.16 gN 6.39 bleu 15.83, Mon Apr  2 10:07:52 2018\n",
      "  step 11900 lr 0.000125 step-time 0.39s wps 11.82K ppl 4.14 gN 6.44 bleu 15.83, Mon Apr  2 10:08:31 2018\n",
      "  step 12000 lr 0.000125 step-time 0.39s wps 11.85K ppl 4.17 gN 6.50 bleu 15.83, Mon Apr  2 10:09:10 2018\n",
      "# Save eval, global step 12000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-12000, time 0.08s\n",
      "  # 361\n",
      "    src: Và ông ấy tiến lại gần hơn và ôm lấy tôi\n",
      "    ref: And this man came over to me and he hugged me .\n",
      "    nmt: And he went on to be close and hugged me .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-12000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-12000, time 0.09s\n",
      "  eval dev: perplexity 17.44, time 2s, Mon Apr  2 10:09:13 2018.\n",
      "  eval test: perplexity 16.69, time 2s, Mon Apr  2 10:09:16 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-12000, time 0.08s\n",
      "  # 15\n",
      "    src: Mảnh ghép tiếp theo của tấm hình là một con thuyền trong sớm hoàng hôn lặng lẽ trườn ra biển .\n",
      "    ref: The next piece of the jigsaw is of a boat in the early dawn slipping silently out to sea .\n",
      "    nmt: The next piece of the photograph was a boat that was very <unk> at the Royal coma .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-12000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/translate.ckpt-12000, time 0.09s\n",
      "  eval dev: perplexity 17.44, time 2s, Mon Apr  2 10:09:21 2018.\n",
      "  eval test: perplexity 16.69, time 2s, Mon Apr  2 10:09:23 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/translate.ckpt-12000, time 0.08s\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 10:09:34 2018.\n",
      "  bleu dev: 15.4\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 10:09:44 2018.\n",
      "  bleu test: 16.1\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# Final, step 12000 lr 0.000125 step-time 0.39s wps 11.85K ppl 4.17 gN 6.50 dev ppl 17.44, dev bleu 15.4, test ppl 16.69, test bleu 16.1, Mon Apr  2 10:09:45 2018\n",
      "# Done training!, time 5224s, Mon Apr  2 10:09:45 2018.\n",
      "# Start evaluating saved best models.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-11000, time 0.08s\n",
      "  # 661\n",
      "    src: Và có hàng trăm con số như thế .\n",
      "    ref: And there &apos;s hundreds of them .\n",
      "    nmt: And there are hundreds of them .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-11000\n",
      "  loaded eval model parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-11000, time 0.09s\n",
      "  eval dev: perplexity 17.08, time 2s, Mon Apr  2 10:09:47 2018.\n",
      "  eval test: perplexity 16.37, time 2s, Mon Apr  2 10:09:50 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_bilinearatt/best_bleu/translate.ckpt-11000, time 0.08s\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_bilinearatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 10s, Mon Apr  2 10:10:00 2018.\n",
      "  bleu dev: 15.8\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_bilinearatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 9s, Mon Apr  2 10:10:10 2018.\n",
      "  bleu test: 16.6\n",
      "  saving hparams to nmt_model_bilinearatt/hparams\n",
      "# Best bleu, step 11000 lr 0.000125 step-time 0.39s wps 11.85K ppl 4.17 gN 6.50 dev ppl 17.08, dev bleu 15.8, test ppl 16.37, test bleu 16.6, Mon Apr  2 10:10:10 2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dev_ppl': 17.442222185685058,\n",
       "  'dev_scores': {'bleu': 15.35520479604916},\n",
       "  'test_ppl': 16.691589883678144,\n",
       "  'test_scores': {'bleu': 16.070008518562382}},\n",
       " 12000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an LSTM model with bilinear attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"),\n",
    "                                  out_dir=\"nmt_model_bilinearatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithBilinearAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab file datasets/nmt_data_vi/vocab.vi exists\n",
      "# Vocab file datasets/nmt_data_vi/vocab.en exists\n",
      "# creating train graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0  DropoutWrapper, dropout=0.2   DropoutWrapper  DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=luong234, start_decay_step=8000, decay_steps 1000, decay_factor 0.5\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_1:0, (1024, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_2:0, (512, 1), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating eval graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTMCellWithFeedForwardAttention, dropout=0   LSTMCellWithFeedForwardAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_1:0, (1024, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_2:0, (512, 1), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "# creating infer graph ...\n",
      "  num_layers = 1, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTMCellWithFeedForwardAttention, dropout=0   LSTMCellWithFeedForwardAttention  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (7709, 512), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (17191, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/kernel:0, (1024, 2048), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/bias:0, (2048,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_1:0, (1024, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_att_2:0, (512, 1), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/lstm_cell_with_feed_forward_attention/W_c:0, (1024, 256), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (256, 17191), \n",
      "  created train model with fresh parameters, time 0.43s\n",
      "  created infer model with fresh parameters, time 0.08s\n",
      "  # 1060\n",
      "    src: Và tất cả mọi lần nó phân chia , chúng đều phân chia không đều những thành phần hoá học trong chúng .\n",
      "    ref: And every time it divided , they got sort of unequal division of the chemicals within them .\n",
      "    nmt: stewards stewards Creek Creek lottery buzzing reef reef reef reef reef wide wide wide wide Nicolas Nicolas average average average average average average average Manya Manya Manya Manya Manya warps warps warps warps mechanical mechanical tongue tongue tongue tongue tongue tongue tongue tongue tongue tongue tongue\n",
      "  created eval model with fresh parameters, time 0.14s\n",
      "  eval dev: perplexity 17366.12, time 5s, Mon Apr  2 10:11:04 2018.\n",
      "  eval test: perplexity 17392.81, time 5s, Mon Apr  2 10:11:10 2018.\n",
      "  created infer model with fresh parameters, time 0.06s\n",
      "# Start step 0, lr 0.001, Mon Apr  2 10:11:10 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 0.001 step-time 0.85s wps 5.48K ppl 580.38 gN 15.53 bleu 0.00, Mon Apr  2 10:12:34 2018\n",
      "  step 200 lr 0.001 step-time 0.71s wps 6.49K ppl 272.66 gN 10.50 bleu 0.00, Mon Apr  2 10:13:45 2018\n",
      "  step 300 lr 0.001 step-time 0.71s wps 6.46K ppl 165.53 gN 8.38 bleu 0.00, Mon Apr  2 10:14:56 2018\n",
      "  step 400 lr 0.001 step-time 0.71s wps 6.48K ppl 127.85 gN 7.88 bleu 0.00, Mon Apr  2 10:16:08 2018\n",
      "  step 500 lr 0.001 step-time 0.71s wps 6.48K ppl 107.89 gN 7.71 bleu 0.00, Mon Apr  2 10:17:19 2018\n",
      "  step 600 lr 0.001 step-time 0.72s wps 6.46K ppl 91.77 gN 7.36 bleu 0.00, Mon Apr  2 10:18:31 2018\n",
      "  step 700 lr 0.001 step-time 0.71s wps 6.46K ppl 75.70 gN 7.58 bleu 0.00, Mon Apr  2 10:19:42 2018\n",
      "  step 800 lr 0.001 step-time 0.71s wps 6.46K ppl 65.10 gN 8.74 bleu 0.00, Mon Apr  2 10:20:53 2018\n",
      "  step 900 lr 0.001 step-time 0.72s wps 6.47K ppl 53.77 gN 7.35 bleu 0.00, Mon Apr  2 10:22:05 2018\n",
      "  step 1000 lr 0.001 step-time 0.71s wps 6.47K ppl 44.57 gN 7.04 bleu 0.00, Mon Apr  2 10:23:16 2018\n",
      "# Save eval, global step 1000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-1000, time 0.09s\n",
      "  # 474\n",
      "    src: Bạn có thể thực hiện hàng loạt những sự thay đổi nối tiếp\n",
      "    ref: You can have a succession of changes .\n",
      "    nmt: You can be the <unk> of the <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-1000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-1000, time 0.10s\n",
      "  eval dev: perplexity 40.72, time 5s, Mon Apr  2 10:23:22 2018.\n",
      "  eval test: perplexity 44.71, time 5s, Mon Apr  2 10:23:28 2018.\n",
      "# Finished an epoch, step 1043. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-1000, time 0.08s\n",
      "  # 1120\n",
      "    src: Giờ chúng ta đã tăng tốc khung thời gian lên một lần nữa .\n",
      "    ref: So now we &apos;ve speeded up the time scales once again .\n",
      "    nmt: We &apos;ve got to be the time of the time .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-1000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-1000, time 0.08s\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 10:24:14 2018.\n",
      "  bleu dev: 7.3\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 1000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 17s, Mon Apr  2 10:24:32 2018.\n",
      "  bleu test: 6.8\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 1100 lr 0.001 step-time 0.80s wps 5.48K ppl 37.50 gN 7.60 bleu 7.25, Mon Apr  2 10:25:25 2018\n",
      "  step 1200 lr 0.001 step-time 0.72s wps 6.46K ppl 33.08 gN 6.95 bleu 7.25, Mon Apr  2 10:26:37 2018\n",
      "  step 1300 lr 0.001 step-time 0.72s wps 6.46K ppl 32.38 gN 8.80 bleu 7.25, Mon Apr  2 10:27:49 2018\n",
      "  step 1400 lr 0.001 step-time 0.71s wps 6.45K ppl 28.44 gN 6.78 bleu 7.25, Mon Apr  2 10:29:00 2018\n",
      "  step 1500 lr 0.001 step-time 0.71s wps 6.47K ppl 26.03 gN 6.96 bleu 7.25, Mon Apr  2 10:30:11 2018\n",
      "  step 1600 lr 0.001 step-time 0.71s wps 6.46K ppl 24.77 gN 6.86 bleu 7.25, Mon Apr  2 10:31:22 2018\n",
      "  step 1700 lr 0.001 step-time 0.72s wps 6.47K ppl 24.10 gN 6.91 bleu 7.25, Mon Apr  2 10:32:35 2018\n",
      "  step 1800 lr 0.001 step-time 0.71s wps 6.45K ppl 22.94 gN 7.18 bleu 7.25, Mon Apr  2 10:33:46 2018\n",
      "  step 1900 lr 0.001 step-time 0.71s wps 6.47K ppl 21.27 gN 6.96 bleu 7.25, Mon Apr  2 10:34:57 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 2000 lr 0.001 step-time 0.71s wps 6.45K ppl 20.48 gN 6.69 bleu 7.25, Mon Apr  2 10:36:08 2018\n",
      "# Save eval, global step 2000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-2000, time 0.08s\n",
      "  # 1473\n",
      "    src: Và chúng tôi muốn vật liệu này tiếp cận được với mọi người .\n",
      "    ref: And we want to make this material accessible to everyone .\n",
      "    nmt: And we want this material to be able to get people .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-2000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-2000, time 0.10s\n",
      "  eval dev: perplexity 21.64, time 5s, Mon Apr  2 10:36:14 2018.\n",
      "  eval test: perplexity 22.24, time 5s, Mon Apr  2 10:36:20 2018.\n",
      "# Finished an epoch, step 2086. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-2000, time 0.08s\n",
      "  # 510\n",
      "    src: Và họ phát hiện ra là trung bình CEO đã làm khoảng 139 nhiệm vụ trong 1 tuần\n",
      "    ref: And they found that the average CEO engaged in about 139 tasks in a week .\n",
      "    nmt: And they found that the average <unk> has been about <unk> <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-2000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-2000, time 0.08s\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 16s, Mon Apr  2 10:37:37 2018.\n",
      "  bleu dev: 11.2\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 16s, Mon Apr  2 10:37:54 2018.\n",
      "  bleu test: 11.4\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 2100 lr 0.001 step-time 0.81s wps 5.53K ppl 19.44 gN 7.02 bleu 11.24, Mon Apr  2 10:38:16 2018\n",
      "  step 2200 lr 0.001 step-time 0.72s wps 6.49K ppl 15.96 gN 6.95 bleu 11.24, Mon Apr  2 10:39:28 2018\n",
      "  step 2300 lr 0.001 step-time 0.71s wps 6.48K ppl 15.99 gN 7.59 bleu 11.24, Mon Apr  2 10:40:39 2018\n",
      "  step 2400 lr 0.001 step-time 0.71s wps 6.47K ppl 15.52 gN 6.69 bleu 11.24, Mon Apr  2 10:41:49 2018\n",
      "  step 2500 lr 0.001 step-time 0.72s wps 6.47K ppl 15.43 gN 6.79 bleu 11.24, Mon Apr  2 10:43:01 2018\n",
      "  step 2600 lr 0.001 step-time 0.71s wps 6.47K ppl 15.00 gN 6.78 bleu 11.24, Mon Apr  2 10:44:12 2018\n",
      "  step 2700 lr 0.001 step-time 0.71s wps 6.48K ppl 14.87 gN 6.73 bleu 11.24, Mon Apr  2 10:45:23 2018\n",
      "  step 2800 lr 0.001 step-time 0.71s wps 6.48K ppl 14.35 gN 6.79 bleu 11.24, Mon Apr  2 10:46:34 2018\n",
      "  step 2900 lr 0.001 step-time 0.71s wps 6.47K ppl 14.26 gN 6.73 bleu 11.24, Mon Apr  2 10:47:45 2018\n",
      "  step 3000 lr 0.001 step-time 0.72s wps 6.47K ppl 14.34 gN 6.87 bleu 11.24, Mon Apr  2 10:48:57 2018\n",
      "# Save eval, global step 3000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-3000, time 0.08s\n",
      "  # 315\n",
      "    src: Và đó cũng là lúc bà Carr nhũi tới , đặt tay lên mặt tôi và nói : &quot; Thế nên cậu phải thật dũng cảm , dũng cảm và dũng cảm &quot; .\n",
      "    ref: And that &apos;s when Ms. Carr leaned forward , she put her finger in my face , she said , &quot; That &apos;s why you &apos;ve got to be brave , brave , brave . &quot;\n",
      "    nmt: And that &apos;s also when she was <unk> , put it up to my face and said , &quot; So , &quot; So , &quot; So , &quot; I should be a sad\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-3000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-3000, time 0.10s\n",
      "  eval dev: perplexity 17.37, time 5s, Mon Apr  2 10:49:03 2018.\n",
      "  eval test: perplexity 17.31, time 5s, Mon Apr  2 10:49:09 2018.\n",
      "  step 3100 lr 0.001 step-time 0.71s wps 6.49K ppl 14.08 gN 6.76 bleu 11.24, Mon Apr  2 10:50:20 2018\n",
      "# Finished an epoch, step 3129. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-3000, time 0.08s\n",
      "  # 444\n",
      "    src: Và những loại cá này dường như cũng thoải mái khi ở đó .\n",
      "    ref: And the fish also were happy to be there .\n",
      "    nmt: And these kinds of fish seem to be comfortable when it &apos;s so comfortable there .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-3000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-3000, time 0.08s\n",
      "# External evaluation, global step 3000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 10:50:57 2018.\n",
      "  bleu dev: 12.6\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 3000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 17s, Mon Apr  2 10:51:15 2018.\n",
      "  bleu test: 12.8\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 3200 lr 0.001 step-time 0.81s wps 5.51K ppl 11.62 gN 6.63 bleu 12.63, Mon Apr  2 10:52:18 2018\n",
      "  step 3300 lr 0.001 step-time 0.71s wps 6.49K ppl 11.19 gN 6.91 bleu 12.63, Mon Apr  2 10:53:29 2018\n",
      "  step 3400 lr 0.001 step-time 0.71s wps 6.46K ppl 11.25 gN 6.66 bleu 12.63, Mon Apr  2 10:54:40 2018\n",
      "  step 3500 lr 0.001 step-time 0.72s wps 6.49K ppl 11.36 gN 6.74 bleu 12.63, Mon Apr  2 10:55:52 2018\n",
      "  step 3600 lr 0.001 step-time 0.71s wps 6.48K ppl 11.26 gN 6.81 bleu 12.63, Mon Apr  2 10:57:03 2018\n",
      "  step 3700 lr 0.001 step-time 0.71s wps 6.47K ppl 11.09 gN 6.69 bleu 12.63, Mon Apr  2 10:58:14 2018\n",
      "  step 3800 lr 0.001 step-time 0.71s wps 6.47K ppl 11.14 gN 6.86 bleu 12.63, Mon Apr  2 10:59:25 2018\n",
      "  step 3900 lr 0.001 step-time 0.72s wps 6.46K ppl 11.23 gN 6.74 bleu 12.63, Mon Apr  2 11:00:37 2018\n",
      "  step 4000 lr 0.001 step-time 0.71s wps 6.45K ppl 11.20 gN 6.67 bleu 12.63, Mon Apr  2 11:01:48 2018\n",
      "# Save eval, global step 4000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-4000, time 0.08s\n",
      "  # 371\n",
      "    src: Chính tôi đã viết những điều điên rồ này .\n",
      "    ref: I had written these crazy things .\n",
      "    nmt: I &apos;ve written these crazy things .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-4000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-4000, time 0.10s\n",
      "  eval dev: perplexity 15.58, time 5s, Mon Apr  2 11:01:54 2018.\n",
      "  eval test: perplexity 15.51, time 5s, Mon Apr  2 11:02:00 2018.\n",
      "  step 4100 lr 0.001 step-time 0.71s wps 6.49K ppl 11.25 gN 6.65 bleu 12.63, Mon Apr  2 11:03:11 2018\n",
      "# Finished an epoch, step 4172. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-4000, time 0.08s\n",
      "  # 1498\n",
      "    src: Đây là Vladimir Tsastsin đến từ Tartu , Estonia\n",
      "    ref: Here &apos;s Vladimir Tsastsin form Tartu in Estonia .\n",
      "    nmt: This is <unk> <unk> from <unk> , Estonia .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-4000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-4000, time 0.08s\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 16s, Mon Apr  2 11:04:17 2018.\n",
      "  bleu dev: 13.6\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 16s, Mon Apr  2 11:04:35 2018.\n",
      "  bleu test: 13.9\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 4200 lr 0.001 step-time 0.81s wps 5.53K ppl 10.36 gN 6.94 bleu 13.56, Mon Apr  2 11:05:07 2018\n",
      "  step 4300 lr 0.001 step-time 0.71s wps 6.46K ppl 8.98 gN 6.89 bleu 13.56, Mon Apr  2 11:06:18 2018\n",
      "  step 4400 lr 0.001 step-time 0.72s wps 6.49K ppl 9.06 gN 6.55 bleu 13.56, Mon Apr  2 11:07:30 2018\n",
      "  step 4500 lr 0.001 step-time 0.71s wps 6.45K ppl 9.04 gN 6.59 bleu 13.56, Mon Apr  2 11:08:41 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 4600 lr 0.001 step-time 0.72s wps 6.46K ppl 9.22 gN 6.65 bleu 13.56, Mon Apr  2 11:09:53 2018\n",
      "  step 4700 lr 0.001 step-time 0.71s wps 6.46K ppl 9.28 gN 6.83 bleu 13.56, Mon Apr  2 11:11:04 2018\n",
      "  step 4800 lr 0.001 step-time 0.72s wps 6.46K ppl 9.30 gN 6.77 bleu 13.56, Mon Apr  2 11:12:16 2018\n",
      "  step 4900 lr 0.001 step-time 0.71s wps 6.47K ppl 9.16 gN 6.59 bleu 13.56, Mon Apr  2 11:13:27 2018\n",
      "  step 5000 lr 0.001 step-time 0.71s wps 6.48K ppl 9.35 gN 6.80 bleu 13.56, Mon Apr  2 11:14:38 2018\n",
      "# Save eval, global step 5000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-5000, time 0.08s\n",
      "  # 609\n",
      "    src: Nếu tôi chỉ ra cho bạn 600 loại tạp chí Và tôi chia nó ra làm 10 loại so với khi tôi chỉ cho bạn 400 tạp chí và chia nó ra thành 20 loại Bạn tin rằng tôi đã đưa cho bạn nhiều sự lựa chọn và những trải nghiệm lựa chọn tốt hơn nếu tôi cho bạn 400 hơn là tôi chỉ cho bạn 600\n",
      "    ref: If I show you 600 magazines and I divide them up into 10 categories , versus I show you 400 magazines and divide them up into 20 categories , you believe that I have given you more choice and a better choosing experience if I gave you the 400 than if I gave you the 600 .\n",
      "    nmt: If I just show you 600 things , I split it up with 10 different types of magazines when I just give you 400 <unk> , and I just give you a lot of <unk> and <unk> that I &apos;ve given about the <unk> and the <unk> of the <unk> , and I just give you a lot of\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-5000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-5000, time 0.10s\n",
      "  eval dev: perplexity 14.90, time 5s, Mon Apr  2 11:14:44 2018.\n",
      "  eval test: perplexity 14.73, time 5s, Mon Apr  2 11:14:50 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-5000, time 0.08s\n",
      "  # 1184\n",
      "    src: Một vài năm trước tôi bắt đầu một chương trình tìm kiếm những siêu sao công nghệ và tạo điều kiện cho họ nghỉ một năm và làm việc trong môi trường dường như họ sẽ rất ghét ; họ phải làm việc cho chính phủ .\n",
      "    ref: So a couple of years ago I started a program to try to get the rockstar tech and design people to take a year off and work in the one environment that represents pretty much everything they &apos;re supposed to hate ; we have them work in government .\n",
      "    nmt: A few years ago I started a program that looked like the technology and made it for them to break their work , and they had to work for a year and they had to work for the public .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-5000, time 0.08s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 11:15:10 2018.\n",
      "  bleu dev: 14.2\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 16s, Mon Apr  2 11:15:27 2018.\n",
      "  bleu test: 13.9\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 5100 lr 0.001 step-time 0.71s wps 6.46K ppl 9.19 gN 6.56 bleu 14.20, Mon Apr  2 11:16:39 2018\n",
      "  step 5200 lr 0.001 step-time 0.72s wps 6.47K ppl 9.35 gN 6.73 bleu 14.20, Mon Apr  2 11:17:51 2018\n",
      "# Finished an epoch, step 5215. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-5000, time 0.08s\n",
      "  # 1275\n",
      "    src: Khi người này giúp người kia , cộng đồng của chúng ta trở nên lớn mạnh hơn .\n",
      "    ref: When one neighbor helps another , we strengthen our communities .\n",
      "    nmt: When this person helped people , our community becomes stronger .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-5000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-5000, time 0.08s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 11:18:16 2018.\n",
      "  bleu dev: 14.2\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 16s, Mon Apr  2 11:18:34 2018.\n",
      "  bleu test: 13.9\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 5300 lr 0.001 step-time 0.81s wps 5.51K ppl 7.42 gN 6.30 bleu 14.20, Mon Apr  2 11:19:46 2018\n",
      "  step 5400 lr 0.001 step-time 0.72s wps 6.47K ppl 7.52 gN 7.07 bleu 14.20, Mon Apr  2 11:20:58 2018\n",
      "  step 5500 lr 0.001 step-time 0.71s wps 6.47K ppl 7.75 gN 6.92 bleu 14.20, Mon Apr  2 11:22:09 2018\n",
      "  step 5600 lr 0.001 step-time 0.71s wps 6.47K ppl 7.81 gN 6.60 bleu 14.20, Mon Apr  2 11:23:20 2018\n",
      "  step 5700 lr 0.001 step-time 0.72s wps 6.44K ppl 7.81 gN 6.70 bleu 14.20, Mon Apr  2 11:24:32 2018\n",
      "  step 5800 lr 0.001 step-time 0.71s wps 6.46K ppl 8.00 gN 6.86 bleu 14.20, Mon Apr  2 11:25:44 2018\n",
      "  step 5900 lr 0.001 step-time 0.71s wps 6.46K ppl 7.98 gN 6.76 bleu 14.20, Mon Apr  2 11:26:54 2018\n",
      "  step 6000 lr 0.001 step-time 0.71s wps 6.46K ppl 7.99 gN 6.78 bleu 14.20, Mon Apr  2 11:28:05 2018\n",
      "# Save eval, global step 6000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-6000, time 0.08s\n",
      "  # 1289\n",
      "    src: Đó chính là những gì OccupytheSEC đã làm .\n",
      "    ref: So that &apos;s OccupytheSEC movement has done .\n",
      "    nmt: That &apos;s what <unk> did .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-6000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-6000, time 0.10s\n",
      "  eval dev: perplexity 14.77, time 5s, Mon Apr  2 11:28:12 2018.\n",
      "  eval test: perplexity 14.75, time 5s, Mon Apr  2 11:28:18 2018.\n",
      "  step 6100 lr 0.001 step-time 0.72s wps 6.46K ppl 8.08 gN 6.73 bleu 14.20, Mon Apr  2 11:29:29 2018\n",
      "  step 6200 lr 0.001 step-time 0.72s wps 6.45K ppl 8.28 gN 6.87 bleu 14.20, Mon Apr  2 11:30:41 2018\n",
      "# Finished an epoch, step 6258. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-6000, time 0.08s\n",
      "  # 778\n",
      "    src: Tôi xem cổ họng anh ta , nó hơi hồng hồng .\n",
      "    ref: I looked at his throat , it was a little bit pink .\n",
      "    nmt: I saw his <unk> , it was a pink <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-6000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-6000, time 0.08s\n",
      "# External evaluation, global step 6000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 11:31:38 2018.\n",
      "  bleu dev: 13.8\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 6000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 17s, Mon Apr  2 11:31:56 2018.\n",
      "  bleu test: 14.3\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 6300 lr 0.001 step-time 0.81s wps 5.51K ppl 7.31 gN 6.54 bleu 14.20, Mon Apr  2 11:32:38 2018\n",
      "  step 6400 lr 0.001 step-time 0.71s wps 6.46K ppl 6.48 gN 6.57 bleu 14.20, Mon Apr  2 11:33:49 2018\n",
      "  step 6500 lr 0.001 step-time 0.72s wps 6.47K ppl 6.71 gN 6.76 bleu 14.20, Mon Apr  2 11:35:01 2018\n",
      "  step 6600 lr 0.001 step-time 0.72s wps 6.45K ppl 6.76 gN 6.67 bleu 14.20, Mon Apr  2 11:36:13 2018\n",
      "  step 6700 lr 0.001 step-time 0.71s wps 6.44K ppl 6.78 gN 6.81 bleu 14.20, Mon Apr  2 11:37:24 2018\n",
      "  step 6800 lr 0.001 step-time 0.71s wps 6.44K ppl 6.92 gN 6.73 bleu 14.20, Mon Apr  2 11:38:35 2018\n",
      "  step 6900 lr 0.001 step-time 0.71s wps 6.45K ppl 6.90 gN 6.94 bleu 14.20, Mon Apr  2 11:39:46 2018\n",
      "  step 7000 lr 0.001 step-time 0.72s wps 6.45K ppl 7.11 gN 6.70 bleu 14.20, Mon Apr  2 11:40:58 2018\n",
      "# Save eval, global step 7000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-7000, time 0.08s\n",
      "  # 326\n",
      "    src: và vì thế , con người chúng ta có phẩm giá cơ bản phải được luật pháp bảo vệ .\n",
      "    ref: And because of that there &apos;s this basic human dignity that must be respected by law .\n",
      "    nmt: And so , as humans , we have the basic dignity of protecting the protection .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-7000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-7000, time 0.10s\n",
      "  eval dev: perplexity 14.89, time 5s, Mon Apr  2 11:41:05 2018.\n",
      "  eval test: perplexity 14.72, time 5s, Mon Apr  2 11:41:11 2018.\n",
      "  step 7100 lr 0.001 step-time 0.71s wps 6.46K ppl 7.07 gN 6.75 bleu 14.20, Mon Apr  2 11:42:22 2018\n",
      "  step 7200 lr 0.001 step-time 0.72s wps 6.46K ppl 7.17 gN 6.80 bleu 14.20, Mon Apr  2 11:43:34 2018\n",
      "  step 7300 lr 0.001 step-time 0.69s wps 6.43K ppl 7.11 gN 6.84 bleu 14.20, Mon Apr  2 11:44:43 2018\n",
      "# Finished an epoch, step 7301. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-7000, time 0.08s\n",
      "  # 1077\n",
      "    src: Thiết lập một cấu trúc ADN là một bước thú vị .\n",
      "    ref: Writing down the DNA was an interesting step .\n",
      "    nmt: The setup is a really interesting way .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-7000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-7000, time 0.08s\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 18s, Mon Apr  2 11:45:02 2018.\n",
      "  bleu dev: 13.1\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 18s, Mon Apr  2 11:45:21 2018.\n",
      "  bleu test: 12.8\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 7400 lr 0.001 step-time 0.84s wps 5.56K ppl 5.73 gN 6.54 bleu 14.20, Mon Apr  2 11:46:45 2018\n",
      "  step 7500 lr 0.001 step-time 0.72s wps 6.43K ppl 5.81 gN 6.75 bleu 14.20, Mon Apr  2 11:47:56 2018\n",
      "  step 7600 lr 0.001 step-time 0.72s wps 6.45K ppl 5.90 gN 6.73 bleu 14.20, Mon Apr  2 11:49:08 2018\n",
      "  step 7700 lr 0.001 step-time 0.71s wps 6.44K ppl 5.98 gN 6.77 bleu 14.20, Mon Apr  2 11:50:19 2018\n",
      "  step 7800 lr 0.001 step-time 0.72s wps 6.44K ppl 6.08 gN 6.94 bleu 14.20, Mon Apr  2 11:51:31 2018\n",
      "  step 7900 lr 0.001 step-time 0.71s wps 6.46K ppl 6.26 gN 6.93 bleu 14.20, Mon Apr  2 11:52:42 2018\n",
      "  step 8000 lr 0.001 step-time 0.71s wps 6.45K ppl 6.33 gN 6.91 bleu 14.20, Mon Apr  2 11:53:54 2018\n",
      "# Save eval, global step 8000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-8000, time 0.08s\n",
      "  # 1175\n",
      "    src: Bởi tất cả công nghệ này đều tự thúc đẩy bản thân nó phát triển .\n",
      "    ref: Because all of these technologies are feeding back on themselves .\n",
      "    nmt: Because all of this technology is <unk> itself .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-8000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-8000, time 0.10s\n",
      "  eval dev: perplexity 15.18, time 5s, Mon Apr  2 11:54:00 2018.\n",
      "  eval test: perplexity 14.87, time 5s, Mon Apr  2 11:54:06 2018.\n",
      "  step 8100 lr 0.001 step-time 0.71s wps 6.45K ppl 6.33 gN 6.98 bleu 14.20, Mon Apr  2 11:55:17 2018\n",
      "  step 8200 lr 0.001 step-time 0.72s wps 6.44K ppl 6.38 gN 7.04 bleu 14.20, Mon Apr  2 11:56:29 2018\n",
      "  step 8300 lr 0.001 step-time 0.72s wps 6.45K ppl 6.47 gN 7.10 bleu 14.20, Mon Apr  2 11:57:41 2018\n",
      "# Finished an epoch, step 8344. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-8000, time 0.08s\n",
      "  # 1549\n",
      "    src: Đây là một vấn đề giữa tự do và sự kiểm soát .\n",
      "    ref: It &apos;s a question of freedom against control .\n",
      "    nmt: This is a cause between freedom and control .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-8000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-8000, time 0.08s\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 11:58:27 2018.\n",
      "  bleu dev: 13.8\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 8000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 17s, Mon Apr  2 11:58:45 2018.\n",
      "  bleu test: 14.3\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 8400 lr 0.001 step-time 0.82s wps 5.49K ppl 5.69 gN 6.78 bleu 14.20, Mon Apr  2 11:59:38 2018\n",
      "  step 8500 lr 0.001 step-time 0.71s wps 6.45K ppl 5.14 gN 6.54 bleu 14.20, Mon Apr  2 12:00:49 2018\n",
      "  step 8600 lr 0.001 step-time 0.71s wps 6.44K ppl 5.21 gN 6.68 bleu 14.20, Mon Apr  2 12:02:01 2018\n",
      "  step 8700 lr 0.001 step-time 0.72s wps 6.46K ppl 5.38 gN 6.89 bleu 14.20, Mon Apr  2 12:03:12 2018\n",
      "  step 8800 lr 0.001 step-time 0.72s wps 6.46K ppl 5.50 gN 7.13 bleu 14.20, Mon Apr  2 12:04:24 2018\n",
      "  step 8900 lr 0.001 step-time 0.71s wps 6.46K ppl 5.52 gN 6.87 bleu 14.20, Mon Apr  2 12:05:35 2018\n",
      "  step 9000 lr 0.001 step-time 0.72s wps 6.45K ppl 5.64 gN 6.98 bleu 14.20, Mon Apr  2 12:06:47 2018\n",
      "# Save eval, global step 9000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-9000, time 0.08s\n",
      "  # 1362\n",
      "    src: Tôi ở đây để chia sẻ với các bạn phong cách nhiếp ảnh của tôi .\n",
      "    ref: I &apos;m here to share my photography .\n",
      "    nmt: I &apos;m here to share with you my photographer .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-9000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-9000, time 0.09s\n",
      "  eval dev: perplexity 15.48, time 5s, Mon Apr  2 12:06:53 2018.\n",
      "  eval test: perplexity 15.46, time 5s, Mon Apr  2 12:06:59 2018.\n",
      "  step 9100 lr 0.0005 step-time 0.72s wps 6.45K ppl 5.55 gN 6.75 bleu 14.20, Mon Apr  2 12:08:11 2018\n",
      "  step 9200 lr 0.0005 step-time 0.71s wps 6.46K ppl 5.49 gN 6.70 bleu 14.20, Mon Apr  2 12:09:22 2018\n",
      "  step 9300 lr 0.0005 step-time 0.72s wps 6.44K ppl 5.47 gN 6.68 bleu 14.20, Mon Apr  2 12:10:34 2018\n",
      "# Finished an epoch, step 9387. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-9000, time 0.08s\n",
      "  # 1316\n",
      "    src: Một trong những điều mà chúng tôi thực hiện tại trường đại học Radboud là bổ nhiệm một chuyên viên lắng nghe\n",
      "    ref: And one of the things we did at Radboud University is we appointed a chief listening officer .\n",
      "    nmt: One of the things we did at the University of <unk> was the <unk> of a <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-9000, time 0.08s\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 16s, Mon Apr  2 12:11:50 2018.\n",
      "  bleu dev: 14.3\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 16s, Mon Apr  2 12:12:07 2018.\n",
      "  bleu test: 14.1\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 9400 lr 0.0005 step-time 0.81s wps 5.50K ppl 5.26 gN 6.67 bleu 14.27, Mon Apr  2 12:12:29 2018\n",
      "  step 9500 lr 0.0005 step-time 0.72s wps 6.46K ppl 4.39 gN 6.28 bleu 14.27, Mon Apr  2 12:13:41 2018\n",
      "  step 9600 lr 0.0005 step-time 0.71s wps 6.46K ppl 4.43 gN 6.35 bleu 14.27, Mon Apr  2 12:14:52 2018\n",
      "  step 9700 lr 0.0005 step-time 0.71s wps 6.47K ppl 4.53 gN 6.48 bleu 14.27, Mon Apr  2 12:16:04 2018\n",
      "  step 9800 lr 0.0005 step-time 0.72s wps 6.47K ppl 4.60 gN 6.59 bleu 14.27, Mon Apr  2 12:17:15 2018\n",
      "  step 9900 lr 0.0005 step-time 0.71s wps 6.42K ppl 4.53 gN 6.52 bleu 14.27, Mon Apr  2 12:18:26 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 10000 lr 0.0005 step-time 0.72s wps 6.45K ppl 4.59 gN 6.64 bleu 14.27, Mon Apr  2 12:19:38 2018\n",
      "# Save eval, global step 10000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-10000, time 0.08s\n",
      "  # 341\n",
      "    src: Quan toà chứng nhận đó là một người trưởng thành , nhưng tôi thấy cậu ấy vẫn còn là một đứa trẻ\n",
      "    ref: And the judge has certified him as an adult , but I see this kid .\n",
      "    nmt: The judge was an adult , but I saw him still as a child .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-10000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-10000, time 0.09s\n",
      "  eval dev: perplexity 15.89, time 5s, Mon Apr  2 12:19:44 2018.\n",
      "  eval test: perplexity 15.94, time 5s, Mon Apr  2 12:19:50 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-10000, time 0.08s\n",
      "  # 1483\n",
      "    src: Và chúng ta , ở phương Tây không thể hiểu tại sao những điều đó lại có thể là sự thật khi những phương thức đó là kẻ thù của tự do ngôn luận .\n",
      "    ref: And we in the West couldn &apos;t understand how anybody could do this , how much this would restrict freedom of speech .\n",
      "    nmt: And we , in the West , can &apos;t understand why that could be the real truth when the forms of the speech is the <unk> of the <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-10000, time 0.08s\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 12:20:10 2018.\n",
      "  bleu dev: 13.3\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 17s, Mon Apr  2 12:20:28 2018.\n",
      "  bleu test: 12.7\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 10100 lr 0.00025 step-time 0.72s wps 6.45K ppl 4.55 gN 6.54 bleu 14.27, Mon Apr  2 12:21:40 2018\n",
      "  step 10200 lr 0.00025 step-time 0.71s wps 6.44K ppl 4.58 gN 6.50 bleu 14.27, Mon Apr  2 12:22:51 2018\n",
      "  step 10300 lr 0.00025 step-time 0.72s wps 6.45K ppl 4.63 gN 6.58 bleu 14.27, Mon Apr  2 12:24:03 2018\n",
      "  step 10400 lr 0.00025 step-time 0.72s wps 6.45K ppl 4.57 gN 6.57 bleu 14.27, Mon Apr  2 12:25:14 2018\n",
      "# Finished an epoch, step 10430. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-10000, time 0.08s\n",
      "  # 623\n",
      "    src: Kĩ thuật thứ 4 của tôi : Điều kiện cho sự phức tạp\n",
      "    ref: My fourth technique : Condition for complexity .\n",
      "    nmt: My fourth arts : The event for complexity\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-10000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-10000, time 0.08s\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 12:25:51 2018.\n",
      "  bleu dev: 13.3\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 10000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 17s, Mon Apr  2 12:26:09 2018.\n",
      "  bleu test: 12.7\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 10500 lr 0.00025 step-time 0.81s wps 5.50K ppl 4.12 gN 6.34 bleu 14.27, Mon Apr  2 12:27:11 2018\n",
      "  step 10600 lr 0.00025 step-time 0.71s wps 6.45K ppl 4.01 gN 6.24 bleu 14.27, Mon Apr  2 12:28:22 2018\n",
      "  step 10700 lr 0.00025 step-time 0.71s wps 6.46K ppl 4.12 gN 6.34 bleu 14.27, Mon Apr  2 12:29:34 2018\n",
      "  step 10800 lr 0.00025 step-time 0.72s wps 6.46K ppl 4.12 gN 6.43 bleu 14.27, Mon Apr  2 12:30:45 2018\n",
      "  step 10900 lr 0.00025 step-time 0.71s wps 6.49K ppl 4.12 gN 6.40 bleu 14.27, Mon Apr  2 12:31:56 2018\n",
      "  step 11000 lr 0.00025 step-time 0.71s wps 6.46K ppl 4.11 gN 6.44 bleu 14.27, Mon Apr  2 12:33:07 2018\n",
      "# Save eval, global step 11000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-11000, time 0.08s\n",
      "  # 1047\n",
      "    src: Và chúng ta biết rằng nó không có ý nghĩa lắm khi nghĩ về 30 , 50 năm sau vì khi ấy tất cả mọi thứ sẽ trở nên rất khác biệt khiến một phép ngoại suy rất đơn giản mà chúng ta thực hiện hôm nay sẽ không có ý nghĩa gì trong thời điểm ấy .\n",
      "    ref: And we know that it just doesn &apos;t make too much sense to think out 30 , 50 years because everything &apos;s going to be so different that a simple extrapolation of what we &apos;re doing just doesn &apos;t make any sense at all .\n",
      "    nmt: And we know that there &apos;s no meaning to think about 30 , 50 years later because when all of the time it will become , there &apos;s no such thing as we can make a very simple sense of what we can do today .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-11000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-11000, time 0.10s\n",
      "  eval dev: perplexity 15.92, time 5s, Mon Apr  2 12:33:13 2018.\n",
      "  eval test: perplexity 15.72, time 5s, Mon Apr  2 12:33:19 2018.\n",
      "  step 11100 lr 0.000125 step-time 0.72s wps 6.47K ppl 4.18 gN 6.49 bleu 14.27, Mon Apr  2 12:34:31 2018\n",
      "  step 11200 lr 0.000125 step-time 0.71s wps 6.44K ppl 4.15 gN 6.42 bleu 14.27, Mon Apr  2 12:35:43 2018\n",
      "  step 11300 lr 0.000125 step-time 0.72s wps 6.49K ppl 4.17 gN 6.52 bleu 14.27, Mon Apr  2 12:36:55 2018\n",
      "  step 11400 lr 0.000125 step-time 0.71s wps 6.44K ppl 4.11 gN 6.38 bleu 14.27, Mon Apr  2 12:38:06 2018\n",
      "# Finished an epoch, step 11473. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-11000, time 0.08s\n",
      "  # 697\n",
      "    src: Tôi thi tốt , tôi tốt nghiệp xuất sắc , loại ưu .\n",
      "    ref: And I did well , I graduated with honors , cum laude .\n",
      "    nmt: I did so well , I graduated from the best , kind of <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-11000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-11000, time 0.08s\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 12:39:14 2018.\n",
      "  bleu dev: 13.7\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 11000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 17s, Mon Apr  2 12:39:32 2018.\n",
      "  bleu test: 13.7\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "  step 11500 lr 0.000125 step-time 0.81s wps 5.50K ppl 4.07 gN 6.40 bleu 14.27, Mon Apr  2 12:40:03 2018\n",
      "  step 11600 lr 0.000125 step-time 0.72s wps 6.47K ppl 3.91 gN 6.30 bleu 14.27, Mon Apr  2 12:41:15 2018\n",
      "  step 11700 lr 0.000125 step-time 0.71s wps 6.46K ppl 3.85 gN 6.28 bleu 14.27, Mon Apr  2 12:42:26 2018\n",
      "  step 11800 lr 0.000125 step-time 0.71s wps 6.48K ppl 3.93 gN 6.36 bleu 14.27, Mon Apr  2 12:43:37 2018\n",
      "  step 11900 lr 0.000125 step-time 0.71s wps 6.47K ppl 3.93 gN 6.35 bleu 14.27, Mon Apr  2 12:44:48 2018\n",
      "  step 12000 lr 0.000125 step-time 0.72s wps 6.46K ppl 3.92 gN 6.43 bleu 14.27, Mon Apr  2 12:46:00 2018\n",
      "# Save eval, global step 12000\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-12000, time 0.08s\n",
      "  # 581\n",
      "    src: Ttrước khi chúng tôi bắt đầu phần chúng tôi buổi chiều nay Tôi đã có cuộc nói chuyện với Gary\n",
      "    ref: Now before we started our session this afternoon , I had a chat with Gary .\n",
      "    nmt: So when we started this part of the evening , I was talking to the conversation tonight .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-12000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-12000, time 0.10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval dev: perplexity 16.20, time 5s, Mon Apr  2 12:46:06 2018.\n",
      "  eval test: perplexity 15.93, time 5s, Mon Apr  2 12:46:12 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-12000, time 0.08s\n",
      "  # 476\n",
      "    src: Và điều đó , với mức độ rộng lớn hơn , là cái chúng ta muốn làm hiện nay .\n",
      "    ref: And that , to a large extent , is what we want to do now .\n",
      "    nmt: And that , for a greater level , is what we want to do today .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-12000\n",
      "  loaded eval model parameters from nmt_model_ffatt/translate.ckpt-12000, time 0.10s\n",
      "  eval dev: perplexity 16.20, time 5s, Mon Apr  2 12:46:20 2018.\n",
      "  eval test: perplexity 15.93, time 5s, Mon Apr  2 12:46:26 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/translate.ckpt-12000\n",
      "  loaded infer model parameters from nmt_model_ffatt/translate.ckpt-12000, time 0.08s\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 17s, Mon Apr  2 12:46:44 2018.\n",
      "  bleu dev: 13.8\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 12000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 17s, Mon Apr  2 12:47:02 2018.\n",
      "  bleu test: 13.5\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# Final, step 12000 lr 0.000125 step-time 0.72s wps 6.46K ppl 3.92 gN 6.43 dev ppl 16.20, dev bleu 13.8, test ppl 15.93, test bleu 13.5, Mon Apr  2 12:47:02 2018\n",
      "# Done training!, time 9352s, Mon Apr  2 12:47:02 2018.\n",
      "# Start evaluating saved best models.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/best_bleu/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_ffatt/best_bleu/translate.ckpt-9000, time 0.08s\n",
      "  # 786\n",
      "    src: Bạn có thể tra bệnh này trên Google , nhưng nó là bệnh nhiễm khuẩn , không phải cổ họng , mà là phần trên của khí quản , và nó có thể dẫn đến tắc khí quản .\n",
      "    ref: You can Google it , but it &apos;s an infection , not of the throat , but of the upper airway , and it can actually cause the airway to close .\n",
      "    nmt: You can burrow down on Google , but it &apos;s a <unk> , not the <unk> ; it &apos;s the <unk> of the <unk> , and it &apos;s able to hit the <unk> .\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/best_bleu/translate.ckpt-9000\n",
      "  loaded eval model parameters from nmt_model_ffatt/best_bleu/translate.ckpt-9000, time 0.09s\n",
      "  eval dev: perplexity 15.48, time 5s, Mon Apr  2 12:47:08 2018.\n",
      "  eval test: perplexity 15.46, time 5s, Mon Apr  2 12:47:14 2018.\n",
      "INFO:tensorflow:Restoring parameters from nmt_model_ffatt/best_bleu/translate.ckpt-9000\n",
      "  loaded infer model parameters from nmt_model_ffatt/best_bleu/translate.ckpt-9000, time 0.08s\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_ffatt/output_dev.\n",
      "  done, num sentences 1553, num translations per input 1, time 16s, Mon Apr  2 12:47:30 2018.\n",
      "  bleu dev: 14.3\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# External evaluation, global step 9000\n",
      "  decoding to output nmt_model_ffatt/output_test.\n",
      "  done, num sentences 1268, num translations per input 1, time 16s, Mon Apr  2 12:47:48 2018.\n",
      "  bleu test: 14.1\n",
      "  saving hparams to nmt_model_ffatt/hparams\n",
      "# Best bleu, step 9000 lr 0.000125 step-time 0.72s wps 6.46K ppl 3.92 gN 6.43 dev ppl 15.48, dev bleu 14.3, test ppl 15.46, test bleu 14.1, Mon Apr  2 12:47:48 2018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'dev_ppl': 16.2045087774781,\n",
       "  'dev_scores': {'bleu': 13.786509957677342},\n",
       "  'test_ppl': 15.926899547146027,\n",
       "  'test_scores': {'bleu': 13.457804929373632}},\n",
       " 12000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an LSTM model with feedforward attention\n",
    "hparams = create_standard_hparams(data_path=os.path.join(\"datasets\", \"nmt_data_vi\"), \n",
    "                                  out_dir=\"nmt_model_ffatt\")\n",
    "hparams.add_hparam(\"attention_cell_class\", LSTMCellWithFeedForwardAttention)\n",
    "train(hparams, AttentionalModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
