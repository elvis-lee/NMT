{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "MAX_LENGTH = 25\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "hidden_size = 512\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "train_source = 'train.vi'\n",
    "train_target = 'train.en'\n",
    "train_source_dir = os.path.join(data_dir, train_source)\n",
    "train_target_dir = os.path.join(data_dir, train_target)\n",
    "vocab_source = 'vocab.vi'\n",
    "vocab_target = 'vocab.en'\n",
    "vocab_source_dir = os.path.join(data_dir, vocab_source)\n",
    "vocab_target_dir = os.path.join(data_dir, vocab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in source training set: 133317\n",
      "Total number of sentences in target training set: 133317\n"
     ]
    }
   ],
   "source": [
    "# load training sets\n",
    "with open(train_source_dir) as f_source:\n",
    "    sentences_source = f_source.readlines()\n",
    "with open(train_target_dir) as f_target:\n",
    "    sentences_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source training set: {}\".format(len(sentences_source)))\n",
    "print(\"Total number of sentences in target training set: {}\".format(len(sentences_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the longest sentence in sentences_source: 25\n",
      "The longest sentence: \n",
      "['Trong', '4', 'phút', ',', 'chuyên', 'gia', 'hoá', 'học', 'khí', 'quyển', 'Rachel', 'Pike', 'giới', 'thiệu', 'sơ', 'lược', 'về', 'những', 'nỗ', 'lực', 'khoa', 'học', 'miệt', 'mài', 'đằng']\n"
     ]
    }
   ],
   "source": [
    "# Truncate sentences by maximum length\n",
    "sentences_source = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_source))\n",
    "sentences_target = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_target))\n",
    "\n",
    "# check the longest sentence after sentence truncation\n",
    "max = 0\n",
    "for s in sentences_source:\n",
    "    if len(s) > max:\n",
    "        max = len(s)\n",
    "        max_s = s\n",
    "print(\"Number of words in the longest sentence in sentences_source: {}\".format(max))\n",
    "print(\"The longest sentence: \\n{}\".format(max_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nummber of words in source vocabulary: 7709\n",
      "Total nummber of words in target vocabulary: 17191\n"
     ]
    }
   ],
   "source": [
    "# load vocabularies\n",
    "\n",
    "# build index2word\n",
    "with open(vocab_source_dir) as f_vocab_source:\n",
    "    #index2word_source = f_vocab_source.readlines()\n",
    "    index2word_source = [line.rstrip() for line in f_vocab_source]\n",
    "with open(vocab_target_dir) as f_vocab_target:\n",
    "    #index2word_target = f_vocab_target.readlines()\n",
    "    index2word_target = [line.rstrip() for line in f_vocab_target]\n",
    "\n",
    "index2word_target = np.array(index2word_target)\n",
    "index2word_source = np.array(index2word_source)\n",
    "\n",
    "# build word2index\n",
    "word2index_source = {}\n",
    "for idx, word in enumerate(index2word_source):\n",
    "    word2index_source[word] = idx\n",
    "word2index_target = {}\n",
    "for idx, word in enumerate(index2word_target):\n",
    "    word2index_target[word] = idx\n",
    "    \n",
    "# check vocabularies size    \n",
    "source_vocab_size = len(index2word_source)\n",
    "target_vocab_size = len(index2word_target)\n",
    "print(\"Total nummber of words in source vocabulary: {}\".format(len(index2word_source)))\n",
    "print(\"Total nummber of words in target vocabulary: {}\".format(len(index2word_target)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funtions to convert sentence in natural language to list of word indexes\n",
    "def sen2idx(sentence, word2index):\n",
    "    return [word2index.get(word, 0) for word in sentence] # assume that 0 is for <unk>\n",
    "\n",
    "def sen2tensor(sentence, word2index):\n",
    "    idxes = sen2idx(sentence, word2index)\n",
    "    idxes.append(EOS_token)\n",
    "    return torch.tensor(idxes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token to be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = target_vocab_size # padding value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def sentences2tensor(sentences, word2index):\n",
    "    sentences_tensor = [sen2tensor(s, word2index) for s in sentences]\n",
    "    sentences_tensor.sort(key=len, reverse=True)\n",
    "    output = pad_sequence(sentences_tensor, batch_first=True)\n",
    "    return output\n",
    "\n",
    "def batch_generator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target):\n",
    "    #output: two PackedSequence object, two indexes to reorder sentences in a batch\n",
    "    \n",
    "    # generate id in one batch\n",
    "    total = len(sentences_source)\n",
    "    sample_id = np.random.choice(total, batch_size, replace=False)\n",
    "    \n",
    "    #generate a source batch\n",
    "    sentences_source_tensor = [sen2tensor(sentences_source[id], word2index_source) for id in sample_id]\n",
    "    \n",
    "    len_array_source = [len(st) for st in sentences_source_tensor]\n",
    "    reorder_idx_source = np.argsort(len_array_source, kind='mergesort')\n",
    "    reorder_idx_source = np.argsort(np.flip(reorder_idx_source, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_source_tensor.sort(key=len)\n",
    "    sentences_source_tensor.reverse()\n",
    "    sentences_source_packed = pack_sequence(sentences_source_tensor)\n",
    "    \n",
    "    #generate a target batch\n",
    "    sentences_target_tensor = [sen2tensor(sentences_target[id], word2index_target) for id in sample_id]\n",
    "    \n",
    "    len_array_target = [len(st) for st in sentences_target_tensor]\n",
    "    reorder_idx_target = np.argsort(len_array_target, kind='mergesort')\n",
    "    reorder_idx_target = np.argsort(np.flip(reorder_idx_target, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_target_tensor.sort(key=len)\n",
    "    sentences_target_tensor.reverse()\n",
    "    sentences_target_packed = pack_sequence(sentences_target_tensor)\n",
    "    \n",
    "    return (sentences_source_packed, reorder_idx_source), (sentences_target_packed, reorder_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([   20,     8,  1352,     2,     2]), batch_sizes=tensor([ 2,  2,  1]))\n",
      "[0 1]\n",
      "(tensor([[   20,  1352,     2],\n",
      "        [    8,     2,     0]]), tensor([ 3,  2]))\n"
     ]
    }
   ],
   "source": [
    "# test batch_generator\n",
    "a = ['I','am','a','boy']\n",
    "b = ['a']\n",
    "c = ['the','goat']\n",
    "\n",
    "sentences_test = [a,b,c]\n",
    "sentences_test2 = [c,b,a]\n",
    "\n",
    "(output_test, reorder_idx_test), (output_test2, reorder_idx_test2) = batch_generator(2, sentences_test, sentences_test2, word2index_target, word2index_target)\n",
    "print(output_test)\n",
    "print(reorder_idx_test)\n",
    "print(pad_packed_sequence(output_test, batch_first=True))\n",
    "\n",
    "del a, b, c, sentences_test, sentences_test2, output_test, reorder_idx_test, output_test2, reorder_idx_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to resume order of sentences in a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order recovery is necessary because sentences have to be sorted in descending order of sentence length to be packed as a PackedSequence object. PackedSequence object helps to deal with inputs with variable length in NMT setting. LSTM, RNN can accept PackedSequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_order(input, idx):\n",
    "    # input \n",
    "    #   input: Tensor: (batch_size, seq_length)\n",
    "    #   idx: Tensor or ndarray: (batch_size)\n",
    "    # output\n",
    "    #   out: Tensor with reordered sentences in batch: (batch_size, seq_length) \n",
    "    \n",
    "    if isinstance(idx, (np.ndarray)):\n",
    "        idx = torch.from_numpy(idx)\n",
    "        if device == torch.device(\"cuda\"):\n",
    "            idx = idx.cuda()\n",
    "    out = torch.index_select(input, 0, idx)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  8,  9],\n",
      "        [ 4,  5,  6],\n",
      "        [ 1,  2,  3]])\n",
      "tensor([[ 7,  8,  9],\n",
      "        [ 4,  5,  6],\n",
      "        [ 1,  2,  3]])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# test resume_order\n",
    "input_test = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],device=device)\n",
    "idx_test = np.array([2,1,0])\n",
    "print(resume_order(input_test, idx_test))\n",
    "input_test = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],device=device)\n",
    "idx_test = torch.tensor([2,1,0], device=device)\n",
    "print(resume_order(input_test, idx_test))\n",
    "print(idx_test.device)\n",
    "del input_test, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Encoder and Decoder classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1, num_directions=1, dropout=0):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "    def forward(self, input_tuple, prev_h, prev_c):\n",
    "        # input\n",
    "        # input size: (batch_size, seq_length)\n",
    "        # prev_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # prec_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # output\n",
    "        # h_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # c_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        (sentences_packed, reorder_idx) = input_tuple\n",
    "        sentences_tensor, sentences_length = pad_packed_sequence(sentences_packed, batch_first=True, padding_value=0)\n",
    "        #sentences_tensor: (batch_size, seq_length)\n",
    "        input_embedded = self.embedding(sentences_tensor) # (batch_size, seq_length, hidden_size)\n",
    "        input_embedded_packed = pack_padded_sequence(input_embedded, sentences_length, batch_first=True)\n",
    "        _, (h_n, c_n) = self.lstm(input_embedded_packed, (prev_h, prev_c))\n",
    "        return h_n, c_n\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers and num_directions for encoder must be 0 when this decoder is used\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, prev_h, prev_c):\n",
    "        input_embedded = self.embedding(input)\n",
    "        h, c = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "        output =self.softmax(self.out(h))\n",
    "        return output, h, c\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DecoderLSTM can't do teacher forcing in training\n",
    "\n",
    "# class DecoderLSTM(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, batch_size, num_layers=1, num_directions=1, dropout=0):\n",
    "#         super(DecoderLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.num_directions = num_directions\n",
    "#         self.dropout = dropout\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "#         self.out = nn.Linear(hidden_size*num_directions, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=2)\n",
    "#     def forward(self, input, prev_h, prev_c):\n",
    "#         # input \n",
    "#         # input size: (batch_size, seq_length)\n",
    "#         # prev_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # prec_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # output\n",
    "#         # h_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # c_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # output size: (batch_size, seq_length, output_size)\n",
    "#         input_embedded = self.embedding(input)\n",
    "#         output, (h_n, c_n) = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "#         output =self.softmax(self.out(h))\n",
    "#         return output, h_n, c_n\n",
    "#     def initHidden(self, batch_size):\n",
    "#         return torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer,batch_size, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden_h = encoder.initHidden()\n",
    "    encoder_hidden_c = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token, size_average=False)\n",
    "    \n",
    "    (sentences_source_packed, reorder_idx_source) = source_tuple\n",
    "    (sentences_target_packed, reorder_idx_target) = target_tuple\n",
    "    sentences_target_tensor, sentences_target_length = pad_packed_sequence(sentences_target_packed, batch_first=True, padding_value=PAD_token)\n",
    "    sentences_target_tensor = resume_order(sentences_target_tensor, reorder_idx_target)\n",
    "    \n",
    "\n",
    "    target_length = sentences_target_tensor.size(1)\n",
    "    #print(\"target length: {}\".format(target_length)) # to be removed\n",
    "    #print(\"target sentence: {}\".format(sentences_target_tensor)) # to be removed\n",
    "    \n",
    "    # encoder_hidden_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    # encoder_hidden_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    encoder_hidden_h, encoder_hidden_c = encoder(source_tuple, encoder_hidden_h, encoder_hidden_c)\n",
    "    \n",
    "    \n",
    "    decoder_input = torch.full((batch_size,), SOS_token, dtype=torch.long, device=device)\n",
    "    decoder_hidden_c = resume_order(encoder_hidden_c[0], reorder_idx_source)\n",
    "    decoder_hidden_h = resume_order(encoder_hidden_h[0], reorder_idx_source)\n",
    "    \n",
    "    to_print = []\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden_h, decoder_hidden_c = decoder(decoder_input, decoder_hidden_h, decoder_hidden_c)\n",
    "        loss += criterion(decoder_output, sentences_target_tensor[:,di])\n",
    "        decoder_input = sentences_target_tensor[:,di]\n",
    "        \n",
    "        #to be removed\n",
    "        decoder_output_np = np.argmax(decoder_output.detach().cpu().numpy(), 1)\n",
    "        if di == 0: \n",
    "            to_print = index2word_target[decoder_output_np]\n",
    "        else:\n",
    "            to_print = np.column_stack((to_print, index2word_target[decoder_output_np]))\n",
    "        #####\n",
    "    \n",
    "    denominator = torch.sum(sentences_target_length).float()\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        denominator = denominator.cuda()\n",
    "    loss = loss / denominator\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item(), to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a batch_generator for overfitting test only\n",
    "# this batch_generator will generate a batch containing selected sentences only\n",
    "\n",
    "def batch_generator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target):\n",
    "    #output: two PackedSequence object, two indexes to reorder sentences in a batch\n",
    "    \n",
    "    # generate id in one batch\n",
    "    total = len(sentences_source)\n",
    "#     sample_id = np.random.choice(total, batch_size, replace=False)\n",
    "    sample_id = np.array([20, 26, 28, 44, 49, 52, 56, 57, 64, 66])\n",
    "    \n",
    "    #generate a source batch\n",
    "    sentences_source_tensor = [sen2tensor(sentences_source[id], word2index_source) for id in sample_id]\n",
    "    \n",
    "    len_array_source = [len(st) for st in sentences_source_tensor]\n",
    "    reorder_idx_source = np.argsort(len_array_source, kind='mergesort')\n",
    "    reorder_idx_source = np.argsort(np.flip(reorder_idx_source, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_source_tensor.sort(key=len)\n",
    "    sentences_source_tensor.reverse()\n",
    "    sentences_source_packed = pack_sequence(sentences_source_tensor)\n",
    "    \n",
    "    #generate a target batch\n",
    "    sentences_target_tensor = [sen2tensor(sentences_target[id], word2index_target) for id in sample_id]\n",
    "    \n",
    "    len_array_target = [len(st) for st in sentences_target_tensor]\n",
    "    reorder_idx_target = np.argsort(len_array_target, kind='mergesort')\n",
    "    reorder_idx_target = np.argsort(np.flip(reorder_idx_target, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_target_tensor.sort(key=len)\n",
    "    sentences_target_tensor.reverse()\n",
    "    sentences_target_packed = pack_sequence(sentences_target_tensor)\n",
    "    \n",
    "    return (sentences_source_packed, reorder_idx_source), (sentences_target_packed, reorder_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run this cell\n",
    "# print out target sentences and select some to be put into the small dataset\n",
    "for idx, s in enumerate(sentences_target):\n",
    "    print(\"idx:{}, sentences:{}\".format(idx,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small dataset for overfitting test\n",
    "\n",
    "20: ['We', 'blow', 'it', 'up', 'and', 'look', 'at', 'the', 'pieces', '.'] length 11\n",
    "\n",
    "26: ['And', 'it', 'takes', 'weeks', 'to', 'perform', 'our', 'integrations', '.'] length 10\n",
    "\n",
    "28: ['We', 'also', 'fly', 'all', 'over', 'the', 'world', 'looking', 'for', 'this', 'thing', '.'] length 13\n",
    "\n",
    "44: ['We', 'do', 'all', 'of', 'this', 'to', 'understand', 'the', 'chemistry', 'of', 'one', 'molecule', '.'] length 14\n",
    "\n",
    "49: ['So', 'you', 'can', 'imagine', 'the', 'scale', 'of', 'the', 'effort', '.'] length 11\n",
    "\n",
    "52: ['Thank', 'you', 'very', 'much', '.'] length 6\n",
    "\n",
    "56: ['You', 'can', 'mimic', 'what', 'you', 'can', 'see', '.'] length 9\n",
    "\n",
    "57: ['You', 'can', 'program', 'the', 'hundreds', 'of', 'muscles', 'in', 'your', 'arm', '.'] length 12\n",
    "\n",
    "64: ['It', 'was', 'terribly', 'dangerous', '.'] length 6\n",
    "\n",
    "66: ['But', 'now', ',', 'we', 'have', 'a', 'real', 'technology', 'to', 'do', 'this', '.'] length 13\n",
    "\n",
    "    index to reorder [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 # to be removed\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.1):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), learning_rate)\n",
    "    \n",
    "    for iter in range(1, n_iters+1):\n",
    "        source_tuple, target_tuple = batch_generator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target)\n",
    "        loss, to_print = train(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter%print_every ==0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(%d %d%%) %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
    "            print(to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10 0%) 9.5885\n",
      "[['We' 'polio' 'Dear' 'pedal' 'harness' 'analogies' 'workshop' 'talents'\n",
      "  'occasional' '.' '</s>' '</s>' 'Kenyan' 'Kenyan']\n",
      " ['We' 'Brendan' 'takes' 'subject' 'equals' 'to' 'Genspace' '13th' '.'\n",
      "  '</s>' '</s>' 'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'polio' 'symptom' 'catadores' 'increasingly' 'cookies' 'effort'\n",
      "  '.' 'rested' 'JN' '.' '.' '</s>' '</s>']\n",
      " ['We' 'polio' 'Left' 'eureka' 'municipal' 'sings' 'to' 'to' 'effort'\n",
      "  'mounting' 'exploding' 'midair' 'Stanley' '</s>']\n",
      " ['We' 'We' 'can' 'the' 'the' '.' '.' 'the' 'effort' '</s>' '</s>' '</s>'\n",
      "  'Kenyan' 'Kenyan']\n",
      " ['We' 'explores' 'can' '</s>' '</s>' '</s>' '</s>' 'Kenyan' 'Kenyan'\n",
      "  'Kenyan' 'Kenyan' 'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'job' 'the' 'initiatives' 'store' 'can' 'the' 'yearly' '</s>'\n",
      "  '</s>' 'Kenyan' 'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'job' 'the' 'the' 'LOL' 'LOL' 'wandering' '</s>' 'compute'\n",
      "  'adolescence' 'TiVo' '</s>' '</s>' 'Kenyan']\n",
      " ['We' '.' 'plea' 'nonetheless' '.' '</s>' '</s>' 'Kenyan' 'Kenyan'\n",
      "  'Kenyan' 'Kenyan' 'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'laborers' 'misdirection' 'dazzling' 'a' 'let' 'spreading' '</s>'\n",
      "  'repairs' 'to' 'weeping' 'pick' '</s>' '</s>']]\n",
      "(20 0%) 9.1370\n",
      "[['We' 'We' 'it' 'takes' '.' 'look' 'at' 'the' '.' '.' '</s>' '</s>'\n",
      "  '</s>' 'Kenyan']\n",
      " ['We' 'We' 'takes' 'weeks' 'to' 'to' 'our' '.' '.' '</s>' '</s>' '</s>'\n",
      "  'Kenyan' 'Kenyan']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' '.' '.' '</s>' '</s>' '.' '.'\n",
      "  '</s>' '</s>']\n",
      " ['We' 'also' '.' '.' 'the' '.' '.' 'the' '.' 'of' 'the' 'the' '.' '</s>']\n",
      " ['We' 'We' 'can' '.' 'the' '.' '.' 'the' '.' '.' '</s>' '</s>' '</s>'\n",
      "  'Kenyan']\n",
      " ['We' 'We' 'can' '</s>' '</s>' '</s>' '</s>' '</s>' 'Kenyan' 'Kenyan'\n",
      "  'Kenyan' 'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'can' 'the' 'the' 'you' 'can' '.' '.' '</s>' '</s>' '</s>'\n",
      "  'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'can' 'the' 'the' '.' 'of' 'the' '</s>' '</s>' '</s>' '.' '</s>'\n",
      "  '</s>' '</s>']\n",
      " ['We' 'We' 'plea' '.' '.' '</s>' '</s>' '</s>' 'Kenyan' 'Kenyan'\n",
      "  'Kenyan' 'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'We' 'We' 'we' 'a' 'a' '</s>' '</s>' '</s>' 'to' '.' '.' '</s>'\n",
      "  '</s>']]\n",
      "(30 0%) 8.5182\n",
      "[['We' 'We' 'it' 'up' '.' 'look' 'at' 'the' '.' '.' '</s>' '</s>' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' 'takes' 'weeks' 'to' 'to' 'our' '.' '.' '</s>' '</s>' '</s>'\n",
      "  '</s>' 'Kenyan']\n",
      " ['We' 'We' 'fly' 'all' '</s>' 'the' '.' '.' '</s>' '</s>' '</s>' '</s>'\n",
      "  '</s>' '</s>']\n",
      " ['We' 'We' '.' '</s>' '</s>' '</s>' '.' 'the' '.' '</s>' 'the' '.' '.'\n",
      "  '</s>']\n",
      " ['We' 'We' 'can' '.' 'the' '.' '.' 'the' '.' '</s>' '</s>' '</s>' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' 'can' '</s>' '</s>' '</s>' '</s>' '</s>' '</s>' 'Kenyan'\n",
      "  'Kenyan' 'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'can' '.' 'the' 'you' 'can' '.' '.' '</s>' '</s>' '</s>' '</s>'\n",
      "  'the' 'Kenyan']\n",
      " ['We' 'can' '.' 'the' '.' 'of' 'the' '</s>' '</s>' '</s>' '.' '</s>'\n",
      "  '</s>' '</s>']\n",
      " ['We' 'We' 'We' '.' '.' '</s>' '</s>' '</s>' '</s>' 'Kenyan' 'Kenyan'\n",
      "  'Kenyan' 'Kenyan' 'Kenyan']\n",
      " ['We' 'We' 'We' 'we' 'a' 'a' '</s>' '</s>' '</s>' 'to' '.' '.' '</s>'\n",
      "  '</s>']]\n",
      "(40 0%) 7.8510\n",
      "[['We' 'We' 'it' 'up' '.' 'look' 'at' 'the' '.' '.' '</s>' '</s>' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' 'takes' '.' 'to' '.' 'our' '.' '.' '</s>' '</s>' '</s>'\n",
      "  '</s>' 'the']\n",
      " ['We' 'We' 'We' 'all' '.' 'the' '.' '.' '</s>' '</s>' '.' '.' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' '.' '.' '.' '.' '.' 'the' '.' '.' '.' '.' '.' '</s>']\n",
      " ['We' 'We' 'can' '.' 'the' '.' '.' 'the' '.' '.' '</s>' '</s>' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' 'can' '</s>' '</s>' '</s>' '</s>' '</s>' '</s>' '</s>' 'the'\n",
      "  'the' 'the' 'the']\n",
      " ['We' 'can' '.' 'the' 'can' 'can' '.' '.' '</s>' '</s>' '</s>' '</s>'\n",
      "  'the' 'the']\n",
      " ['We' 'can' '.' 'the' '.' 'of' 'the' '.' '</s>' '</s>' '.' '</s>' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' 'We' '.' '.' '</s>' '</s>' '</s>' '</s>' 'the' 'the' 'the'\n",
      "  'the' 'the']\n",
      " ['We' 'We' 'We' 'we' 'a' 'a' '</s>' '</s>' '</s>' '.' '.' '.' '</s>'\n",
      "  '</s>']]\n",
      "(50 0%) 7.3091\n",
      "[['We' 'We' 'We' 'up' '.' 'look' 'the' 'the' '.' '.' '</s>' '</s>' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' '.' '.' 'to' '.' 'our' '.' '.' '</s>' '</s>' '</s>' '</s>'\n",
      "  '.']\n",
      " ['We' 'We' 'We' 'all' '.' 'the' '.' '.' '.' '.' '.' '.' '</s>' '</s>']\n",
      " ['We' 'We' '.' '.' '.' '.' '.' 'the' '.' '.' '.' '.' '.' '</s>']\n",
      " ['We' 'We' 'can' '.' 'the' '.' '.' '.' '.' '.' '</s>' '</s>' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' 'can' '.' '.' '</s>' '</s>' '</s>' '</s>' '.' 'the' 'the'\n",
      "  'the' 'the']\n",
      " ['We' 'can' '.' '.' '.' 'can' '.' '.' '</s>' '</s>' '</s>' '</s>' '.'\n",
      "  'the']\n",
      " ['We' 'can' '.' 'the' '.' '.' '.' '.' '</s>' '.' '.' '</s>' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'We' 'We' '.' '.' '</s>' '</s>' '</s>' '</s>' '.' 'the' 'the'\n",
      "  'the' 'the']\n",
      " ['We' 'We' 'We' '.' '.' 'a' '</s>' '</s>' '</s>' '.' '.' '.' '</s>'\n",
      "  '</s>']]\n",
      "(60 0%) 6.6020\n",
      "[['We' 'We' 'We' '.' '.' '.' 'the' 'the' '.' '.' '</s>' '</s>' '</s>' '.']\n",
      " ['We' 'We' '.' '.' '.' '.' '.' '.' '.' '</s>' '</s>' '</s>' '.' '.']\n",
      " ['We' 'We' '.' '.' '.' 'the' '.' '.' '.' '.' '.' '.' '</s>' '</s>']\n",
      " ['We' 'We' '.' '.' '.' '.' '.' '.' '.' '.' '.' '.' '.' '</s>']\n",
      " ['We' 'We' 'can' '.' 'the' '.' '.' '.' '.' '.' '</s>' '</s>' '</s>' '.']\n",
      " ['We' 'We' 'can' '.' '.' '</s>' '</s>' '</s>' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' '.' '.' '.' 'can' '.' '.' '</s>' '</s>' '</s>' '.' '.' '.']\n",
      " ['We' 'can' '.' 'the' '.' '.' '.' '.' '.' '.' '.' '</s>' '</s>' '</s>']\n",
      " ['We' 'We' '.' '.' '.' '</s>' '</s>' '</s>' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'We' '.' '.' '.' '.' '.' '.' '.' '.' '.' '.' '</s>' '</s>']]\n",
      "(70 0%) 5.6720\n",
      "[['We' 'can' 'can' '.' '.' '.' 'the' 'the' '.' '.' '</s>' '</s>' '.' '.']\n",
      " ['We' '.' '.' '.' '.' '.' '.' '.' '.' '</s>' '</s>' '.' '.' '.']\n",
      " ['We' 'can' '.' '.' '.' 'the' '.' '.' '.' '.' '.' '.' '</s>' '</s>']\n",
      " ['We' 'can' '.' '.' '.' '.' '.' '.' '.' '.' '.' '.' '.' '</s>']\n",
      " ['We' 'can' 'can' '.' 'the' '.' '.' '.' '.' '.' '</s>' '</s>' '.' '.']\n",
      " ['We' 'can' 'can' '.' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' '.' '.' '.' 'can' '.' '.' '</s>' '</s>' '.' '.' '.' '.']\n",
      " ['We' 'can' '.' 'the' '.' '.' '.' '.' '.' '.' '.' '</s>' '</s>' '.']\n",
      " ['We' 'can' '.' '.' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' '.' '.' '.' '.' '.' '.' '.' '.' '.' '.' '.' '</s>' '</s>']]\n",
      "(80 0%) 4.7392\n",
      "[['We' 'can' 'can' 'the' '.' 'the' 'the' 'the' '.' '.' '</s>' '</s>' '.'\n",
      "  '.']\n",
      " ['We' 'can' '.' '.' '.' 'the' 'the' '.' '.' '</s>' '</s>' '.' '.' '.']\n",
      " ['We' 'can' 'the' 'the' 'the' 'the' '.' '.' '.' '.' '.' '.' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'can' '.' 'the' 'the' '.' '.' 'the' '.' '.' 'the' '.' '.' '</s>']\n",
      " ['We' 'can' 'can' 'the' 'the' '.' '.' 'the' '.' '.' '</s>' '.' '.' '.']\n",
      " ['We' 'you' 'can' '.' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' 'the']\n",
      " ['We' 'can' 'the' 'the' 'can' 'can' '.' '.' '</s>' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'the' 'the' 'the' 'of' 'the' '.' '.' '.' '.' '</s>' '</s>'\n",
      "  '.']\n",
      " ['We' 'can' '.' '.' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' 'the']\n",
      " ['We' 'can' 'can' 'the' 'the' '.' '.' '.' '.' 'the' '.' '.' '</s>' '.']]\n",
      "(90 0%) 3.9096\n",
      "[['We' 'you' 'you' 'the' '.' 'the' 'the' 'the' 'of' '.' '</s>' '.' '.'\n",
      "  '.']\n",
      " ['We' 'you' '.' '.' 'to' 'the' 'the' '.' '.' '</s>' '.' '.' '.' '.']\n",
      " ['We' 'you' 'the' 'the' 'of' 'the' 'of' '.' '.' '.' '.' '.' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'you' 'of' 'of' 'the' '.' 'the' 'the' 'of' 'of' 'the' '.' '.'\n",
      "  '</s>']\n",
      " ['We' 'you' 'can' 'the' 'the' 'of' 'of' 'the' 'of' '.' '</s>' '.' '.'\n",
      "  '.']\n",
      " ['We' 'you' 'can' '.' '.' '</s>' '</s>' '.' '.' '.' '.' '.' 'the' 'the']\n",
      " ['We' 'can' 'can' 'the' 'you' 'can' 'the' '.' '</s>' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'can' 'the' 'of' 'of' 'the' '.' '.' '.' '.' '</s>' '</s>'\n",
      "  '.']\n",
      " ['We' 'you' 'you' '.' '.' '</s>' '.' '.' '.' '.' '.' 'the' 'the' 'the']\n",
      " ['We' 'you' 'can' 'the' 'the' '.' '.' 'the' 'to' 'the' '.' '.' '</s>'\n",
      "  '.']]\n",
      "(100 0%) 3.1315\n",
      "[['We' 'do' 'it' 'it' 'the' 'the' 'the' 'the' 'of' '.' '</s>' '.' '.' '.']\n",
      " ['We' 'it' 'it' 'to' 'to' 'to' 'to' '.' '.' '</s>' '.' '.' '.' '.']\n",
      " ['We' 'do' 'all' 'all' 'of' 'the' 'of' '.' 'of' 'this' '.' '.' '</s>'\n",
      "  '.']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'to' 'the' 'of' 'of' 'the' '.' '.'\n",
      "  '</s>']\n",
      " ['We' 'you' 'can' 'can' 'the' 'of' 'of' 'the' 'of' '.' '</s>' '.' '.'\n",
      "  '.']\n",
      " ['We' 'you' 'can' '.' '.' '</s>' '</s>' '.' '.' '.' '.' '.' 'the' 'the']\n",
      " ['We' 'can' 'can' 'the' 'you' 'can' 'the' '.' '</s>' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'can' 'the' 'of' 'of' 'the' '.' '.' '.' '.' '</s>' '</s>'\n",
      "  '.']\n",
      " ['We' 'you' 'you' '.' '.' '</s>' '.' '.' '.' '.' '.' '.' 'the' 'the']\n",
      " ['We' 'you' 'you' 'the' 'the' '.' 'to' 'to' 'to' 'to' 'this' '.' '</s>'\n",
      "  '.']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110 0%) 2.4073\n",
      "[['We' 'do' 'it' 'it' 'it' 'the' 'the' 'the' 'of' '.' '</s>' '.' '.' '.']\n",
      " ['We' 'it' 'it' 'to' 'to' 'to' 'to' '.' '.' '</s>' '.' '.' '.' '.']\n",
      " ['We' 'do' 'all' 'all' 'of' 'the' 'of' 'of' 'this' 'this' '.' '.' '</s>'\n",
      "  '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'to' 'the' 'of' 'of' 'this' '.' '.'\n",
      "  '</s>']\n",
      " ['We' 'you' 'can' 'can' 'the' 'of' 'of' 'the' 'of' '.' '</s>' '.' '.'\n",
      "  '.']\n",
      " ['We' 'you' 'can' '.' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' 'the']\n",
      " ['We' 'can' 'can' 'you' 'you' 'can' 'the' '.' '</s>' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'can' 'the' 'of' 'of' 'the' '.' '.' '.' '.' '</s>' '</s>'\n",
      "  '.']\n",
      " ['We' 'you' 'it' '.' '.' '</s>' '.' '.' '.' '.' '.' '.' 'the' 'the']\n",
      " ['We' 'you' ',' 'we' 'the' 'a' 'to' 'to' 'to' 'to' 'this' '.' '</s>' '.']]\n",
      "(120 0%) 1.7500\n",
      "[['We' 'do' 'it' 'up' 'and' 'the' 'the' 'the' 'of' '.' '</s>' '</s>' '.'\n",
      "  '.']\n",
      " ['We' 'it' 'it' 'to' 'to' 'do' 'to' '.' '.' '</s>' '.' '.' '.' '.']\n",
      " ['We' 'do' 'all' 'all' 'of' 'the' 'world' 'looking' 'this' 'this' '.'\n",
      "  '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'this' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'of' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'can' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'program' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'can' 'program' 'the' 'of' 'of' 'the' '.' '.' '.' '.' '</s>'\n",
      "  '</s>' '.']\n",
      " ['We' 'was' 'terribly' '.' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.'\n",
      "  'the']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'to' 'to' 'do' 'this' '.' '</s>'\n",
      "  '</s>']]\n",
      "(130 0%) 1.2255\n",
      "[['We' 'do' 'it' 'up' 'and' 'look' 'the' 'the' 'pieces' '.' '</s>' '</s>'\n",
      "  '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '.' '.' '</s>' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'can' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'program' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' '.'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(140 0%) 0.8869\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(150 0%) 0.6949\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(160 0%) 0.5838\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(170 0%) 0.5128\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(180 0%) 0.4632\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(190 0%) 0.4263\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(200 0%) 0.3977\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210 0%) 0.3749\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(220 0%) 0.3563\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(230 0%) 0.3407\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(240 0%) 0.3276\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'do' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(250 0%) 0.3162\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(260 0%) 0.3062\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(270 0%) 0.2972\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(280 0%) 0.2890\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '.' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(290 0%) 0.2814\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300 0%) 0.2742\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['We' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(310 0%) 0.2674\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(320 0%) 0.2607\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['We' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(330 0%) 0.2541\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(340 0%) 0.2475\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(350 0%) 0.2409\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['You' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(360 0%) 0.2341\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['You' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['We' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(370 0%) 0.2271\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['You' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['You' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.'\n",
      "  '.' '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(380 0%) 0.2199\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['You' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['You' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.'\n",
      "  '.' '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390 0%) 0.2125\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['You' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['You' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.'\n",
      "  '.' '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(400 0%) 0.2049\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['You' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(410 0%) 0.1971\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['You' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(420 0%) 0.1892\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['You' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.' '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(430 0%) 0.1811\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['Thank' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['We' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this' '.'\n",
      "  '</s>' '</s>']]\n",
      "(440 0%) 0.1731\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['We' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['Thank' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['But' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this'\n",
      "  '.' '</s>' '</s>']]\n",
      "(450 0%) 0.1651\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['So' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['Thank' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['But' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this'\n",
      "  '.' '</s>' '</s>']]\n",
      "(460 0%) 0.1571\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['So' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['Thank' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['But' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this'\n",
      "  '.' '</s>' '</s>']]\n",
      "(470 0%) 0.1493\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['We' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['So' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['Thank' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['But' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this'\n",
      "  '.' '</s>' '</s>']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480 0%) 0.1417\n",
      "[['We' 'blow' 'it' 'up' 'and' 'look' 'at' 'the' 'pieces' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['And' 'it' 'takes' 'weeks' 'to' 'perform' 'our' '<unk>' '.' '</s>'\n",
      "  '</s>' '.' '.' '.']\n",
      " ['We' 'also' 'fly' 'all' 'over' 'the' 'world' 'looking' 'for' 'this'\n",
      "  'thing' '.' '</s>' '</s>']\n",
      " ['We' 'do' 'all' 'of' 'this' 'to' 'understand' 'the' 'chemistry' 'of'\n",
      "  'one' 'molecule' '.' '</s>']\n",
      " ['So' 'you' 'can' 'imagine' 'the' 'scale' 'of' 'the' 'effort' '.' '</s>'\n",
      "  '</s>' '.' '.']\n",
      " ['Thank' 'you' 'very' 'much' '.' '</s>' '</s>' '.' '.' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'mimic' 'what' 'you' 'can' 'see' '.' '</s>' '.' '.' '.' '.'\n",
      "  '.']\n",
      " ['You' 'can' 'program' 'the' 'hundreds' 'of' 'muscles' 'in' 'your' 'arm'\n",
      "  '.' '</s>' '</s>' '.']\n",
      " ['It' 'was' 'terribly' 'dangerous' '.' '</s>' '</s>' '.' '.' '.' '.' '.'\n",
      "  '.' '.']\n",
      " ['But' 'now' ',' 'we' 'have' 'a' 'real' 'technology' 'to' 'do' 'this'\n",
      "  '.' '</s>' '</s>']]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ac52be0d94fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# +1 is a wordaround for ignore_index field of NLLLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m133317\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-e74fce348579>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msource_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2index_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2index_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_print\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-3ca446bd4d85>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size, max_length)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/pytorch/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/pytorch/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderLSTM(source_vocab_size, hidden_size, batch_size).to(device)\n",
    "decoder1 = DecoderLSTM(hidden_size, target_vocab_size+1, batch_size).to(device) # +1 is a wordaround for ignore_index field of NLLLoss\n",
    "trainIters(encoder1, decoder1, 133317, print_every=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
