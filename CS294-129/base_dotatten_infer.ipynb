{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update:\n",
    "1. Add setBatchSize function to both encoder and decoder classes.\n",
    "2. (todo)Change initHidden function of decoder. Now it produces SOS_token for decoder_input. (initHidden function of decoder has never been called before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "MAX_LENGTH = 25\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\") # to be removed\n",
    "print(device)\n",
    "\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "hidden_size = 512\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "attention_vector_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "train_source = 'train.vi'\n",
    "train_target = 'train.en'\n",
    "train_source_dir = os.path.join(data_dir, train_source)\n",
    "train_target_dir = os.path.join(data_dir, train_target)\n",
    "\n",
    "test_source = 'tst2012.vi'\n",
    "test_target = 'tst2012.en'\n",
    "test_source_dir = os.path.join(data_dir, test_source)\n",
    "test_target_dir = os.path.join(data_dir, test_target)\n",
    "\n",
    "vocab_source = 'vocab.vi'\n",
    "vocab_target = 'vocab.en'\n",
    "vocab_source_dir = os.path.join(data_dir, vocab_source)\n",
    "vocab_target_dir = os.path.join(data_dir, vocab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in source training set: 133317\n",
      "Total number of sentences in target training set: 133317\n",
      "Total number of sentences in source testing set: 1553\n",
      "Total number of sentences in target testing set: 1553\n"
     ]
    }
   ],
   "source": [
    "# load training sets\n",
    "with open(train_source_dir) as f_source:\n",
    "    sentences_source = f_source.readlines()\n",
    "with open(train_target_dir) as f_target:\n",
    "    sentences_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source training set: {}\".format(len(sentences_source)))\n",
    "print(\"Total number of sentences in target training set: {}\".format(len(sentences_target)))\n",
    "\n",
    "# load testing sets\n",
    "with open(test_source_dir) as f_source:\n",
    "    test_source = f_source.readlines()\n",
    "with open(test_target_dir) as f_target:\n",
    "    test_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source testing set: {}\".format(len(test_source)))\n",
    "print(\"Total number of sentences in target testing set: {}\".format(len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the longest sentence in sentences_source: 3199\n",
      "The longest sentence: \n",
      "Thula Mama , Thula Mama , Thula Mama , Thula Mama . Trong kí ức tuổi thơ con , qua những giọt lệ nhoè mắt bà , con thấy chân lý trong nụ cười của bà , con thấy chân lý trong nụ cười của bà , xuyên thấu màn đêm u tối trong sự vô tri của con . Ôi , có một người bà đang nằm nghỉ bà ốm đau và trái tim bà rơi lệ . Băn khoăn , băn khoăn , băn khoăn , băn khoăn liệu thế giới này đang đi về đâu . Lẽ nào chuyện trẻ nhỏ phải tự xoay xở lấy là đúng ? Không , không , không , không , không , không . Lẽ nào phiền muộn dồn hết lên mái đầu người phụ nữ già là đúng ? Những người vô danh bất hạnh . Thula Mama Mama , Thula Mama . Thula Mama Mama . Thula Mama , Thula Mama , Thula Mama Mama , Thula Mama . Ngày mai sẽ tốt đẹp hơn . Ngày mai trèo đèo lội suối sẽ dễ hơn , bà ơi . Thula Mama , Thula Mama . Tôi có nên tan vào bài hát này như người đàn ông hát nhạc blues hay một người hát rong . Và rồi từ rất xa , không phải trong câu lạc bộ nhạc blues nào hết , tôi hát , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi . Bây giờ tôi có nên ngừng hát về tình yêu khi kí ức tôi đã nhuộm đầy máu ? Chị em ơi , ồ tại sao có khi ta lại tưởng lầm mụn nhọt là ung thư ? Thế thì , ai lại đi nói , giờ đây không còn bài thơ tình nào nữa ? Tôi muốn hát một bản tình ca cho người phụ nữ có thai đã dám nhảy qua hàng rào và vẫn sinh ra em bé khoẻ mạnh . Nhẹ nhàng thôi , tôi đi vào tia nắng của nụ cười sẽ đốt bùng lên bản tình ca của tôi , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời . Ooh , tôi chưa từng cố chạy trốn những bài ca , tôi nghe tiếng gọi da diết , mạnh mẽ hơn bom đạn kẻ thù . Bài ca rửa sạch cuộc đời ta và những cơn mưa dòng máu ta . Bài ca của tôi về tình yêu và bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi muốn mọi người cùng hát với tôi nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời -- mọi người cùng hát với tôi đi -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi không nghe thấy tiếng các bạn -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi biết bạn hát to hơn được mà -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- hát nữa , hát nữa nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , vâng , bài ca của tôi về tình yêu -- các bạn hát to hơn được nữa mà -- bài ca của tôi về cuộc đời , chính nó , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- cứ hát đi , hát đi , hát lên đi -- bài ca của tôi về tình yêu . Oh yeah . Bài ca -- một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the longest sentence after sentence truncation\n",
    "max = 0\n",
    "for s in sentences_source:\n",
    "    if len(s) > max:\n",
    "        max = len(s)\n",
    "        max_s = s\n",
    "print(\"Number of words in the longest sentence in sentences_source: {}\".format(max))\n",
    "print(\"The longest sentence: \\n{}\".format(max_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sentences by maximum length\n",
    "sentences_source = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_source))\n",
    "sentences_target = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_target))\n",
    "test_source = list(map(lambda src:src.split()[:MAX_LENGTH], test_source))\n",
    "test_target = list(map(lambda src:src.split()[:MAX_LENGTH], test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133166\n",
      "133166\n",
      "1553\n",
      "1553\n"
     ]
    }
   ],
   "source": [
    "# Delete empty sentences in source and target\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del sentences_source[i]\n",
    "        del sentences_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(sentences_source))\n",
    "print(len(sentences_target))\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del test_source[i]\n",
    "        del test_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(test_source))\n",
    "print(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nummber of words in source vocabulary: 7709\n",
      "Total nummber of words in target vocabulary: 17191\n"
     ]
    }
   ],
   "source": [
    "# load vocabularies\n",
    "\n",
    "# build index2word\n",
    "with open(vocab_source_dir) as f_vocab_source:\n",
    "    #index2word_source = f_vocab_source.readlines()\n",
    "    index2word_source = [line.rstrip() for line in f_vocab_source]\n",
    "with open(vocab_target_dir) as f_vocab_target:\n",
    "    #index2word_target = f_vocab_target.readlines()\n",
    "    index2word_target = [line.rstrip() for line in f_vocab_target]\n",
    "\n",
    "# build word2index\n",
    "word2index_source = {}\n",
    "for idx, word in enumerate(index2word_source):\n",
    "    word2index_source[word] = idx\n",
    "word2index_target = {}\n",
    "for idx, word in enumerate(index2word_target):\n",
    "    word2index_target[word] = idx\n",
    "    \n",
    "# check vocabularies size    \n",
    "source_vocab_size = len(index2word_source)\n",
    "target_vocab_size = len(index2word_target)\n",
    "print(\"Total nummber of words in source vocabulary: {}\".format(len(index2word_source)))\n",
    "print(\"Total nummber of words in target vocabulary: {}\".format(len(index2word_target)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funtions to convert sentence in natural language to list of word indexes\n",
    "def sen2idx(sentence, word2index):\n",
    "    return [word2index.get(word, 0) for word in sentence] # assume that 0 is for <unk>\n",
    "\n",
    "def sen2tensor(sentence, word2index):\n",
    "    idxes = sen2idx(sentence, word2index)\n",
    "    idxes.append(EOS_token)\n",
    "    return torch.tensor(idxes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token to be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = target_vocab_size # padding value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator():\n",
    "    def __init__(self, batch_size, sentences_source, sentences_target, word2index_source, word2index_target):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences_source = sentences_source\n",
    "        self.sentences_target = sentences_target\n",
    "        self.word2index_source = word2index_source\n",
    "        self.word2index_target = word2index_target\n",
    "        self.num_sentence = len(sentences_source)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.consumed = 0\n",
    "        self.permutation = np.random.permutation(self.num_sentence)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        # generate id in one batch\n",
    "        if self.consumed + self.batch_size > self.num_sentence:\n",
    "            self.reset()\n",
    "        sample_id = self.permutation[self.consumed:self.consumed + self.batch_size]\n",
    "        self.consumed += self.batch_size\n",
    "\n",
    "        #generate a source batch\n",
    "        sentences_source_tensor = [sen2tensor(self.sentences_source[id], self.word2index_source) for id in sample_id]\n",
    "\n",
    "        len_array_source = [len(st) for st in sentences_source_tensor]\n",
    "        reorder_idx_source = np.argsort(len_array_source, kind='mergesort')\n",
    "        reorder_idx_source = np.argsort(np.flip(reorder_idx_source, 0)) #index to restore unsorted order\n",
    "\n",
    "        sentences_source_tensor.sort(key=len)\n",
    "        sentences_source_tensor.reverse()\n",
    "        sentences_source_packed = pack_sequence(sentences_source_tensor)\n",
    "\n",
    "        #generate a target batch\n",
    "        sentences_target_tensor = [sen2tensor(self.sentences_target[id], self.word2index_target) for id in sample_id]\n",
    "\n",
    "        len_array_target = [len(st) for st in sentences_target_tensor]\n",
    "        reorder_idx_target = np.argsort(len_array_target, kind='mergesort')\n",
    "        reorder_idx_target = np.argsort(np.flip(reorder_idx_target, 0)) #index to restore unsorted order\n",
    "\n",
    "        sentences_target_tensor.sort(key=len)\n",
    "        sentences_target_tensor.reverse()\n",
    "        sentences_target_packed = pack_sequence(sentences_target_tensor)\n",
    "\n",
    "        return (sentences_source_packed, reorder_idx_source), (sentences_target_packed, reorder_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be depreciated\n",
    "\n",
    "# def sentences2tensor(sentences, word2index):\n",
    "#     sentences_tensor = [sen2tensor(s, word2index) for s in sentences]\n",
    "#     sentences_tensor.sort(key=len, reverse=True)\n",
    "#     output = pad_sequence(sentences_tensor, batch_first=True)\n",
    "#     return output\n",
    "\n",
    "def batch_generator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target):\n",
    "    #output: two PackedSequence object, two indexes to reorder sentences in a batch\n",
    "    \n",
    "    # generate id in one batch\n",
    "    total = len(sentences_source)\n",
    "    sample_id = np.random.choice(total, batch_size, replace=False)\n",
    "    \n",
    "    #generate a source batch\n",
    "    sentences_source_tensor = [sen2tensor(sentences_source[id], word2index_source) for id in sample_id]\n",
    "    \n",
    "    len_array_source = [len(st) for st in sentences_source_tensor]\n",
    "    reorder_idx_source = np.argsort(len_array_source, kind='mergesort')\n",
    "    reorder_idx_source = np.argsort(np.flip(reorder_idx_source, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_source_tensor.sort(key=len)\n",
    "    sentences_source_tensor.reverse()\n",
    "    sentences_source_packed = pack_sequence(sentences_source_tensor)\n",
    "    \n",
    "    #generate a target batch\n",
    "    sentences_target_tensor = [sen2tensor(sentences_target[id], word2index_target) for id in sample_id]\n",
    "    \n",
    "    len_array_target = [len(st) for st in sentences_target_tensor]\n",
    "    reorder_idx_target = np.argsort(len_array_target, kind='mergesort')\n",
    "    reorder_idx_target = np.argsort(np.flip(reorder_idx_target, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_target_tensor.sort(key=len)\n",
    "    sentences_target_tensor.reverse()\n",
    "    sentences_target_packed = pack_sequence(sentences_target_tensor)\n",
    "    \n",
    "    return (sentences_source_packed, reorder_idx_source), (sentences_target_packed, reorder_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([   20,     8,  1352,     2,     2]), batch_sizes=tensor([ 2,  2,  1]))\n",
      "[0 1]\n",
      "(tensor([[   20,  1352,     2],\n",
      "        [    8,     2,     0]]), tensor([ 3,  2]))\n"
     ]
    }
   ],
   "source": [
    "# test batch_generator\n",
    "a = ['I','am','a','boy']\n",
    "b = ['a']\n",
    "c = ['the','goat']\n",
    "\n",
    "sentences_test = [a,b,c]\n",
    "sentences_test2 = [c,b,a]\n",
    "\n",
    "BG_test = BatchGenerator(2, sentences_test, sentences_test2, word2index_target, word2index_target)\n",
    "\n",
    "(output_test, reorder_idx_test), (output_test2, reorder_idx_test2) = BG_test.get_batch()\n",
    "print(output_test)\n",
    "print(reorder_idx_test)\n",
    "print(pad_packed_sequence(output_test, batch_first=True))\n",
    "\n",
    "del a, b, c, sentences_test, sentences_test2, output_test, reorder_idx_test, output_test2, reorder_idx_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to resume order of sentences in a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order recovery is necessary because sentences have to be sorted in descending order of sentence length to be packed as a PackedSequence object. PackedSequence object helps to deal with inputs with variable length in NMT setting. LSTM, RNN can accept PackedSequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_order(input, idx):\n",
    "    # input \n",
    "    #   input: Tensor: (batch_size, seq_length)\n",
    "    #   idx: Tensor or ndarray: (batch_size)\n",
    "    # output\n",
    "    #   out: Tensor with reordered sentences in batch: (batch_size, seq_length) \n",
    "    \n",
    "    if isinstance(idx, (np.ndarray)):\n",
    "        idx = torch.from_numpy(idx)\n",
    "        if device == torch.device(\"cuda\"):\n",
    "            idx = idx.cuda()\n",
    "    out = torch.index_select(input, 0, idx)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  8,  9],\n",
      "        [ 4,  5,  6],\n",
      "        [ 1,  2,  3]])\n",
      "tensor([[ 7,  8,  9],\n",
      "        [ 4,  5,  6],\n",
      "        [ 1,  2,  3]])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# test resume_order\n",
    "input_test = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],device=device)\n",
    "idx_test = np.array([2,1,0])\n",
    "print(resume_order(input_test, idx_test))\n",
    "input_test = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],device=device)\n",
    "idx_test = torch.tensor([2,1,0], device=device)\n",
    "print(resume_order(input_test, idx_test))\n",
    "print(idx_test.device)\n",
    "del input_test, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Encoder and Decoder classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1, num_directions=1, dropout=0):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "    def forward(self, input_tuple, prev_h, prev_c):\n",
    "        # input\n",
    "        # input size: (batch_size, seq_length)\n",
    "        # prev_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # prec_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # output\n",
    "        # h_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # c_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        (sentences_packed, reorder_idx) = input_tuple\n",
    "        sentences_tensor, sentences_length = pad_packed_sequence(sentences_packed, batch_first=True, padding_value=0)\n",
    "        #sentences_tensor: (batch_size, seq_length)\n",
    "        input_embedded = self.embedding(sentences_tensor) # (batch_size, seq_length, hidden_size)\n",
    "        input_embedded_packed = pack_padded_sequence(input_embedded, sentences_length, batch_first=True)\n",
    "        output, (h_n, c_n) = self.lstm(input_embedded_packed, (prev_h, prev_c))\n",
    "        return output, h_n, c_n\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_size, device=device)\n",
    "    def setBatchSize(self, batch_size):\n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers and num_directions for encoder must be 0 when this decoder is used\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, prev_h, prev_c, encoder_output=None):\n",
    "        input_embedded = self.embedding(input)\n",
    "        h, c = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "        output = self.softmax(self.out(h))\n",
    "        return output, h, c\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size, device=device)\n",
    "    def setBatchSize(self, batch_size):\n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttenDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(DotAttenDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size*2, attention_vector_size)\n",
    "        self.out2 = nn.Linear(attention_vector_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, prev_h, prev_c, encoder_output):\n",
    "        # encoder_output: PackedSequence to be converted to (batch_size, seq_length, hidden_size*num_directions)\n",
    "        input_embedded = self.embedding(input)\n",
    "        h, c = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "        \n",
    "        # trick: use padding value = -inf to do variable length attention correctly \n",
    "        encoder_output, _ = pad_packed_sequence(encoder_output, batch_first=True, padding_value=0)\n",
    "        #print(encoder_output) # to be removed\n",
    "        scores = torch.matmul(encoder_output, h.unsqueeze(-1)) # (batch_size, seq_length, 1)\n",
    "        scores[scores==0] = -10e10\n",
    "        #print(scores) # to be removed\n",
    "        scores = F.softmax(scores, dim=1)\n",
    "        context_vector = torch.matmul(torch.transpose(encoder_output, 1, 2), scores).squeeze(-1) # (batch_size, hidden_size)\n",
    "        attention_vector = F.tanh(self.out(torch.cat((context_vector, h), -1))) # (batch_size, attention_vector_size)\n",
    "        output = self.softmax(self.out2(attention_vector))\n",
    "        return output, h, c\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size, device=device)\n",
    "    def setBatchSize(self, batch_size):\n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DecoderLSTM can't do teacher forcing in training\n",
    "\n",
    "# class DecoderLSTM(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, batch_size, num_layers=1, num_directions=1, dropout=0):\n",
    "#         super(DecoderLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.num_directions = num_directions\n",
    "#         self.dropout = dropout\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "#         self.out = nn.Linear(hidden_size*num_directions, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=2)\n",
    "#     def forward(self, input, prev_h, prev_c):\n",
    "#         # input \n",
    "#         # input size: (batch_size, seq_length)\n",
    "#         # prev_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # prec_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # output\n",
    "#         # h_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # c_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # output size: (batch_size, seq_length, output_size)\n",
    "#         input_embedded = self.embedding(input)\n",
    "#         output, (h_n, c_n) = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "#         output =self.softmax(self.out(h))\n",
    "#         return output, h_n, c_n\n",
    "#     def initHidden(self, batch_size):\n",
    "#         return torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer,batch_size, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden_h = encoder.initHidden()\n",
    "    encoder_hidden_c = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token, size_average=False)\n",
    "    \n",
    "    (sentences_source_packed, reorder_idx_source) = source_tuple\n",
    "    (sentences_target_packed, reorder_idx_target) = target_tuple\n",
    "    sentences_target_tensor, sentences_target_length = pad_packed_sequence(sentences_target_packed, batch_first=True, padding_value=PAD_token)\n",
    "    sentences_target_tensor = resume_order(sentences_target_tensor, reorder_idx_target)\n",
    "\n",
    "    target_length = sentences_target_tensor.size(1)\n",
    "    \n",
    "    # encoder_output size: (batch_size, seq_length, hidden_size*num_directions)\n",
    "    # encoder_hidden_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    # encoder_hidden_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    encoder_output, encoder_hidden_h, encoder_hidden_c = encoder(source_tuple, encoder_hidden_h, encoder_hidden_c)\n",
    "    \n",
    "    \n",
    "    decoder_input = torch.full((batch_size,), SOS_token, dtype=torch.long, device=device)\n",
    "    decoder_hidden_c = resume_order(encoder_hidden_c[0], reorder_idx_source)\n",
    "    decoder_hidden_h = resume_order(encoder_hidden_h[0], reorder_idx_source)\n",
    "    \n",
    "    to_print = []\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden_h, decoder_hidden_c = decoder(decoder_input, decoder_hidden_h, decoder_hidden_c, encoder_output)\n",
    "        loss += criterion(decoder_output, sentences_target_tensor[:,di])\n",
    "        decoder_input = sentences_target_tensor[:,di]\n",
    "        \n",
    "        decoder_output_np = np.argmax(decoder_output.detach().cpu().numpy(), 1)[0]\n",
    "        to_print.append(index2word_target[decoder_output_np])\n",
    "    \n",
    "    denominator = torch.sum(sentences_target_length).float()\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        denominator = denominator.cuda()\n",
    "    loss = loss / denominator\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item(), to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(batch_generator, encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.1):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), learning_rate)\n",
    "    \n",
    "    for iter in range(1, n_iters+1):\n",
    "        source_tuple, target_tuple = batch_generator.get_batch()\n",
    "        loss, to_print = train(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter%print_every ==0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(%d %d%%) %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
    "            print(to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator1 = BatchGenerator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target)\n",
    "encoder1 = EncoderLSTM(source_vocab_size, hidden_size, batch_size).to(device)\n",
    "decoder1 = DotAttenDecoderLSTM(hidden_size, target_vocab_size+1, batch_size).to(device) # +1 is a wordaround for ignore_index field of NLLLoss\n",
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step12000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step12000_decoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step24000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step24000_decoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step36000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step36000_decoder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "\n",
    "    sentence_tensor = sen2tensor(sentence, word2index_source)\n",
    "    #print(sentence_tensor) # to be removed\n",
    "    \n",
    "    encoder.setBatchSize(1)\n",
    "    encoder_hidden_h = encoder.initHidden()\n",
    "    encoder_hidden_c = encoder.initHidden()\n",
    "    #print(sentence_tensor.size()) # to be removed\n",
    "    input_tuple = (pack_padded_sequence(sentence_tensor.unsqueeze(0), [len(sentence_tensor)], batch_first=True), np.array([0]))\n",
    "    #print(input_tuple) # to be removed\n",
    "    encoder_output, h_n, c_n = encoder(input_tuple, encoder_hidden_h, encoder_hidden_c)\n",
    "    #print(encoder_output) # to be removed\n",
    "    # encoder_output is a PackedSequence object\n",
    "    # encoder_hidden_h: (1, 1, hidden_size)\n",
    "    # encoder_hidden_c: (1, 1, hidden_size)\n",
    "    #encoder_output = pad_packed_sequence(encoder_output, batch_first=True) # encoder_output: (1, seq_length, hidden_size)\n",
    "    #encoder_output = encoder_output[0] # encoder_output: (seq_length, hidden_size)\n",
    "\n",
    "    decoder.setBatchSize(1)\n",
    "    decoder_input = torch.full((1,), SOS_token, dtype=torch.long, device=device)\n",
    "    decoder_hidden_h = h_n[0]\n",
    "    decoder_hidden_c = c_n[0]\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden_h, decoder_hidden_c = decoder(decoder_input, decoder_hidden_h, decoder_hidden_c, encoder_output)\n",
    "        idx = torch.argmax(decoder_output)\n",
    "        decoder_input = idx.unsqueeze(0)\n",
    "        output.append(index2word_target[idx])\n",
    "        if idx == EOS_token:\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(source_vocab_size, hidden_size, batch_size).to(device)\n",
    "decoder = DotAttenDecoderLSTM(hidden_size, target_vocab_size+1, batch_size).to(device) # +1 is a wordaround for ignore_index field of NLLLoss\n",
    "encoder.load_state_dict(torch.load(os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step36000_encoder\"), map_location='cpu'))\n",
    "decoder.load_state_dict(torch.load(os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step36000_decoder\"), map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples in training set\n",
      "\n",
      "Source: Nó phát ra toàn bộ năng lượng ngay lập tức , và đó là một vụ nổ khó có thể tưởng tượng nổi .\n",
      "Target: It released its energy all at once , and it was an explosion that was mind-numbing .\n",
      "NMT: It brought fusion to its energy exposure , and the one that was finished &#91; with &#93; . </s>\n",
      "\n",
      "Source: Hẳn bạn có thể tưởng tượng , khi là một nhà văn , mọi thứ sẽ trở nên rất căng thẳng và gấp gáp .\n",
      "Target: And you can imagine , if you &apos;re a writer , that things would get really crowded around deadlines .\n",
      "NMT: You can imagine , they start with a very , very , very different character , rich , like a very slight difference . </s>\n",
      "\n",
      "Source: Đây là năng lượng tạo bằng sức gió . Tất cả bóng đèn đều là bóng đèn tích kiệm năng lượng .\n",
      "Target: This is powered by wind . All of the lights are daylight bulbs .\n",
      "NMT: This is powered by wind . I mean every light about water quantum function flow . </s>\n",
      "\n",
      "Some examples in testing set\n",
      "\n",
      "Source: Tôi là một y sĩ theo khái niệm mới .\n",
      "Target: I am a redefined physician .\n",
      "NMT: I was a creator Johnson design . </s>\n",
      "\n",
      "Source: Một thứ được gọi là &quot; Jazz &quot; còn cái kia được gọi là &quot; Swing &quot;\n",
      "Target: One is called &quot; Jazz &quot; and the other one is called &quot; Swing . &quot;\n",
      "NMT: Something called the &quot; <unk> . &quot; &quot; Here &apos;s a call . &quot; </s>\n",
      "\n",
      "Source: và đôi khi bạn đến với TED nó mang lại ý nghĩa và sức mạnh cho bạn mà nếu như bạn không đến bạn sẽ\n",
      "Target: And sometimes when it comes through TED , it has meaning and power that it doesn &apos;t have when it doesn &apos;t .\n",
      "NMT: And sometimes it comes from you and says , &quot; How do you get from your money to function ? &quot; </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some examples in training set\n",
    "print(\"Some examples in training set\")\n",
    "print()\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(len(sentences_source))\n",
    "    translated_sentence = infer(encoder, decoder, sentences_source[idx])\n",
    "    translated_sentence = ' '.join(w for w in translated_sentence)\n",
    "    print(\"Source: {}\".format(' '.join(w for w in sentences_source[idx])))\n",
    "    print(\"Target: {}\".format(' '.join(w for w in sentences_target[idx])))\n",
    "    print (\"NMT: {}\".format(translated_sentence))\n",
    "    print()\n",
    "\n",
    "# Some examples in testing set\n",
    "print(\"Some examples in testing set\")\n",
    "print()\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(len(test_source))\n",
    "    translated_sentence = infer(encoder, decoder, test_source[idx])\n",
    "    translated_sentence = ' '.join(w for w in translated_sentence)\n",
    "    print(\"Source: {}\".format(' '.join(w for w in test_source[idx])))\n",
    "    print(\"Target: {}\".format(' '.join(w for w in test_target[idx])))\n",
    "    print (\"NMT: {}\".format(translated_sentence))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
