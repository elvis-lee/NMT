{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "MAX_LENGTH = 25\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\") # to be removed\n",
    "print(device)\n",
    "\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "hidden_size = 512\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "attention_vector_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "train_source = 'train.vi'\n",
    "train_target = 'train.en'\n",
    "train_source_dir = os.path.join(data_dir, train_source)\n",
    "train_target_dir = os.path.join(data_dir, train_target)\n",
    "vocab_source = 'vocab.vi'\n",
    "vocab_target = 'vocab.en'\n",
    "vocab_source_dir = os.path.join(data_dir, vocab_source)\n",
    "vocab_target_dir = os.path.join(data_dir, vocab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in source training set: 133317\n",
      "Total number of sentences in target training set: 133317\n"
     ]
    }
   ],
   "source": [
    "# load training sets\n",
    "with open(train_source_dir) as f_source:\n",
    "    sentences_source = f_source.readlines()\n",
    "with open(train_target_dir) as f_target:\n",
    "    sentences_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source training set: {}\".format(len(sentences_source)))\n",
    "print(\"Total number of sentences in target training set: {}\".format(len(sentences_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the longest sentence in sentences_source: 3199\n",
      "The longest sentence: \n",
      "Thula Mama , Thula Mama , Thula Mama , Thula Mama . Trong kí ức tuổi thơ con , qua những giọt lệ nhoè mắt bà , con thấy chân lý trong nụ cười của bà , con thấy chân lý trong nụ cười của bà , xuyên thấu màn đêm u tối trong sự vô tri của con . Ôi , có một người bà đang nằm nghỉ bà ốm đau và trái tim bà rơi lệ . Băn khoăn , băn khoăn , băn khoăn , băn khoăn liệu thế giới này đang đi về đâu . Lẽ nào chuyện trẻ nhỏ phải tự xoay xở lấy là đúng ? Không , không , không , không , không , không . Lẽ nào phiền muộn dồn hết lên mái đầu người phụ nữ già là đúng ? Những người vô danh bất hạnh . Thula Mama Mama , Thula Mama . Thula Mama Mama . Thula Mama , Thula Mama , Thula Mama Mama , Thula Mama . Ngày mai sẽ tốt đẹp hơn . Ngày mai trèo đèo lội suối sẽ dễ hơn , bà ơi . Thula Mama , Thula Mama . Tôi có nên tan vào bài hát này như người đàn ông hát nhạc blues hay một người hát rong . Và rồi từ rất xa , không phải trong câu lạc bộ nhạc blues nào hết , tôi hát , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi . Bây giờ tôi có nên ngừng hát về tình yêu khi kí ức tôi đã nhuộm đầy máu ? Chị em ơi , ồ tại sao có khi ta lại tưởng lầm mụn nhọt là ung thư ? Thế thì , ai lại đi nói , giờ đây không còn bài thơ tình nào nữa ? Tôi muốn hát một bản tình ca cho người phụ nữ có thai đã dám nhảy qua hàng rào và vẫn sinh ra em bé khoẻ mạnh . Nhẹ nhàng thôi , tôi đi vào tia nắng của nụ cười sẽ đốt bùng lên bản tình ca của tôi , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời . Ooh , tôi chưa từng cố chạy trốn những bài ca , tôi nghe tiếng gọi da diết , mạnh mẽ hơn bom đạn kẻ thù . Bài ca rửa sạch cuộc đời ta và những cơn mưa dòng máu ta . Bài ca của tôi về tình yêu và bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi muốn mọi người cùng hát với tôi nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời -- mọi người cùng hát với tôi đi -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi không nghe thấy tiếng các bạn -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi biết bạn hát to hơn được mà -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- hát nữa , hát nữa nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , vâng , bài ca của tôi về tình yêu -- các bạn hát to hơn được nữa mà -- bài ca của tôi về cuộc đời , chính nó , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- cứ hát đi , hát đi , hát lên đi -- bài ca của tôi về tình yêu . Oh yeah . Bài ca -- một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the longest sentence after sentence truncation\n",
    "max = 0\n",
    "for s in sentences_source:\n",
    "    if len(s) > max:\n",
    "        max = len(s)\n",
    "        max_s = s\n",
    "print(\"Number of words in the longest sentence in sentences_source: {}\".format(max))\n",
    "print(\"The longest sentence: \\n{}\".format(max_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sentences by maximum length\n",
    "sentences_source = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_source))\n",
    "sentences_target = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133166\n",
      "133166\n"
     ]
    }
   ],
   "source": [
    "# Delete empty sentences in source and target\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del sentences_source[i]\n",
    "        del sentences_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(sentences_source))\n",
    "print(len(sentences_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nummber of words in source vocabulary: 7709\n",
      "Total nummber of words in target vocabulary: 17191\n"
     ]
    }
   ],
   "source": [
    "# load vocabularies\n",
    "\n",
    "# build index2word\n",
    "with open(vocab_source_dir) as f_vocab_source:\n",
    "    #index2word_source = f_vocab_source.readlines()\n",
    "    index2word_source = [line.rstrip() for line in f_vocab_source]\n",
    "with open(vocab_target_dir) as f_vocab_target:\n",
    "    #index2word_target = f_vocab_target.readlines()\n",
    "    index2word_target = [line.rstrip() for line in f_vocab_target]\n",
    "\n",
    "# build word2index\n",
    "word2index_source = {}\n",
    "for idx, word in enumerate(index2word_source):\n",
    "    word2index_source[word] = idx\n",
    "word2index_target = {}\n",
    "for idx, word in enumerate(index2word_target):\n",
    "    word2index_target[word] = idx\n",
    "    \n",
    "# check vocabularies size    \n",
    "source_vocab_size = len(index2word_source)\n",
    "target_vocab_size = len(index2word_target)\n",
    "print(\"Total nummber of words in source vocabulary: {}\".format(len(index2word_source)))\n",
    "print(\"Total nummber of words in target vocabulary: {}\".format(len(index2word_target)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funtions to convert sentence in natural language to list of word indexes\n",
    "def sen2idx(sentence, word2index):\n",
    "    return [word2index.get(word, 0) for word in sentence] # assume that 0 is for <unk>\n",
    "\n",
    "def sen2tensor(sentence, word2index):\n",
    "    idxes = sen2idx(sentence, word2index)\n",
    "    idxes.append(EOS_token)\n",
    "    return torch.tensor(idxes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token to be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = target_vocab_size # padding value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator():\n",
    "    def __init__(self, batch_size, sentences_source, sentences_target, word2index_source, word2index_target):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences_source = sentences_source\n",
    "        self.sentences_target = sentences_target\n",
    "        self.word2index_source = word2index_source\n",
    "        self.word2index_target = word2index_target\n",
    "        self.num_sentence = len(sentences_source)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.consumed = 0\n",
    "        self.permutation = np.random.permutation(self.num_sentence)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        # generate id in one batch\n",
    "        if self.consumed + self.batch_size > self.num_sentence:\n",
    "            self.reset()\n",
    "        sample_id = self.permutation[self.consumed:self.consumed + self.batch_size]\n",
    "        self.consumed += self.batch_size\n",
    "\n",
    "        #generate a source batch\n",
    "        sentences_source_tensor = [sen2tensor(self.sentences_source[id], self.word2index_source) for id in sample_id]\n",
    "\n",
    "        len_array_source = [len(st) for st in sentences_source_tensor]\n",
    "        reorder_idx_source = np.argsort(len_array_source, kind='mergesort')\n",
    "        reorder_idx_source = np.argsort(np.flip(reorder_idx_source, 0)) #index to restore unsorted order\n",
    "\n",
    "        sentences_source_tensor.sort(key=len)\n",
    "        sentences_source_tensor.reverse()\n",
    "        sentences_source_packed = pack_sequence(sentences_source_tensor)\n",
    "\n",
    "        #generate a target batch\n",
    "        sentences_target_tensor = [sen2tensor(self.sentences_target[id], self.word2index_target) for id in sample_id]\n",
    "\n",
    "        len_array_target = [len(st) for st in sentences_target_tensor]\n",
    "        reorder_idx_target = np.argsort(len_array_target, kind='mergesort')\n",
    "        reorder_idx_target = np.argsort(np.flip(reorder_idx_target, 0)) #index to restore unsorted order\n",
    "\n",
    "        sentences_target_tensor.sort(key=len)\n",
    "        sentences_target_tensor.reverse()\n",
    "        sentences_target_packed = pack_sequence(sentences_target_tensor)\n",
    "\n",
    "        return (sentences_source_packed, reorder_idx_source), (sentences_target_packed, reorder_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be depreciated\n",
    "\n",
    "# def sentences2tensor(sentences, word2index):\n",
    "#     sentences_tensor = [sen2tensor(s, word2index) for s in sentences]\n",
    "#     sentences_tensor.sort(key=len, reverse=True)\n",
    "#     output = pad_sequence(sentences_tensor, batch_first=True)\n",
    "#     return output\n",
    "\n",
    "def batch_generator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target):\n",
    "    #output: two PackedSequence object, two indexes to reorder sentences in a batch\n",
    "    \n",
    "    # generate id in one batch\n",
    "    total = len(sentences_source)\n",
    "    sample_id = np.random.choice(total, batch_size, replace=False)\n",
    "    \n",
    "    #generate a source batch\n",
    "    sentences_source_tensor = [sen2tensor(sentences_source[id], word2index_source) for id in sample_id]\n",
    "    \n",
    "    len_array_source = [len(st) for st in sentences_source_tensor]\n",
    "    reorder_idx_source = np.argsort(len_array_source, kind='mergesort')\n",
    "    reorder_idx_source = np.argsort(np.flip(reorder_idx_source, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_source_tensor.sort(key=len)\n",
    "    sentences_source_tensor.reverse()\n",
    "    sentences_source_packed = pack_sequence(sentences_source_tensor)\n",
    "    \n",
    "    #generate a target batch\n",
    "    sentences_target_tensor = [sen2tensor(sentences_target[id], word2index_target) for id in sample_id]\n",
    "    \n",
    "    len_array_target = [len(st) for st in sentences_target_tensor]\n",
    "    reorder_idx_target = np.argsort(len_array_target, kind='mergesort')\n",
    "    reorder_idx_target = np.argsort(np.flip(reorder_idx_target, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_target_tensor.sort(key=len)\n",
    "    sentences_target_tensor.reverse()\n",
    "    sentences_target_packed = pack_sequence(sentences_target_tensor)\n",
    "    \n",
    "    return (sentences_source_packed, reorder_idx_source), (sentences_target_packed, reorder_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([  47,    8,  382,    2,    8,  897,    2], device='cuda:0'), batch_sizes=tensor([ 2,  2,  1,  1,  1]))\n",
      "[0 1]\n",
      "(tensor([[  47,  382,    8,  897,    2],\n",
      "        [   8,    2,    0,    0,    0]], device='cuda:0'), tensor([ 5,  2]))\n"
     ]
    }
   ],
   "source": [
    "# test batch_generator\n",
    "a = ['I','am','a','boy']\n",
    "b = ['a']\n",
    "c = ['the','goat']\n",
    "\n",
    "sentences_test = [a,b,c]\n",
    "sentences_test2 = [c,b,a]\n",
    "\n",
    "BG_test = BatchGenerator(2, sentences_test, sentences_test2, word2index_target, word2index_target)\n",
    "\n",
    "(output_test, reorder_idx_test), (output_test2, reorder_idx_test2) = BG_test.get_batch()\n",
    "print(output_test)\n",
    "print(reorder_idx_test)\n",
    "print(pad_packed_sequence(output_test, batch_first=True))\n",
    "\n",
    "del a, b, c, sentences_test, sentences_test2, output_test, reorder_idx_test, output_test2, reorder_idx_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to resume order of sentences in a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order recovery is necessary because sentences have to be sorted in descending order of sentence length to be packed as a PackedSequence object. PackedSequence object helps to deal with inputs with variable length in NMT setting. LSTM, RNN can accept PackedSequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_order(input, idx):\n",
    "    # input \n",
    "    #   input: Tensor: (batch_size, seq_length)\n",
    "    #   idx: Tensor or ndarray: (batch_size)\n",
    "    # output\n",
    "    #   out: Tensor with reordered sentences in batch: (batch_size, seq_length) \n",
    "    \n",
    "    if isinstance(idx, (np.ndarray)):\n",
    "        idx = torch.from_numpy(idx)\n",
    "        if device == torch.device(\"cuda\"):\n",
    "            idx = idx.cuda()\n",
    "    out = torch.index_select(input, 0, idx)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  8,  9],\n",
      "        [ 4,  5,  6],\n",
      "        [ 1,  2,  3]], device='cuda:0')\n",
      "tensor([[ 7,  8,  9],\n",
      "        [ 4,  5,  6],\n",
      "        [ 1,  2,  3]], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# test resume_order\n",
    "input_test = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],device=device)\n",
    "idx_test = np.array([2,1,0])\n",
    "print(resume_order(input_test, idx_test))\n",
    "input_test = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],device=device)\n",
    "idx_test = torch.tensor([2,1,0], device=device)\n",
    "print(resume_order(input_test, idx_test))\n",
    "print(idx_test.device)\n",
    "del input_test, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Encoder and Decoder classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1, num_directions=1, dropout=0):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "    def forward(self, input_tuple, prev_h, prev_c):\n",
    "        # input\n",
    "        # input size: (batch_size, seq_length)\n",
    "        # prev_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # prec_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # output\n",
    "        # h_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # c_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        (sentences_packed, reorder_idx) = input_tuple\n",
    "        sentences_tensor, sentences_length = pad_packed_sequence(sentences_packed, batch_first=True, padding_value=0)\n",
    "        #sentences_tensor: (batch_size, seq_length)\n",
    "        input_embedded = self.embedding(sentences_tensor) # (batch_size, seq_length, hidden_size)\n",
    "        input_embedded_packed = pack_padded_sequence(input_embedded, sentences_length, batch_first=True)\n",
    "        output, (h_n, c_n) = self.lstm(input_embedded_packed, (prev_h, prev_c))\n",
    "        return output, h_n, c_n\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers and num_directions for encoder must be 0 when this decoder is used\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, prev_h, prev_c, encoder_output=None):\n",
    "        input_embedded = self.embedding(input)\n",
    "        h, c = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "        output = self.softmax(self.out(h))\n",
    "        return output, h, c\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttenDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(DotAttenDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size*2, attention_vector_size)\n",
    "        self.out2 = nn.Linear(attention_vector_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, prev_h, prev_c, encoder_output):\n",
    "        # encoder_output: PackedSequence to be converted to (batch_size, seq_length, hidden_size*num_directions)\n",
    "        input_embedded = self.embedding(input)\n",
    "        h, c = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "        \n",
    "        # trick: use padding value = -inf to do variable length attention correctly \n",
    "        encoder_output, _ = pad_packed_sequence(encoder_output, batch_first=True, padding_value=0)\n",
    "        #print(encoder_output) # to be removed\n",
    "        scores = torch.matmul(encoder_output, h.unsqueeze(-1)) # (batch_size, seq_length, 1)\n",
    "        scores[scores==0] = -10e10\n",
    "        #print(scores) # to be removed\n",
    "        scores = F.softmax(scores, dim=1)\n",
    "        context_vector = torch.matmul(torch.transpose(encoder_output, 1, 2), scores).squeeze(-1) # (batch_size, hidden_size)\n",
    "        attention_vector = F.tanh(self.out(torch.cat((context_vector, h), -1))) # (batch_size, attention_vector_size)\n",
    "        output = self.softmax(self.out2(attention_vector))\n",
    "        return output, h, c\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DecoderLSTM can't do teacher forcing in training\n",
    "\n",
    "# class DecoderLSTM(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, batch_size, num_layers=1, num_directions=1, dropout=0):\n",
    "#         super(DecoderLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.num_directions = num_directions\n",
    "#         self.dropout = dropout\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "#         self.out = nn.Linear(hidden_size*num_directions, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=2)\n",
    "#     def forward(self, input, prev_h, prev_c):\n",
    "#         # input \n",
    "#         # input size: (batch_size, seq_length)\n",
    "#         # prev_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # prec_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # output\n",
    "#         # h_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # c_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # output size: (batch_size, seq_length, output_size)\n",
    "#         input_embedded = self.embedding(input)\n",
    "#         output, (h_n, c_n) = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "#         output =self.softmax(self.out(h))\n",
    "#         return output, h_n, c_n\n",
    "#     def initHidden(self, batch_size):\n",
    "#         return torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer,batch_size, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden_h = encoder.initHidden()\n",
    "    encoder_hidden_c = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token, size_average=False)\n",
    "    \n",
    "    (sentences_source_packed, reorder_idx_source) = source_tuple\n",
    "    (sentences_target_packed, reorder_idx_target) = target_tuple\n",
    "    sentences_target_tensor, sentences_target_length = pad_packed_sequence(sentences_target_packed, batch_first=True, padding_value=PAD_token)\n",
    "    sentences_target_tensor = resume_order(sentences_target_tensor, reorder_idx_target)\n",
    "\n",
    "    target_length = sentences_target_tensor.size(1)\n",
    "    \n",
    "    # encoder_output size: (batch_size, seq_length, hidden_size*num_directions)\n",
    "    # encoder_hidden_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    # encoder_hidden_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    encoder_output, encoder_hidden_h, encoder_hidden_c = encoder(source_tuple, encoder_hidden_h, encoder_hidden_c)\n",
    "    \n",
    "    \n",
    "    decoder_input = torch.full((batch_size,), SOS_token, dtype=torch.long, device=device)\n",
    "    decoder_hidden_c = resume_order(encoder_hidden_c[0], reorder_idx_source)\n",
    "    decoder_hidden_h = resume_order(encoder_hidden_h[0], reorder_idx_source)\n",
    "    \n",
    "    to_print = []\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden_h, decoder_hidden_c = decoder(decoder_input, decoder_hidden_h, decoder_hidden_c, encoder_output)\n",
    "        loss += criterion(decoder_output, sentences_target_tensor[:,di])\n",
    "        decoder_input = sentences_target_tensor[:,di]\n",
    "        \n",
    "        decoder_output_np = np.argmax(decoder_output.detach().cpu().numpy(), 1)[0]\n",
    "        to_print.append(index2word_target[decoder_output_np])\n",
    "    \n",
    "    denominator = torch.sum(sentences_target_length).float()\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        denominator = denominator.cuda()\n",
    "    loss = loss / denominator\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item(), to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(batch_generator, encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.1):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), learning_rate)\n",
    "    \n",
    "    for iter in range(1, n_iters+1):\n",
    "        source_tuple, target_tuple = batch_generator.get_batch()\n",
    "        loss, to_print = train(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter%print_every ==0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(%d %d%%) %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
    "            print(to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100 0%) 5.9977\n",
      "['And', 'I', '&apos;m', 'to', 'the', ',', 'the', 'of', ',', ',', 'and', 'is', 'a', '<unk>', '.', 'the', '<unk>', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(200 1%) 5.0734\n",
      "['And', ',', 'the', ',', ',', 'I', 'was', 'got', 'to', 'little', 'to', ',', ',', 'the', 'the', '<unk>', '.', '.', '.', 'the', '.', 'the', '.', '</s>', '</s>', '</s>']\n",
      "(300 2%) 4.8125\n",
      "['It', 'are', 'the', ',', 'the', ',', '<unk>', 'of', ',', 'and', 'it', 'can', '&apos;t', 'that', 'the', '&apos;s', 'not', 'to', 'the', ',', 'the', ',', '.', '<unk>', '</s>', '</s>']\n",
      "(400 3%) 4.6265\n",
      "['You', 'you', 'can', 'see', 'you', '<unk>', ',', 'but', '&apos;s', 'a', '<unk>', 'of', 'but', 'it', 'you', 'it', '&apos;t', 'be', '.', 'but', 'can', '&apos;t', 'be', '.', '.', '</s>']\n",
      "(500 4%) 4.4876\n",
      "['&quot;', 'It', '&apos;m', 'a', '<unk>', 'the', 'of', ',', ',', '&quot;', 'it', 'is', '&apos;t', 'know', 'like', 'this', '.', 'the', '.', '<unk>', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(600 5%) 4.3686\n",
      "['If', 'you', ',', 'the', ',', ',', ',', 'and', '&apos;s', 'the', 'own', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(700 5%) 4.2943\n",
      "['The', 'first', 'is', 'is', 'me', 'own', '.', 'than', '.', '<unk>', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(800 6%) 4.2070\n",
      "['We', 'have', 'to', ',', 'we', 'have', 'a', ',', ',', 'we', 'have', 'to', ',', 'and', 'of', ',', ',', '</s>', ',', ',', '.', '.', '.', '.', '.', '</s>']\n",
      "(900 7%) 4.1598\n",
      "['I', 'I', 'don', 'to', 'longer', 'of', 'to', 'do', 'the', ',', '.', 'he', '&apos;s', 'to', 'been', 'to', 'the', '<unk>', 'time', '.', 'the', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(1000 8%) 4.0858\n",
      "['My', '<unk>', 'was', 'been', '.', '<unk>', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1100 9%) 3.9435\n",
      "['For', ',', 'in', ',', 'you', ',', 'you', 'you', '&apos;s', 'a', 'be', 'this', 'the', '<unk>', 'of', 'the', 'planet', ',', 'of', 'the', ',', 'and', 'know', ',', '</s>', '</s>']\n",
      "(1200 10%) 3.8610\n",
      "['When', 'the', 'problem', 'is', 'the', 'world', 'is', 'the', 'is', 'have', 'the', '<unk>', ',', 'and', '&apos;s', 'the', '<unk>', ',', '</s>', ',', '.', '.', '.', '.', '.', '</s>']\n",
      "(1300 10%) 3.8228\n",
      "['So', 'it', '&apos;s', 'me', 'think', ',', 'I', 'way', '<unk>', 'of', 'the', 'is', 'be', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1400 11%) 3.7915\n",
      "['So', 'I', 'I', '&apos;m', 'like', 'to', 'show', 'you', 'how', 'little', 'bit', 'of', 'what', 'I', '.', '.', 'like', '.', 'the', '.', '</s>', '.', '.', '.', '</s>', '</s>']\n",
      "(1500 12%) 3.7612\n",
      "['When', 'you', '&apos;re', 'to', 'the', ',', 'the', ',', ',', '<unk>', ',', 'you', 'can', 'a', 'the', 'and', 'and', 'the', 'and', 'to', 'the', 'a', '<unk>', ',', '.', '</s>']\n",
      "(1600 13%) 3.7522\n",
      "['It', 'that', '&apos;s', 'the', 'the', '<unk>', 'of', 'the', '<unk>', ',', 'the', '<unk>', ',', 'and', 'you', 'you', 'the', 'you', 'sudden', 'you', 'can', 'see', ',', 'it', '<unk>', '</s>']\n",
      "(1700 14%) 3.6987\n",
      "['This', '&apos;s', 'a', 'example', 'of', 'the', '<unk>', 'in', '<unk>', '.', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1800 15%) 3.6763\n",
      "['Because', 'now', 'we', 'we', '&apos;re', 'been', 'this', ',', ',', 'the', 'as', 'the', ',', 'the', 'year', ',', 'we', 'the', 'percent', ',', 'the', 'United', 'States', ',', 'the', '</s>']\n",
      "(1900 15%) 3.6485\n",
      "['So', 'this', '&apos;s', 'the', 'question', ':', 'the', ':', 'How', 'are', 'of', 'the', 'people', 'people', 'are', ',', 'than', 'to', 'be', 'a', 'in', 'the', '<unk>', ',', 'and', '</s>']\n",
      "(2000 16%) 3.6263\n",
      "['I', 'is', 'the', 'I', 'think', 'the', 'in', 'most', '<unk>', 'revolution', 'of', 'life', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2100 17%) 3.5480\n",
      "['How', 'does', 'it', 'be', 'been', 'down', 'the', 'a', '?', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2200 18%) 3.3505\n",
      "['This', 'this', 'room', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2300 19%) 3.3478\n",
      "['So', 'he', 'is', 'very', 'interesting', 'in', 'the', 'the', 'he', 'people', 'he', 'the', 'was', 'on', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(2400 20%) 3.3388\n",
      "['In', '2008', ',', 'we', 'the', ',', 'we', 'found', 'the', '<unk>', ',', '<unk>', ',', 'was', 'a', '<unk>', ',', 'a', ',', ',', 'and', 'we', '<unk>', 'of', ',', '</s>']\n",
      "(2500 20%) 3.3438\n",
      "['And', 'was', 'like', 'when', 'that', '<unk>', 'in', '<unk>', 'in', 'I', 'had', 'in', 'the', '<unk>', 'of', 'the', '<unk>', 'was', 'the', '<unk>', ',', 'the', ',', 'and', 'York', '</s>']\n",
      "(2600 21%) 3.3309\n",
      "['So', 'this', '&apos;s', 'a', 'very', 'data', 'of', 'data', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2700 22%) 3.3265\n",
      "['They', '&apos;re', 'like', 'social', 'of', 'social', 'social', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2800 23%) 3.3304\n",
      "['There', '&apos;s', 'no', '<unk>', '<unk>', ',', 'us', ',', 'and', 'it', 'of', 'know', 'about', 'that', 'it', 'was', 'a', 'in', 'of', 'the', 'planet', ',', ',', '</s>', '</s>', '</s>']\n",
      "(2900 24%) 3.3001\n",
      "['And', 'one', ',', 'remember', ',', 'one', 'of', 'you', 'have', 'and', 'many', '<unk>', ',', ',', 'and', ',', 'and', 'then', '&apos;s', 'a', '<unk>', '<unk>', '.', 'a', '.', '</s>']\n",
      "(3000 25%) 3.2921\n",
      "['Within', 'the', 'four', 'months', 'of', 'the', ',', 'we', '&apos;ve', 'been', 'a', 'in', 'of', 'the', 'to', 'we', 'the', 'of', 'of', '&apos;ve', 'going', 'with', 'the', 'and', '.', '</s>']\n",
      "(3100 25%) 3.2990\n",
      "['A', 'a', '&apos;s', 'a', 'good', '<unk>', '<unk>', 'a', 'good', 'thing', '.', 'a', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3200 26%) 3.0428\n",
      "['And', 'really', 'really', 'the', '&apos;s', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3300 27%) 2.9945\n",
      "['I', 'was', 'a', '<unk>', 'to', 'the', '<unk>', 'to', 'the', 'the', 'in', 'the', 'man', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(3400 28%) 2.9975\n",
      "['Women', 'are', 'not', 'more', 'likely', '<unk>', 'than', 'they', '&apos;re', '&apos;t', 'know', 'to', 'they', ',', 'they', 'can', 'going', 'to', 'be', 'able', 'to', 'do', 'something', 'different', '</s>', '</s>']\n",
      "(3500 29%) 3.0196\n",
      "['We', 'we', 'go', 'got', 'a', 'minutes', ',', 'and', 'we', 'who', 'us', 'a', 'and', 'we', 'us', 'a', 'one', 'we', 'we', 'we', 'we', 'do', ',', ',', 'we', '</s>']\n",
      "(3600 30%) 3.0279\n",
      "['A', 'is', 'is', 'a', 'design', 'design', 'the', 'health', 'for', 'the', 'technology', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3700 30%) 3.0345\n",
      "['And', 'when', 'the', 'the', 'the', 'the', 'people', 'of', 'between', 'the', 'the', 'to', 'to', 'the', ',', 'the', '<unk>', 'is', 'you', 'information', 'is', 'to', 'to', 'idea', '.', '</s>']\n",
      "(3800 31%) 3.0178\n",
      "['<unk>', 'says', ',', '&quot;', '<unk>', 'is', 'of', '<unk>', 'the', 'you', 'get', 'to', 'the', '<unk>', 'to', 'the', '?', '&quot;', '</s>', '</s>', '?', '.', '.', '.', '.', '.']\n",
      "(3900 32%) 3.0173\n",
      "['I', 'I', 'said', ',', '&quot;', 'Oh', ',', 'it', '&apos;s', 'a', 'same', 'thing', '&quot;', '&apos;s', 'a', 'a', '<unk>', '.', '&quot;', '</s>', '.', '.', '.', '.', '.', '.']\n",
      "(4000 33%) 3.0281\n",
      "['You', 'have', 'to', 'have', 'a', 'good', '.', 'good', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4100 34%) 3.0244\n",
      "['The', 'problem', 'is', 'is', 'to', 'to', 'you', 'get', 'the', 'out', 'the', 'and', 'you', 'go', 'going', 'used', 'up', 'the', 'and', 'you', '&apos;re', 'going', 'able', 'and', '</s>', '</s>']\n",
      "(4200 35%) 2.9039\n",
      "['And', 'the', 'nurses', 'told', 'me', 'that', 'things', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4300 35%) 2.7026\n",
      "['But', 'even', 'though', 'we', 'same', 'we', 'still', ',', 'we', 'can', 'see', 'to', '<unk>', 'on', 'the', 'that', 'we', 'can', 'see', 'and', 'of', '<unk>', 'of', 'the', '<unk>', '</s>']\n",
      "(4400 36%) 2.7309\n",
      "['Because', 'it', 'the', 'is', 'good', 'for', 'because', 'it', 'a', 'be', '.', 'good', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500 37%) 2.7344\n",
      "['It', 'it', 'seems', 'seem', 'to', 'a', 'simple', 'idea', ',', 'but', 'it', '&apos;s', ',', 'but', 'it', 'idea', 'that', 'has', 'be', 'to', 'the', '&apos;s', 'thoughts', '.', 'be', '</s>']\n",
      "(4600 38%) 2.7450\n",
      "['These', 'is', 'the', 'creatures', 'because', 'because', '<unk>', ',', 'because', 'a', ',', 'because', 'they', '<unk>', 'animal', 'are', 'have', 'a', '<unk>', 'nickname', '.', '</s>', '.', '.', '.', '</s>']\n",
      "(4700 39%) 2.7720\n",
      "['<unk>', 'percent', 'of', 'people', 'have', 'that', '<unk>', 'their', '<unk>', ',', 'they', '&apos;re', 'have', 'a', 'percent', 'of', 'people', ',', '</s>', ',', 'to', 'and', ',', '</s>', '</s>', '</s>']\n",
      "(4800 40%) 2.7765\n",
      "['In', '&apos;re', 'very', 'very', 'different', '.', 'They', '&apos;re', 'very', '<unk>', 'different', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4900 40%) 2.7689\n",
      "['The', '<unk>', 'station', 'are', 'out', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5000 41%) 2.7868\n",
      "['About', 'years', 'years', 'ago', ',', 'the', 'the', 'the', 'first', 'of', 'the', ',', 'to', 'he', '<unk>', 'had', '.', 'the', 'was', 'not', 'longer', ',', '.', 'the', '.', '</s>']\n",
      "(5100 42%) 2.7935\n",
      "['I', 'heard', 'never', 'heard', 'about', 'the', '<unk>', 'in', 'years', 'ago', ',', 'or', '<unk>', '<unk>', ',', 'or', ',', 'with', '&apos;s', 'the', ',', 'the', 'the', ',', ',', '</s>']\n",
      "(5200 43%) 2.7878\n",
      "['We', 'we', 'think', 'to', 'think', 'of', 'about', 'and', 'the', 'and', ',', 'and', 'and', 'simple', 'and', 'very', 'simple', ',', 'behavior', '.', '</s>', 'creativity', '.', '.', '.', '</s>']\n",
      "(5300 44%) 2.4398\n",
      "['Why', '?', 'Because', 'we', 'cannot', 'ignore', 'the', 'metaphorical', 'of', 'of', 'our', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5400 45%) 2.4834\n",
      "['And', 'one', 'day', ',', 'is', '&apos;t', 'get', 'up', 'to', '<unk>', 'of', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5500 45%) 2.5149\n",
      "['It', 'as', 'a', ',', 'we', 'take', 'take', 'a', 'kids', ',', ',', 'we', 'take', 'take', 'a', 'back', 'to', 'and', 'started', 'start', 'to', 'to', 'to', 'and', 'we', '</s>']\n",
      "(5600 46%) 2.5268\n",
      "['I', '&apos;m', 'from', 'a', ',', '<unk>', 'a', 'famous', '<unk>', 'by', 'the', '<unk>', ',', 'in', '<unk>', 'in', 'the', '1950s', ',', 'and', '&quot;', 'The', '<unk>', '&quot;', ',', '</s>']\n",
      "(5700 47%) 2.5381\n",
      "['Now', ',', 'the', 'ability', 'is', 'to', ',', 'the', 'own', ',', 'or', 'the', '&apos;s', '&apos;re', 'want', ',', 'or', 'we', '&apos;re', 'up', '.', 'the', 'we', 'could', 'looking', '</s>']\n",
      "(5800 48%) 2.5527\n",
      "['In', ',', 'the', 'course', ',', 'and', 'were', 'the', 'button', 'and', 'the', 'the', '<unk>', 'and', 'and', 'you', 'the', 'day', ',', 'you', 'put', 'to', '<unk>', '<unk>', 'the', '</s>']\n",
      "(5900 49%) 2.5557\n",
      "['And', 'it', '&apos;s', 'enough', 'enough', 'that', 'that', 'we', 'could', 'use', 'the', '<unk>', 'of', 'the', ',', ',', 'which', 'though', 'the', '<unk>', 'extent', '.', 'the', 'the', 'last', '</s>']\n",
      "(6000 50%) 2.5658\n",
      "['This', 'this', 'one', 'of', 'our', '<unk>', 'is', 'be', 'that', 'the', 'universe', 'is', 'not', 'that', ',', '<unk>', ',', 'but', 'we', 'don', 'look', 'be', '.', 'it', 'of', '</s>']\n",
      "(6100 50%) 2.5798\n",
      "['Women', 'are', 'to', 'go', 'different', 'differently', 'than', 'that', 'do', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6200 51%) 2.5732\n",
      "['As', 'you', 'can', 'see', ',', 'it', '&apos;ve', 'across', 'the', 'entire', 'Pacific', ',', 'over', 'way', ',', 'the', ',', 'the', ',', 'from', ',', 'and', ',', 'and', 'and', '</s>']\n",
      "(6300 52%) 2.3830\n",
      "['And', 'what', 'these', 'people', 'who', 'met', 'met', 'met', 'this', 'victim', 'of', 'this', 'own', '?', '</s>', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6400 53%) 2.2462\n",
      "['We', 'can', 'decide', 'whether', 'be', 'as', '<unk>', ',', 'or', '<unk>', ',', 'or', 'two', 'other', 'of', 'people', 'people', '.', '</s>', '</s>', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6500 54%) 2.2836\n",
      "['But', 'more', 'importantly', ',', 'she', '&apos;s', '<unk>', 'to', 'give', 'you', 'a', 'bucks', 'to', 'give', 'to', 'a', '<unk>', 'pit', 'bull', '.', '</s>', '</s>', '.', '.', '.', '.']\n",
      "(6600 55%) 2.3166\n",
      "['This', 'is', 'a', 'of', 'of', 'data', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6700 55%) 2.3362\n",
      "['And', '&apos;s', 'a', 'for', 'me', ',', 'which', 'that', '&apos;s', 'like', 'a', 'was', 'a', 'very', 'important', 'thing', 'to', 'do', 'to', 'the', 'in', 'the', '</s>', '.', '.', '.']\n",
      "(6800 56%) 2.3508\n",
      "['I', 'I', 'I', '&apos;m', 'I', 'a', 'young', 'of', 'the', 'young', '<unk>', 'I', 'around', 'the', 'and', '<unk>', ',', 'and', 'they', 'the', '.', '</s>', '.', '.', '</s>', '</s>']\n",
      "(6900 57%) 2.3661\n",
      "['The', 'important', 'important', 'thing', 'I', 'want', 'to', 'talk', 'is', 'tell', 'about', 'this', 'is', 'that', '<unk>', 'and', '<unk>', '<unk>', '<unk>', ',', '.', 'about', '<unk>', 'of', 'the', '</s>']\n",
      "(7000 58%) 2.3897\n",
      "['But', 'the', ',', 'the', ',', 'if', 'if', 'to', 'long', 'to', 'you', '&apos;ve', 'to', '<unk>', 'to', 'go', '.', '.', 'the', '</s>', '.', '.', '.', '.', '.', '.']\n",
      "(7100 59%) 2.3864\n",
      "['The', 'oil', 'from', 'from', 'the', 'a', 'produces', 'a', 'than', 'gas', 'than', 'than', 'any', 'other', 'strand', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(7200 60%) 2.3852\n",
      "['And', 'were', 'been', 'been', 'back', 'to', 'the', 'countryside', 'area', '.', 'they', 'came', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7300 60%) 2.3378\n",
      "['If', 'if', 'that', ',', 'I', 'think', ',', '&apos;s', 'a', 'good', 'things', 'that', 'will', 'be', 'out', '.', 'it', '.', '</s>', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7400 61%) 2.0601\n",
      "['It', ',', 'not', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7500 62%) 2.0944\n",
      "['What', 'is', 'our', 'dream', '?', 'How', 'did', 'we', 'proceed', 'our', '?', '&quot;', '</s>', '?', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7600 63%) 2.1244\n",
      "['And', 'you', 'also', 'like', 'like', 'this', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7700 64%) 2.1475\n",
      "['And', 'as', 'as', 'every', 'of', ',', 'it', 'the', 'certain', 'point', 'it', '&apos;s', 'it', 'that', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7800 65%) 2.1682\n",
      "['Well', ',', 'the', 'was', 'the', 'answer', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7900 65%) 2.1877\n",
      "['The', 'quarks', 'are', 'different', 'together', 'because', 'another', 'things', 'called', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8000 66%) 2.1961\n",
      "['And', 'million', 'a', 'half', 'million', 'people', 'have', 'been', 'of', 'free', 'lives', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8100 67%) 2.2140\n",
      "['Have', 'you', 'ever', 'forgotten', 'your', 'seat', '?', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8200 68%) 2.2246\n",
      "['Natalie', '<unk>', 'sings', 'from', 'a', '<unk>', '<unk>', ',', '<unk>', 'lt', ';', 'em', '&amp;', 'gt', ';', 'Leave', '&amp;', 'Sleep', '.', '&amp;', 'lt', ';', '/', 'em', '&amp;', '</s>']\n",
      "(8300 69%) 2.2437\n",
      "['David', 'Byrne', ',', 'Ethel', '.', 'David', 'Dolby', '.', '&quot;', '<unk>', '&quot;', '</s>', 'the', 'quartet', '</s>', '</s>', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(8400 70%) 1.9591\n",
      "['Put', 'it', 'in', 'a', 'big', 'and', 'it', 'will', '<unk>', 'for', '<unk>', 'hours', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(8500 70%) 1.9229\n",
      "['What', '&apos;s', 'this', 'evidence', 'on', 'this', 'it', 'is', 'based', '?', '</s>', '?', '?', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>']\n",
      "(8600 71%) 1.9560\n",
      "['And', 'this', 'is', 'after', 'quote', 'from', 'the', 'after', 'writing', 'wrote', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8700 72%) 1.9870\n",
      "['Anyway', 'all', ',', 'it', '&apos;s', 'no', '<unk>', 'or', 'jumping', ',', 'and', 'the', '<unk>', 'is', 'up', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8800 73%) 2.0042\n",
      "['And', 'the', 'great', 'thing', 'is', ',', 'if', 'those', 'numbers', 'are', 'different', 'value', 'values', 'or', 'compared', 'number', 'size', ',', 'or', 'number', 'is', 'as', 'compared', 'knew', 'it', '</s>']\n",
      "(8900 74%) 2.0267\n",
      "['Because', 'he', 'saw', 'staring', ',', 'was', 'front', '<unk>', 'diner', 'suit', ',', 'and', 'mustache', ',', 'staring', 'at', 'me', ',', '</s>', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(9000 75%) 2.0430\n",
      "['The', 'change', 'changed', 'the', 'children', 'have', 'changed', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9100 75%) 2.0564\n",
      "['It', 'has', 'got', 'a', 'sensors', ',', 'the', 'motors', ',', ',', 'from', ',', 'of', 'the', 'autonomy', ',', ',', 'and', 'it', 'than', 'a', 'kilos', 'of', 'the', 'in', '</s>']\n",
      "(9200 76%) 2.0774\n",
      "['In', 'fact', ',', 'Apophis', 'is', 'a', 'great', ',', 'disguise', ',', 'because', 'it', '&apos;s', 'us', 'up', 'to', 'the', 'age', 'of', 'the', 'things', '.', '</s>', ',', '</s>', '</s>']\n",
      "(9300 77%) 2.0915\n",
      "['And', 'she', 'used', 'this', 'case', 'in', 'my', 'own', 'that', '.', 'well', '.', '</s>', 'in', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9400 78%) 1.9548\n",
      "['Mitchell', '<unk>', ':', 'Remember', 'the', 'images', 'I', 'showed', 'you', 'the', 'the', 'we', 'doctors', 'and', 'nurses', 'were', 'are', 'these', 'Africa', '.', '</s>', '?', '?', '.', '.', '.']\n",
      "(9500 79%) 1.7722\n",
      "['We', '&apos;re', 'this', 'in', 'the', 'time', 'design', '.', 'time', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9600 80%) 1.8011\n",
      "['But', ',', 'I', 'found', 'it', 'actually', 'a', 'the', ',', 'Japan', ',', 'but', 'it', 'is', 'thing', 'that', '&apos;s', 'really', 'in', 'in', 'single', 'of', 'a', 'endeavor', '.', '</s>']\n",
      "(9700 80%) 1.8308\n",
      "['The', '&apos;s', '4', 'minute', '15', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9800 81%) 1.8658\n",
      "['Thank', 'you', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9900 82%) 1.8787\n",
      "['It', '&apos;s', 'basically', ',', '<unk>', ',', ',', 'up', 'information', 'there', 'can', 'freely', 'over', 'available', 'and', 'people', 'person', ',', 'the', 'world', '.', '</s>', ',', ',', ',', '</s>']\n",
      "(10000 83%) 1.8988\n",
      "['The', 'the', 'only', 'thing', 'of', 'thing', 'you', 'if', 'only', 'thing', 'we', 'know', 'about', 'now', 'is', 'we', 'be', 'with', 'the', 'neurons', 'of', 'devices', 'cells', '</s>', '</s>', '</s>']\n",
      "(10100 84%) 1.9106\n",
      "['Imagine', 'if', '&apos;re', 'going', 'to', 'play', 'baseball', 'and', 'you', '&apos;re', 'to', 'take', 'where', 'to', 'ball', 'is', 'going', 'to', 'take', 'back', 'you', 'would', 'to', '.', 'next', '</s>']\n",
      "(10200 85%) 1.9377\n",
      "['There', 'are', 'other', 'differences', 'that', 'that', 'differences', 'in', 'the', 'that', 'can', 'be', 'to', 'a', 'populations', 'that', 'have', 'able', 'the', 'of', 'a', 'a', 'adverse', 'information', 'information', '</s>']\n",
      "(10300 85%) 1.9344\n",
      "['The', 'real', 'problem', 'is', '--', 'and', 'the', 'is', 'the', 'answer', 'to', 'the', 'two', 'giant', '--', 'this', 'are', 'are', 'the', '.', '</s>', '?', '.', '.', '.', '</s>']\n",
      "(10400 86%) 1.9597\n",
      "['These', 'are', 'are', 'called', '<unk>', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10500 87%) 1.6369\n",
      "['I', 'wish', 'I', 'could', 'do', 'something', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10600 88%) 1.6573\n",
      "['So', 'in', 'closing', ',', '&apos;ll', 'just', 'you', 'to', ':', 'we', 'scale', 'is', 'the', 'canoe', ',', 'and', 'we', '&apos;re', 'the', '<unk>', 'of', '</s>', 'up', '.', '.', '</s>']\n",
      "(10700 89%) 1.6932\n",
      "['I', 'fact', 'sense', ',', 'these', 'people', 'visionaries', 'who', 'think', 'think', 'of', 'the', 'world', 'of', 'art', ',', 'but', 'and', 'culture', ',', 'been', 'most', 'to', 'listen', 'a', '</s>']\n",
      "(10800 90%) 1.7209\n",
      "['In', 'in', 'that', 'moment', ',', 'I', 'can', 'be', 'able', 'to', 'say', 'and', 'the', 'and', ',', 'maybe', 'can', 'say', 'that', 'it', 'what', 'the', 'fact', 'of', 'that', '</s>']\n",
      "(10900 90%) 1.7536\n",
      "['The', 'last', 'statement', 'to', 'say', 'say', ',', '<unk>', 'that', '&apos;ve', ',', 'the', 'that', 'is', 'you', 'to', 'to', 'me', ',', ',', 'and', 'the', 'Indian', 'of', 'the', '</s>']\n",
      "(11000 91%) 1.7681\n",
      "['We', '&apos;ve', 'mapped', 'that', 'afterglow', 'with', 'the', 'precision', ',', 'and', 'it', 'of', 'the', 'shocks', 'about', 'it', 'is', 'that', 'it', '&apos;s', 'a', 'certainly', '<unk>', '.', '</s>', '</s>']\n",
      "(11100 92%) 1.7837\n",
      "['Because', 'Arabic', '&apos;s', 'history', 'to', 'now', '<unk>', 'take', 'the', 'dictionary', ',', 'so', 'I', '&apos;m', 'the', 'words', 'translations', 'and', 'nothing', 'to', 'use', 'the', 'all', ',', 'and', '</s>']\n",
      "(11200 93%) 1.8099\n",
      "['In', 'fact', ',', 'there', '&apos;s', 'a', '<unk>', 'number', 'and', 'there', '<unk>', 'in', 'Pakistan', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11300 94%) 1.8164\n",
      "['And', 'to', 'put', 'the', 'little', 'with', 'the', 'green', 'North', 'Atlantic', ',', ',', 'I', 'think', 'to', 'the', '<unk>', 'pristine', 'state', 'of', 'Southern', 'right', 'whales', 'that', 'was', '</s>']\n",
      "(11400 95%) 1.8342\n",
      "['The', 'problem', 'problem', 'of', 'the', 'biology', 'is', 'that', 'are', 'not', 'going', 'to', 'understand', ',', ',', 'and', 'now', '&apos;s', 'going', 'matter', 'that', '.', 'up', '.', 'a', '</s>']\n",
      "(11500 95%) 1.6518\n",
      "['So', 'today', 'I', '&apos;m', 'to', 'share', 'you', 'about', 'a', 'research', 'research', 'that', 'has', 'shed', 'light', 'research', 'on', 'this', 'question', '.', '</s>', '?', '?', '.', 'differently', '</s>']\n",
      "(11600 96%) 1.5510\n",
      "['And', 'I', 'think', 'that', 'was', 'really', 'the', 'to', 'all', 'her', 'voice', 'every', 'after', 'hour', 'for', 'day', ',', 'he', 'him', 'think', 'of', ',', 'in', 'know', ',', '</s>']\n",
      "(11700 97%) 1.5704\n",
      "['But', 'nobody', 'believed', 'ever', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11800 98%) 1.5941\n",
      "['We', '&apos;re', 'very', 'familiar', 'to', 'this', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(11900 99%) 1.6243\n",
      "['It', 'is', 'a', 'one', 'of', 'a', 'future', 'future', 'for', 'the', 'exploration', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'to', '.', '.', '.']\n",
      "(12000 100%) 1.6431\n",
      "['Second', ',', 'the', 'story', 'story', 'is', 'a', '<unk>', 'of', 'how', 'old', 'Goliath', 'is', ',', 'a', 'one', 'thing', 'to', 'do', ',', 'you', '&apos;re', 'a', 'the', '<unk>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "batch_generator1 = BatchGenerator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target)\n",
    "encoder1 = EncoderLSTM(source_vocab_size, hidden_size, batch_size).to(device)\n",
    "decoder1 = DecoderLSTM(hidden_size, target_vocab_size+1, batch_size).to(device) # +1 is a wordaround for ignore_index field of NLLLoss\n",
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_noatten_lr0.001_step12000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_noatten_lr0.001_step12000_decoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100 0%) 1.7033\n",
      "['I', 'start', 'with', 'the', 'ideas', '.', 'I', 'mean', 'it', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(200 1%) 1.7319\n",
      "['Now', 'now', 'I', '&apos;m', 'going', 'to', 'try', 'the', 'from', 'pulse', 'to', 'my', 'brain', ',', 'and', 'my', 'brain', 'of', 'my', 'brain', '.', 'controls', 'your', 'brain', '.', '</s>']\n",
      "(300 2%) 1.7440\n",
      "['So', 'now', 'we', 'go', 'to', ',', 'I', 'want', 'to', 'show', 'you', 'that', 'it', 'it', 'means', 'in', 'if', 'the', '.', '</s>', 'to', '.', '.', '.', '.', '</s>']\n",
      "(400 3%) 1.7367\n",
      "['Because', 'the', 'brain', 'of', 'the', 'brain', 'that', 'controls', 'decision-making', ',', '&apos;t', 'control', 'the', '.', '</s>', 'decisions', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(500 4%) 1.6890\n",
      "['And', 'he', '&apos;s', 'use', 'this', 'new', 'and', 'and', 'it', 'it', 'quite', 'extraordinary', '.', '</s>', '?', '?', '?', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(600 5%) 1.4422\n",
      "['And', 'I', '10', 'years', 'ago', ',', 'I', 'was', 'that', 'babies', 'might', 'be', 'able', 'a', 'same', 'thing', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(700 5%) 1.4698\n",
      "['<unk>', ':', 'engaging', 'in', 'activities', 'that', 'are', 'meaningful', 'and', 'satisfying', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(800 6%) 1.5004\n",
      "['And', 'I', 'started', 'to', 'think', 'that', ',', ',', 'really', 'would', 'just', 'so', 'strong', 'as', 'the', '<unk>', 'of', '<unk>', '.', 'you', 'can', '.', 'to', 'your', '.', '</s>']\n",
      "(900 7%) 1.5183\n",
      "['We', 'get', 'a', 'stuff', 'that', '.', 'I', 'I', 'was', 'that', 'it', 'all', 'certain', 'age', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1000 8%) 1.5522\n",
      "['So', 'whenever', 'we', ',', 'choose', 'choose', 'alternatives', ',', 'the', 'plastics', ',', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(1100 9%) 1.5615\n",
      "['This', 'is', 'a', '<unk>', '.', 'He', '&apos;s', 'not', 'a', 'vending', 'machine', ',', 'but', 'he', 'is', 'a', 'of', 'the', 'founders', 'of', 'Genspace', ',', 'he', 'community', 'that', '</s>']\n",
      "(1200 10%) 1.5936\n",
      "['<unk>', '<unk>', 'in', '2010', 'last', '2000', 'the', ',', 'between', '<unk>', 'doubled', 'and', 'time', 'it', 'on', 'the', 'tripled', '.', '</s>', 'up', 'the', '.', 'that', 'that', '.', '</s>']\n",
      "(1300 10%) 1.6012\n",
      "['<unk>', 'didn', '&apos;t', '.', '.', '.', 'He', 'can', 'have', 'been', 'the', 'that', ',', 'same', 'thing', ',', 'but', 'he', 'can', 'not', 'if', 'he', 'partner', 'care', 'a', '</s>']\n",
      "(1400 11%) 1.6143\n",
      "['One', 'is', 'piloting', ',', 'the', 'rest', 'is', 'is', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1500 12%) 1.6452\n",
      "['They', '&apos;ve', 'digitized', 'millions', 'of', 'books', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1600 13%) 1.3991\n",
      "['Let', 'me', 'pause', 'pause', 'this', 'for', 'a', 'second', '.', 'I', '&apos;m', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1700 14%) 1.3647\n",
      "['We', 'need', 'to', 'understand', 'understand', 'why', 'people', 'understand', 'decisions', 'in', 'the', 'of', 'the', ',', 'and', 'why', 'we', '</s>', 'decisions', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(1800 15%) 1.3890\n",
      "['Because', 'there', '&apos;s', 'a', 'sport', ',', 'there', '&apos;s', 'going', 'animal', 'going', 'on', ',', 'we', '&apos;re', 'going', 'going', 'of', '.', 'and', '&apos;s', '&apos;re', '.', '</s>', '</s>', '</s>']\n",
      "(1900 15%) 1.4154\n",
      "['But', 'before', 'we', 'get', ',', ',', 'we', 'get', 'going', 'in', 'a', 'struggle', 'between', 'good', 'and', 'evil', ',', 'and', '<unk>', 'news', 'the', ',', 'the', 'people', 'of', '</s>']\n",
      "(2000 16%) 1.4425\n",
      "['Thank', 'you', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2100 17%) 1.4676\n",
      "['It', 'took', 'us', 'two', 'days', 'to', 'to', 'look', 'the', 'chest', ',', 'but', 'hey', ',', 'we', 'know', ',', 'we', 'were', 'out', 'the', '.', '</s>', 'he', 'look', '</s>']\n",
      "(2200 18%) 1.4844\n",
      "['Or', 'you', 'can', 'go', 'back', 'the', 'way', 'back', 'to', 'the', 'A.D.', '--', 'and', 'and', 'there', '&apos;s', 'a', 'guy', 'man', 'who', 'doesn', '&apos;t', 'know', 'to', 'idea', '</s>']\n",
      "(2300 19%) 1.5059\n",
      "['But', 'how', 'do', 'take', 'it', '?', '</s>', '.', '?', '?', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>']\n",
      "(2400 20%) 1.5236\n",
      "['The', '<unk>', '&apos;s', 'obese', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2500 20%) 1.5244\n",
      "['And', 'he', 'came', 'to', 'to', 'his', 'and', 'he', 'said', ',', '&quot;', 'Don', 'don', '&apos;t', 'understand', 'have', 'to', 'work', 'yourself', 'own', ',', 'you', '&apos;ve', 'them', 'do', '</s>']\n",
      "(2600 21%) 1.4369\n",
      "['The', 'corporation', 'owns', 'the', 'software', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2700 22%) 1.2698\n",
      "['Unfortunately', ',', 'I', 'I', 'was', 'there', ',', 'couldn', '&apos;t', 'do', ',', ',', 'the', ',', 'but', 'you', 'were', 'been', '<unk>', '.', 'well', '.', '</s>', '.', '.', '.']\n",
      "(2800 23%) 1.2968\n",
      "['This', 'this', 'is', 'what', 'the', 'next', 'work', 'is', 'about', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(2900 24%) 1.3287\n",
      "['When', 'we', 'animal', 'of', 'the', 'brain', 'is', 'active', ',', 'it', 'that', 'neural', 'area', ',', 'active', ',', 'it', '&apos;s', '<unk>', 'flow', '.', 'to', 'the', 'area', '.', '</s>']\n",
      "(3000 25%) 1.3494\n",
      "['In', 'at', 'the', 'same', 'time', 'we', '&apos;re', 'losing', 'a', 'equivalent', 'of', 'of', 'existing', '<unk>', 'and', '<unk>', 'and', 'erosion', '.', '</s>', '.', '.', '.', '.', '</s>', '</s>']\n",
      "(3100 25%) 1.3658\n",
      "['Now', 'as', 'as', 'people', 'got', 'to', ',', 'it', ',', 'the', '!', '</s>', '!', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3200 26%) 1.3973\n",
      "['You', 'have', 'a', 'dinner', 'party', '.', 'So', 'more', 'folds', 'out', 'to', 'dinner', 'in', 'steps', 'in', 'a', 'a', 'piano', '<unk>', ',', 'or', 'maybe', 'a', 'have', 'a', '</s>']\n",
      "(3300 27%) 1.4167\n",
      "['Because', 'from', 'a', 'global', 'perspective', ',', 'economic', 'degradation', 'begets', 'environmental', 'degradation', ',', 'and', 'begets', 'social', 'degradation', '.', '</s>', ',', ',', ',', 'and', 'differently', 'that', 'that', '</s>']\n",
      "(3400 28%) 1.4400\n",
      "['In', 'way', ',', 'when', 'they', 'get', 'money', 'money', ',', 'they', 'they', 'get', 'a', 'lot', 'raise', ',', 'they', 'can', '&apos;t', 'get', 'to', 'wait', 'each', 'money', '.', '</s>']\n",
      "(3500 29%) 1.4461\n",
      "['<unk>', 'are', 'important', 'for', 'biodiversity', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3600 30%) 1.4593\n",
      "['Is', 'that', 'a', 'model', 'of', 'model', 'for', 'creativity', '?', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3700 30%) 1.1833\n",
      "['Look', 'at', 'the', 'data', 'of', 'data', 'that', '&apos;s', 'available', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3800 31%) 1.2218\n",
      "['<unk>', 'for', 'instruments', ',', 'using', 'for', 'up', 'with', 'a', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'to', '</s>']\n",
      "(3900 32%) 1.2417\n",
      "['It', '&apos;s', 'cooling', 'off', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4000 33%) 1.2643\n",
      "['Thank', 'you', '.', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4100 34%) 1.2843\n",
      "['I', 'doesn', '&apos;t', 'matter', 'what', 'my', '&apos;ve', 'been', 'made', 'for', 'I', 'I', '&apos;ve', 'do', 'to', 'my', 'heart', '.', '</s>', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4200 35%) 1.3149\n",
      "['And', 'I', 'was', 'afraid', 'to', 'do', ',', 'because', 'I', 'was', 'just', '<unk>', 'to', 'read', 'people', 'who', 'read', 'had', 'read', '.', '</s>', '.', '.', '.', '.', '.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4300 35%) 1.3370\n",
      "['In', 'addition', 'to', 'the', '<unk>', 'generated', 'the', 'the', 'charcoal', 'is', 'about', 'million', 'dollars', '.', '</s>', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4400 36%) 1.3600\n",
      "['So', 'this', 'this', 'woman', ',', 'Mrs.', '<unk>', 'Han', ',', 'I', 'suppose', ',', 'be', 'taught', 'me', 'to', 'most', 'powerful', 'they', 'have', 'learned', 'about', '.', '.', '</s>', '</s>']\n",
      "(4500 37%) 1.3570\n",
      "['So', 'here', '&apos;s', 'the', ':', ':', '<unk>', 'interdependence', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4600 38%) 1.3834\n",
      "['And', 'these', 'genes', 'don', 'it', 'situation', 'to', 'just', 'as', 'a', 'as', 'a', 'gene', 'gene', '.', '.', 'acted', '.', '</s>', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4700 39%) 1.2235\n",
      "['And', 'if', 'you', 'think', 'about', 'this', 'this', 'experiment', 'testing', ',', 'in', 'order', 'introducing', 'context', 'elements', 'of', 'randomness', 'in', 'the', 'the', 'of', 'testing', 'and', 'training', ',', '</s>']\n",
      "(4800 40%) 1.1325\n",
      "['There', 'are', 'a', 'raft', 'of', 'research', ',', 'but', 'I', 'know', 'from', 'from', 'my', 'book', 'life', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4900 40%) 1.1544\n",
      "['When', 'you', 'are', 'happy', ',', 'you', 'know', 'grateful', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5000 41%) 1.1933\n",
      "['<unk>', 'million', 'hectares', 'of', 'rainforest', 'are', 'lost', 'in', 'year', 'to', 'reach', 'the', '<unk>', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(5100 42%) 1.2190\n",
      "['Thank', 'you', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5200 43%) 1.2324\n",
      "['Even', 'little', 'bit', 'further', 'out', 'as', 'the', 'article', 'it', 'would', '&quot;', '--', 'death', 'that', '&apos;s', 'destroy', 'be', 'a', '<unk>', 'shelf', '<unk>', 'to', ',', 'replace', '.', '</s>']\n",
      "(5300 44%) 1.2522\n",
      "['One', ',', 'two', ',', 'one', ',', 'two', ',', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5400 45%) 1.2780\n",
      "['You', 'can', 'even', 'it', 'even', 'getting', 'a', 'comfortable', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(5500 45%) 1.2969\n",
      "['Or', 'better', ',', 'when', 'do', 'you', 'think', 'it', 'people', 'can', 'able', 'to', 'achieve', 'things', 'that', 'can', 'to', 'defy', 'all', 'of', 'the', 'words', '?', '</s>', '.', '</s>']\n",
      "(5600 46%) 1.2979\n",
      "['They', 'change', 'the', 'person', 'to', 'person', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5700 47%) 1.2748\n",
      "['We', 'the', 'difference', 'within', 'Africa', 'African', 'country', '--', 'very', '&apos;s', 'from', 'a', 'low', 'level', 'to', 'the', 'high', 'level', ',', 'and', 'the', 'of', 'the', 'provinces', 'in', '</s>']\n",
      "(5800 48%) 1.0660\n",
      "['If', 'if', 'you', 'beat', 'your', 'wife', ',', 'you', '&apos;re', 'not', 'to', 'be', 'dancing', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5900 49%) 1.0904\n",
      "['So', 'if', 'you', 'told', 'me', 'I', 'can', '&apos;t', 'those', 'impossible', ',', 'I', 'can', 'tell', 'laugh', 'at', 'you', '.', '</s>', '?', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(6000 50%) 1.1228\n",
      "['So', 'we', '&apos;ve', 'our', 'likelihood', 'of', 'suffering', 'from', 'cancer', ',', 'or', 'in', 'a', 'car', 'accident', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6100 50%) 1.1361\n",
      "['It', 'also', 'also', 'them', 'to', 'help', 'a', 'productive', 'so', 'that', 'they', 'can', 'afford', 'to', 'own', 'insurance', 'within', 'time', 'and', 'not', 'assistance', '.', '</s>', 'money', 'for', '</s>']\n",
      "(6200 51%) 1.1651\n",
      "['So', ',', 'we', 'can', 'go', 'go', 'back', 'in', 'our', 'lives', ',', 'if', 'we', 'can', 'got', 'some', 'interviewer', 'who', 'can', 'more', ',', 'and', 'that', 'to', 'do', '</s>']\n",
      "(6300 52%) 1.1801\n",
      "['The', 'older', 'face', 'is', '<unk>', 'pleased', 'with', '.', 'decision', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6400 53%) 1.2090\n",
      "['They', 'made', 'his', 'silly', '<unk>', '<unk>', 'and', 'this', ',', 'and', 'they', 'had', 'it', 'silly', '<unk>', ',', 'like', 'something', '.', '</s>', 'himself', '.', '.', '.', '.', '</s>']\n",
      "(6500 54%) 1.2184\n",
      "['How', 'many', 'people', 'here', 'there', 'are', 'among', 'love', '?', 'Do', 'in', 'love', 'in', '.', '?', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6600 55%) 1.2358\n",
      "['And', ',', 'the', '--', 'are', '--', 'typically', 'fact', 'to', 'what', 'most', 'people', 'think', 'about', 'what', '--', 'is', 'are', 'going', 'sociable', 'creatures', '.', '</s>', '.', '</s>', '</s>']\n",
      "(6700 55%) 1.2569\n",
      "['And', 'if', 'any', 'current', 'of', 'been', 'so', 'for', 'so', 'long', ',', 'why', 'it', 'few', 'is', 'advances', '?', '</s>', '?', '?', '?', '?', '</s>', '</s>', '</s>', '</s>']\n",
      "(6800 56%) 1.0573\n",
      "['I', '&apos;m', 'not', 'Kofi', 'Annan', '.', 'He', 'can', '&apos;t', 'stop', '.', 'war', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(6900 57%) 1.0200\n",
      "['I', 'was', 'not', 'put', 'Charles', '<unk>', 'hip', 'for', 'TED2008', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '.', '.', '.', '.', '.']\n",
      "(7000 58%) 1.0535\n",
      "['And', 'can', 'we', 'not', 'to', 'to', 'thank', 'Thank', 'you', '.', '</s>', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7100 59%) 1.0744\n",
      "['Surely', 'not', '.', 'Let', '&apos;s', 'make', 'the', 'best', '100', 'years', '.', 'best', 'of', 'centuries', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7200 60%) 1.0940\n",
      "['And', 'that', '&apos;s', 'just', 'the', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7300 60%) 1.1219\n",
      "['With', 'this', 'interface', ',', 'it', 'have', 'up', 'the', 'traditional', 'array', 'of', 'traditional', 'in', 'developing', 'traditional', 'sports', 'games', 'and', '<unk>', 'games', ',', 'the', 'the', 'physical', 'possibilities', '</s>']\n",
      "(7400 61%) 1.1352\n",
      "['I', 'want', 'like', 'to', 'talk', 'about', 'what', 'I', 'mean', 'the', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(7500 62%) 1.1542\n",
      "['<unk>', 'me', 'for', 'it', 'meant', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7600 63%) 1.1757\n",
      "['So', '&apos;s', 'like', 'of', 'like', 'invitation', 'to', 'pick', 'up', 'the', 'over', '&apos;s', 'the', 'numbers', 'which', 'out', 'here', 'and', 'let', 'go', 'sit', 'it', 'into', 'the', 'box', '</s>']\n",
      "(7700 64%) 1.1935\n",
      "['I', 'had', 'not', 'to', 'to', 'be', 'difficult', 'to', 'difficult', '<unk>', 'nights', 'of', 'so', ',', ',', 'of', 'I', 'was', 'not', 'expect', 'of', '<unk>', 'of', '<unk>', 'and', '</s>']\n",
      "(7800 65%) 1.0990\n",
      "['So', 'he', '&apos;s', 'invited', 'me', 'the', 'opportunity', 'to', 'do', 'the', 'same', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7900 65%) 0.9628\n",
      "['But', 'anyway', ',', 'not', 'only', 'does', 'this', 'individual', 'have', 'this', 'three', 'meaning', ',', 'but', 'have', 'them', 'attention', 'on', 'a', '.', '</s>', '?', '.', '.', '.', '</s>']\n",
      "(8000 66%) 0.9808\n",
      "['Year', 'up', 'year', ',', 'after', 'year', ',', 'after', 'year', ',', 'I', 'have', 'more', 'innovative', 'than', 'their', 'their', 'money', '.', '</s>', '</s>', '.', '.', '.', '.', '.']\n",
      "(8100 67%) 1.0109\n",
      "['And', 'in', 'addition', ',', 'I', 'believe', 'that', 'there', 'oiled', 'animal', 'deserves', 'a', 'second', 'chance', 'at', 'life', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8200 68%) 1.0339\n",
      "['Two', 'weeks', 'old', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(8300 69%) 1.0651\n",
      "['I', 'mean', ',', 'from', '1965', 'to', '1973', 'there', 'are', 'more', 'than', 'than', 'would', 'on', 'the', 'than', 'in', 'the', 'of', 'the', 'World', 'II', 'Japan', ',', 'including', '</s>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8400 70%) 1.0792\n",
      "['We', 'created', 'an', '<unk>', 'of', 'a', 'flash', 'of', 'insight', ',', '&quot;', 'stroke', 'of', 'insight', ',', 'we', 'have', ',', ',', 'or', 'call', '&quot;', 'eureka', '!', '&quot;', '</s>']\n",
      "(8500 70%) 1.0983\n",
      "['I', 'work', 'done', 'doing', 'this', 'for', 'the', 'last', '15', 'years', ';', 'my', '&apos;s', 'my', 'job', 'job', ',', 'and', 'you', 'want', 'to', '</s>', '.', '.', '.', '.']\n",
      "(8600 71%) 1.1228\n",
      "['Mac', 'Barnett', ':', 'Why', 'a', 'book', 'book', 'is', 'a', 'secret', 'door', '</s>', '</s>', '.', '.', '?', '?', '?', '?', '?', '?', '?', '</s>', '</s>', '</s>', '</s>']\n",
      "(8700 72%) 1.1294\n",
      "['Look', ',', 'I', '&apos;m', 'go', 'around', 'and', 'say', ',', 'of', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8800 73%) 1.1449\n",
      "['Country', 'A', ',', 'in', '1990', ',', 'had', 'about', '$', '300', 'per', 'capita', 'GDP', 'as', 'compared', 'with', 'Country', 'B', ',', 'which', 'in', '$', '<unk>', 'per', 'America', '</s>']\n",
      "(8900 74%) 0.9121\n",
      "['I', 'don', '&apos;t', 'have', 'any', 'patents', ',', 'and', 'I', 'never', 'never', 'met', 'any', 'money', 'from', 'a', 'medical', 'imaging', 'company', ',', 'and', 'I', '&apos;ve', 'not', 'seeking', '</s>']\n",
      "(9000 75%) 0.9280\n",
      "['You', 'decode', 'what', 'you', 'want', 'to', 'do', 'up', 'into', 'here', 'which', 'the', '<unk>', 'of', 'angles', '--', 'so', 'negative', '120', ',', 'negative', '120', ',', '0', ',', '</s>']\n",
      "(9100 75%) 0.9459\n",
      "['Many', 'of', 'them', 'will', 'think', 'thinking', ',', 'well', 'I', 'we', 'can', 'still', 'be', 'looking', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9200 76%) 0.9777\n",
      "['She', 'she', '&apos;s', 'a', 'door', 'to', 'a', 'very', 'wealthy', ',', '<unk>', 'neighbor', '.', '</s>', '.', '?', '?', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(9300 77%) 0.9956\n",
      "['About', 'six', 'years', 'ago', 'I', 'decided', 'that', 'I', 'had', 'learn', 'from', ',', 'which', 'is', 'out', 'to', 'be', 'a', '<unk>', 'of', 'language', '.', '</s>', '.', '.', '</s>']\n",
      "(9400 78%) 1.0294\n",
      "['Here', ',', 'you', 'can', 'see', 'over', '2006', ',', 'they', '&apos;re', 'almost', 'a', 'quarters', 'of', 'a', 'chick', 'per', 'nest', ',', 'and', 'you', 'see', 'see', 'that', 'it', '</s>']\n",
      "(9500 79%) 1.0418\n",
      "['All', 'right', ',', 'so', 'I', '&apos;ve', 'to', 'collection', 'of', 'the', 'paintings', 'from', 'and', 'I', 'said', 'at', 'my', 'and', 'I', 'said', 'them', 'and', 'I', 'said', 'them', '</s>']\n",
      "(9600 80%) 1.0566\n",
      "['The', 'worst', 'part', 'was', 'this', 'year', 'was', 'that', 'year', 'decided', 'it', 'were', 'to', 'go', 'a', 'long', '<unk>', 'operation', '.', 'Mt', '.', 'Everest', '.', '</s>', '</s>', '</s>']\n",
      "(9700 80%) 1.0748\n",
      "['The', 'idea', 'is', 'really', 'quite', 'simple', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', 'to', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(9800 81%) 1.0974\n",
      "['Well', ',', '&apos;s', 'is', 'a', 'technical', 'detail', ',', 'but', 'it', 'details', '.', 'matter', '.', '</s>', '<unk>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9900 82%) 0.9665\n",
      "['&quot;', 'says', ',', '&quot;', 'I', 'can', '&apos;t', 'grow', '.', 'stuff', '.', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10000 83%) 0.8789\n",
      "['But', 'now', 'I', '&apos;m', 'feeling', 'really', 'weird', 'because', 'I', '&apos;m', 'speaking', 'my', 'Sun', 'Tzu', 'said', 'one', 'week', '.', '</s>', 'again', '.', '.', 'differently', 'differently', 'differently', '</s>']\n",
      "(10100 84%) 0.9013\n",
      "['So', 'when', 'does', 'the', 'transition', 'begin', '?', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10200 85%) 0.9248\n",
      "['And', 'to', 'order', 'to', 'introduce', 'you', 'to', 'what', ',', 'I', 'have', 'got', 'to', 'here', 'that', 'I', '&apos;m', 'going', 'to', 'unveil', ',', 'which', 'is', 'a', 'of', '</s>']\n",
      "(10300 85%) 0.9492\n",
      "['Lots', 'of', 'credit', 'card', 'debt', ',', 'huge', 'problem', 'footprints', ',', 'and', 'perhaps', 'not', 'coincidentally', ',', 'our', 'ability', 'levels', ',', ',', 'the', 'last', 'time', 'years', '.', '</s>']\n",
      "(10400 86%) 0.9693\n",
      "['And', 'the', 'first', 'time', 'I', 'figured', 'out', 'that', 'was', 'was', 'in', 'a', 'way', '<unk>', 'submersible', '.', 'Deep', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.']\n",
      "(10500 87%) 0.9860\n",
      "['It', ',', 'this', 'year', ',', 'the', '50th', 'anniversary', 'of', '<unk>', '&apos;s', '<unk>', ',', 'which', 'is', 'a', 'of', 'my', 'favorite', 'cars', '.', '</s>', '.', '.', '.', '.']\n",
      "(10600 88%) 1.0077\n",
      "['And', 'daddy', ',', 'if', 'you', 'were', 'like', ',', 'you', '&apos;d', 'say', 'like', 'dad', '.', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10700 89%) 1.0309\n",
      "['You', ',', ',', 'the', 'big', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(10800 90%) 1.0397\n",
      "['But', 'think', 'again', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10900 90%) 1.0126\n",
      "['Traditionally', ',', 'the', 'requires', 'expensive', 'equipment', 'to', 'examine', 'the', 'artificial', 'called', 'the', 'retina', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11000 91%) 0.8335\n",
      "['Now', '&apos;m', 'going', 'to', 'show', 'you', 'something', 'very', 'very', 'weird', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(11100 92%) 0.8527\n",
      "['I', 'said', ',', '&quot;', 'Because', 'Saddam', 'Hussein', 'compared', 'me', 'to', 'a', '<unk>', 'serpent', '.', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11200 93%) 0.8766\n",
      "['This', 'is', '&quot;', 'Sleep', ',', '&quot;', 'the', 'Virtual', 'Choir', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11300 94%) 0.9027\n",
      "['This', 'is', 'the', 'river', ',', 'covered', ',', 'in', 'as', 'a', 'trash', 'yard', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11400 95%) 0.9203\n",
      "['A', 'transistor', 'is', 'nothing', 'more', 'than', 'a', 'wall', 'switch', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11500 95%) 0.9433\n",
      "['And', 'that', '&apos;s', 'exactly', 'where', 'we', '&apos;re', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11600 96%) 0.9585\n",
      "['You', 'learn', 'from', 'your', 'mistakes', ',', 'and', 'you', 'continuously', 'correct', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11700 97%) 0.9767\n",
      "['What', 'I', '&apos;m', 'you', 'to', 'to', 'do', '&#93;', 'now', 'is', 'to', 'see', 'how', 'the', '<unk>', 'sex', 'that', 'relation', 'between', 'mental', 'in', 'terms', 'mortality', 'and', 'decrease', '</s>']\n",
      "(11800 98%) 0.9908\n",
      "['And', 'the', 'complete', 'fit', 'perfectly', '.', '</s>', 'perfectly', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11900 99%) 1.0098\n",
      "['It', 'could', 'tell', 'a', 'lawyer', '&apos;s', 'or', 'could', 'cellphone', 'provider', '&apos;s', ',', 'and', 'it', '&apos;s', 'not', 'the', 'the', 'gay', 'service', 'anyway', '.', '</s>', '.', '.', '</s>']\n",
      "(12000 100%) 0.8345\n",
      "['Nelson', 'Mandela', 'went', 'to', 'jail', 'believing', 'in', 'violence', ',', 'and', 'his', 'years', 'later', 'he', 'and', 'his', 'colleagues', 'had', 'to', 'and', 'carefully', '<unk>', 'the', 'answer', ',', '</s>']\n"
     ]
    }
   ],
   "source": [
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_noatten_lr0.001_step24000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_noatten_lr0.001_step24000_decoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100 0%) 0.8351\n",
      "['He', 'said', ',', '&quot;', '<unk>', '&apos;s', 'Billy', 'Graham', ',', 'the', 'preacher', '.', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(200 1%) 0.8563\n",
      "['So', ',', 'what', 'do', 'it', 'to', '?', '</s>', '.', '?', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'to', 'to', 'to', 'to', 'to']\n",
      "(300 2%) 0.8698\n",
      "['There', 'people', 'are', 'cooking', 'fires', 'in', 'the', 'cooking', ',', 'whether', 'it', '&apos;s', 'camel', 'dung', 'or', 'wood', '.', '</s>', 'photos', '.', '.', '.', '.', '.', '.', '.']\n",
      "(400 3%) 0.8958\n",
      "['Up', 'to', 'the', 'around', 'about', 'war', 'Second', 'World', 'War', ',', '<unk>', '<unk>', 'invariably', 'dictators', '--', 'these', '<unk>', 'figures', 'who', 'were', 'rehearse', ',', 'not', 'just', 'the', '</s>']\n",
      "(500 4%) 0.9070\n",
      "['And', 'indeed', ',', 'that', '&apos;s', 'exactly', 'what', 'this', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>']\n",
      "(600 5%) 0.9338\n",
      "['<unk>', 'motivated', 'motives', 'and', 'feelings', 'of', 'others', 'is', 'a', 'shared', 'talent', 'for', 'people', '.', 'They', 'on', 'to', 'you', 'engage', 'it', '?', '<unk>', '&apos;s', 'Rebecca', '<unk>', '</s>']\n",
      "(700 5%) 0.9381\n",
      "['And', 'to', 'do', 'that', 'we', 'have', 'done', 'a', 'number', 'of', 'partnerships', 'with', 'NGOs', 'and', 'universities', 'to', 'gather', 'data', 'on', 'the', 'world', 'interface', ',', 'on', 'the', '</s>']\n",
      "(800 6%) 0.9571\n",
      "['And', 'I', 'I', 'always', 'always', 'been', 'fascinated', 'by', 'free-style', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(900 7%) 0.9721\n",
      "['Innovation', 'is', 'hard', 'because', 'it', 'means', 'something', 'something', 'meaningful', 'is', 'think', '&apos;t', 'think', 'meaningful', 'much', ',', 'what', 'sure', 'basics', 'part', ',', '</s>', 'in', ',', '</s>', '</s>']\n",
      "(1000 8%) 0.8913\n",
      "['Thank', 'you', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1100 9%) 0.7668\n",
      "['Pamela', 'Meyer', ':', 'How', 'to', 'spot', 'a', 'liar', '</s>', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '.', '.', '.']\n",
      "(1200 10%) 0.7834\n",
      "['So', 'that', '&apos;s', 'my', 'work', ',', 'and', 'what', 'was', 'the', 'work', 'to', 'reimagining', 'the', 'future', 'for', 'future', 'research', ',', 'which', 'figuring', 'out', 'how', 'what', 'this', '</s>']\n",
      "(1300 10%) 0.8038\n",
      "['So', 'researchers', 'wanted', 'to', 'know', 'why', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1400 11%) 0.8251\n",
      "['It', 'doesn', '&apos;t', 'really', 'out', ',', 'particularly', ',', 'but', 'that', '&apos;s', 'a', 'particularly', 'tall', 'bicycle', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1500 12%) 0.8508\n",
      "['I', 'suspect', 'there', 'are', 'some', 'people', 'out', 'there', 'in', 'next', 'a', 'of', '<unk>', 'questions', 'questions', ',', 'so', ',', 'well', 'who', 'in', ',', 'wait', ',', 'wait', '</s>']\n",
      "(1600 13%) 0.8650\n",
      "['And', 'that', '&apos;s', 'where', 'the', 'kids', 'kids', 'got', 'arrested', ',', 'and', 'that', '&apos;s', 'a', 'grandmother', 'of', 'a', 'of', 'them', '.', '</s>', '.', '.', '.', '.', '.']\n",
      "(1700 14%) 0.8866\n",
      "['Choose', 'your', 'frame', 'of', 'reference', 'and', 'the', 'brain', 'value', 'and', 'everything', 'the', 'actual', 'value', 'is', 'completely', '<unk>', '.', '</s>', 'merely', '.', '?', 'to', 'to', 'to', '</s>']\n",
      "(1800 15%) 0.9069\n",
      "['The', 'ship', 'they', 'go', 'on', 'tour', '.', 'it', '</s>', 'point', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1900 15%) 0.9213\n",
      "['You', 'need', 'a', 'place', 'where', 'people', 'can', 'leave', 'back', ',', 'not', 'down', 'with', 'talking', 'of', 'judgment', ',', 'without', '<unk>', ',', 'to', 'respect', 'stand', 'the', 'team', '</s>']\n",
      "(2000 16%) 0.9414\n",
      "['But', 'it', 'is', 'not', 'an', 'foregone', 'conclusion', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2100 17%) 0.7303\n",
      "['We', '&apos;ve', 'put', 'a', 'on', 'the', 'moon', '.', 'We', '&apos;ve', 'done', 'all', 'the', 'of', 'extraordinary', 'things', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2200 18%) 0.7555\n",
      "['But', 'the', 'problem', 'is', ',', 'how', 'do', 'you', 'decide', 'what', 'is', 'going', 'that', 'to', 'make', 'you', 'decisions', 'and', 'make', 'you', 'you', 'make', 'sure', 'that', 'they', '</s>']\n",
      "(2300 19%) 0.7720\n",
      "['So', 'so', 'little', 'we', 'had', 'a', 'information', 'about', 'the', 'young', 'health', 'to', 'share', 'with', 'him', ',', 'maybe', 'we', 'could', 'have', 'never', 'a', 'better', 'diagnosis', 'that', '</s>']\n",
      "(2400 20%) 0.7802\n",
      "['Now', 'you', 'may', 'looked', 'at', 'them', 'now', 'some', 'few', 'times', 'today', ',', 'and', 'I', '&apos;ll', 'going', 'to', 'ask', 'you', 'to', 'question', 'about', 'these', ',', '</s>', '</s>']\n",
      "(2500 20%) 0.8135\n",
      "['Basically', 'we', 'found', 'that', 'almost', 'anything', 'that', '&apos;s', 'related', 'to', 'this', 'internationally', 'is', 'related', 'to', 'whatever', 'amongst', 'the', 'species', 'states', 'in', 'this', 'game', 'test', 'bed', '</s>']\n",
      "(2600 21%) 0.8372\n",
      "['So', 'was', '.', '.', 'And', '&apos;s', 'all', 'he', 'didn', '.', 'much', 'options', '.', 'And', 'not', 'too', 'long', 'off', '.', '</s>', '.', '.', '.', '.', '.', '.']\n",
      "(2700 22%) 0.8377\n",
      "['And', 'it', 'the', 'family', 'world', 'started', 'to', 'think', 'family', 'planning', '.', '</s>', 'thinking', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2800 23%) 0.8762\n",
      "['<unk>', ',', '<unk>', ',', 'convictions', ',', 'exclamation', 'marks', ',', 'paradigms', ',', 'dogmas', '.', '</s>', ',', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(2900 24%) 0.8841\n",
      "['And', 'we', 'have', 'a', 'story', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3000 25%) 0.8948\n",
      "['I', 'was', 'been', 'with', 'to', 'mess', 'with', 'things', 'like', 'chain', 'link', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3100 25%) 0.7841\n",
      "['So', 'with', 'the', 'models', 'like', 'this', ',', 'we', 'can', 'now', 'back', 'and', 'and', 'ever', 'before', 'and', 'even', 'the', 'disease', ',', 'than', 'ever', 'before', ',', 'and', '</s>']\n",
      "(3200 26%) 0.7156\n",
      "['This', 'is', 'a', 'of', 'a', 'self-portrait', '<unk>', 'box', 'capsule', 'piece', 'called', 'for', 'Point', 'Just', '<unk>', ',', 'I', 'which', 'I', 'call', 'myself', 'on', 'the', 'of', 'a', '</s>']\n",
      "(3300 27%) 0.7387\n",
      "['And', 'that', '&apos;s', 'hard', ',', 'but', 'it', 'it', 'hard', 'boring', 'as', 'having', 'to', 'use', 'a', 'condom', 'every', 'time', 'you', 'have', 'sex', ',', 'you', 'matter', 'how', '</s>']\n",
      "(3400 28%) 0.7541\n",
      "['But', 'actually', 'fact', ',', 'the', '&apos;s', 'not', 'what', 'Frankenstein', '&apos;s', 'lab', 'looks', 'like', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3500 29%) 0.7754\n",
      "['And', 'I', 'didn', '&apos;t', 'have', 'money', 'for', 'the', 'flight', '.', '</s>', '?', '?', '?', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3600 30%) 0.7974\n",
      "['So', 'we', 'have', 'have', 'implemented', 'it', 'in', 'in', 'some', 'selected', 'clinics', 'in', 'the', 'provinces', ',', 'and', 'you', '&apos;re', 'the', 'first', 'to', 'see', 'if', 'hospitals', '.', '</s>']\n",
      "(3700 30%) 0.8101\n",
      "['They', 'flash', 'up', 'into', 'my', 'memory', ',', 'just', 'like', 'Google', 'for', 'pictures', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3800 31%) 0.8294\n",
      "['You', 'need', 'need', 'advocates', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3900 32%) 0.8452\n",
      "['The', 'next', 'stage', 'introduces', 'entities', 'that', 'are', 'significantly', 'more', 'fragile', ',', 'significantly', 'more', 'vulnerable', ',', 'but', 'they', '&apos;re', '<unk>', 'really', 'more', '<unk>', 'and', 'much', '</s>', '</s>']\n",
      "(4000 33%) 0.8659\n",
      "['And', 'usually', 'when', 'I', 'tell', 'people', 'I', '&apos;m', 'talking', 'artist', ',', 'or', '&apos;re', 'do', 'at', 'me', 'and', 'said', ',', '&quot;', 'Well', 'you', 'really', '?', '&quot;', '</s>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4100 34%) 0.8380\n",
      "['So', ',', 'in', 'the', 'same', 'way', 'that', 'you', 'wake', 'up', ',', 'if', 'a', 'shower', 'and', 'get', 'dressed', ',', 'you', 'have', 'to', 'be', 'to', 'them', 'that', '</s>']\n",
      "(4200 35%) 0.6910\n",
      "['Now', ',', 'the', 'the', 'people', 'that', 'could', '&quot;', 'them', 'apart', ',', 'when', 'the', 'labels', 'were', 'off', ',', 'they', 'were', '&quot;', '<unk>', ',', '&quot;', 'and', 'when', '</s>']\n",
      "(4300 35%) 0.7045\n",
      "['So', 'they', 'were', 'happy', 'with', 'it', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4400 36%) 0.7194\n",
      "['That', 'guides', 'you', 'back', 'to', 'the', 'location', 'that', 'you', 'want', 'to', 'remember', '.', '</s>', '.', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4500 37%) 0.7416\n",
      "['It', '&apos;s', 'very', 'different', 'than', 'the', 'different', 'cultures', '.', '&quot;', 'right', '?', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4600 38%) 0.7518\n",
      "['The', 'accusation', 'was', 'that', 'I', 'had', 'successfully', 'prepared', 'a', 'entire', 'way', 'Stalin', '&apos;s', 'life', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4700 39%) 0.7750\n",
      "['In', 'fact', ',', 'what', 'people', 'say', 'talked', 'about', '<unk>', 'were', 'poor', 'is', 'just', 'of', 'the', '<unk>', 'note', '.', '</s>', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4800 40%) 0.7919\n",
      "['The', '&apos;s', 'a', 'a', '<unk>', 'term', 'for', 'this', ':', 'lethal', 'autonomy', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4900 40%) 0.8089\n",
      "['But', 'I', 'love', 'to', 'talk', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5000 41%) 0.8356\n",
      "['And', 'so', 'I', 'can', 'here', 'standing', 'on', 'the', 'shoulders', 'of', 'the', 'people', '.', '</s>', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5100 42%) 0.8416\n",
      "['The', '<unk>', 'is', 'is', 'a', 'system', 'where', 'which', 'it', '&apos;s', 'going', 'that', 'input', 'and', 'output', 'spaces', 'are', '<unk>', '.', '</s>', 'work', '?', '.', '?', '.', '.']\n",
      "(5200 43%) 0.7015\n",
      "['And', 'a', 'lot', 'of', 'my', 'school', 'teachers', 'didn', '&apos;t', 'appreciate', 'that', ';', 'harder', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5300 44%) 0.6720\n",
      "['You', 'don', '&apos;t', 'have', 'enough', 'time', '.', '</s>', 'up', '.', '.', '.', '.', '?', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(5400 45%) 0.6807\n",
      "['But', 'importantly', ',', 'it', '&apos;s', 'a', 'rich', 'source', 'of', 'vitamin', '<unk>', '.', '</s>', ',', '.', '?', '.', '.', '.', '.', '</s>', '</s>', '.', '.', '.', '.']\n",
      "(5500 45%) 0.7061\n",
      "['J', 'So', 'I', 'are', 'telling', 'they', 'I', ',', 'I', 'say', 'say', ',', 'they', 'whistleblowers', ',', 'and', 'they', 'have', 'a', 'lot', 'of', 'their', 'that', 'them', 'to', '</s>']\n",
      "(5600 46%) 0.7182\n",
      "['Five', 'years', 'ago', 'I', 'worked', 'worked', 'at', 'the', 'JPL', 'during', 'the', 'summer', 'as', 'a', 'faculty', 'fellow', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5700 47%) 0.7464\n",
      "['These', 'things', 'of', 'things', '.', 'People', 'are', 'bringing', 'their', 'attention', 'selves', ',', 'towards', '.', 'They', '&apos;re', '<unk>', 'themselves', '.', '</s>', ',', ',', '.', '.', ',', '</s>']\n",
      "(5800 48%) 0.7624\n",
      "['I', 'said', 'describing', 'this', 'in', 'Berlin', 'once', 'and', 'the', 'next', 'day', 'in', 'a', 'newspaper', ',', 'headline', 'said', ',', '&quot;', 'I', 'am', 'the', 'Queen', 'of', 'England', '</s>']\n",
      "(5900 49%) 0.7835\n",
      "['We', ',', 'we', 'like', 'to', 'think', 'that', 'technology', 'creates', 'jobs', 'as', 'a', 'period', 'of', 'time', 'after', 'a', 'short', ',', 'temporary', 'period', 'of', '<unk>', ',', 'and', '</s>']\n",
      "(6000 50%) 0.7969\n",
      "['The', 'green', 'represents', 'for', 'percentage', '.', '</s>', '.', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6100 50%) 0.8104\n",
      "['I', 'mean', ',', 'there', 'bright', ',', '<unk>', '<unk>', 'of', '<unk>', '.', 'there', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(6200 51%) 0.7554\n",
      "['This', 'is', 'called', '&quot;', 'Dawn', 'to', 'Dawn', '.', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6300 52%) 0.6439\n",
      "['And', 'a', 'final', 'thing', ':', 'When', 'you', 'love', 'a', 'value', 'on', 'the', 'like', 'health', ',', 'love', ',', 'sex', 'and', 'pleasure', 'things', '--', 'and', 'having', 'to', '</s>']\n",
      "(6400 53%) 0.6461\n",
      "['We', 'don', '&apos;t', 'have', 'about', 'the', 'Einstein', 'who', 'used', 'the', 'celebrity', 'claims', 'advocate', 'for', 'political', 'prisoners', 'in', 'the', 'or', 'the', 'American', 'boys', 'in', 'the', '</s>', '</s>']\n",
      "(6500 54%) 0.6658\n",
      "['But', 'I', '&apos;d', 'like', 'to', 'do', 'up', 'up', 'by', 'by', 'making', 'a', 'a', 'from', 'the', 'in', 'the', 'presentation', '.', '</s>', '.', '.', '.', '.', '.', '.']\n",
      "(6600 55%) 0.6917\n",
      "['But', 'I', 'couldn', '&apos;t', 'do', 'it', 'else', '.', '</s>', '?', '?', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6700 55%) 0.7108\n",
      "['But', 'this', 'time', ',', 'China', 'Chinese', 'doesn', '&apos;t', 'have', 'it', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6800 56%) 0.7282\n",
      "['And', 'the', 'changes', 'the', 'color', 'in', 'a', 'given', 'color', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6900 57%) 0.7475\n",
      "['<unk>', 'and', 'his', 'colleague', ',', '<unk>', '<unk>', ',', 'the', 'subjects', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(7000 58%) 0.7703\n",
      "['The', 'only', 'rule', 'is', 'no', '&apos;t', 'think', 'of', 'hard', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7100 59%) 0.7926\n",
      "['So', 'the', 'key', 'detail', 'is', 'this', 'is', 'that', 'Fey', '&apos;s', 'scripts', 'weren', '&apos;t', 'written', 'by', 'and', 'and', 'they', 'weren', '&apos;t', 'written', 'by', 'the', 'American', 'writers', '</s>']\n",
      "(7200 60%) 0.8072\n",
      "['On', 'August', '14th', 'and', '15th', ',', 'they', 'took', 'eight', 'stool', 'sample', ',', 'and', 'by', 'the', '25th', 'of', 'September', ',', 'it', 'was', 'confirmed', 'he', 'had', 'Type', '</s>']\n",
      "(7300 60%) 0.6185\n",
      "['So', 'when', 'I', 'really', 'the', 'calculations', 'for', 'the', 'lunar', 'data', 'and', 'the', 'solar', 'system', 'at', 'the', 'location', 'on', 'Earth', 'at', 'the', 'same', 'of', 'the', 'incident', '</s>']\n",
      "(7400 61%) 0.6320\n",
      "['I', '&apos;m', 'going', 'going', 'to', 'give', 'you', 'a', 'demo', '.', 'She', 'the', 'way', ',', 'I', '&apos;m', 'going', 'going', 'to', 'nerd', 'out', 'for', 'the', 'a', 'few', '</s>']\n",
      "(7500 62%) 0.6462\n",
      "['And', 'I', 'want', 'to', 'start', 'by', 'telling', 'you', 'the', 'story', 'about', 'the', 'leadership', 'in', 'Africa', 'seems', '</s>', '.', '.', '.', '.', '.', 'to', 'to', '</s>', '</s>']\n",
      "(7600 63%) 0.6608\n",
      "['And', 'when', 'I', '&apos;s', 'said', ',', ',', ',', 'it', '&apos;s', 'quite', 'quite', 'abstract', '.', '</s>', 'drives', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7700 64%) 0.6835\n",
      "['And', 'we', 'need', 'to', 'diversify', 'our', 'farms', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'to', 'to']\n",
      "(7800 65%) 0.7042\n",
      "['Jonathan', 'Drori', ':', 'Why', 'we', '&apos;re', 'storing', 'billions', 'of', 'seeds', '</s>', '?', '?', '?', '?', '?', '?', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(7900 65%) 0.7246\n",
      "['We', '&apos;re', 'mostly', 'done', 'building', 'things', ',', 'and', 'now', 'we', '&apos;re', 'testing', 'it', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8000 66%) 0.7480\n",
      "['This', ',', ',', 'for', 'example', ',', 'is', 'probably', 'of', '<unk>', 'to', 'it', 'provides', 'sums', 'up', 'what', 'the', 'can', 'be', '.', ',', 'the', 'because', 'and', 'it', '</s>']\n",
      "(8100 67%) 0.7493\n",
      "['I', '&apos;m', 'not', 'sure', 'if', '&apos;d', 'want', 'to', 'to', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8200 68%) 0.7733\n",
      "['Lord', '<unk>', 'once', 'said', 'he', 'saw', 'people', 'bathing', 'in', 'the', 'sky', 'Sea', ',', 'and', 'he', 'said', ',', '&quot;', 'Why', 'don', 'you', 'one', 'on', 'you', 'what', '</s>']\n",
      "(8300 69%) 0.6685\n",
      "['And', 'it', '&apos;s', 'the', 'we', 'can', 'bend', 'the', 'arc', 'of', 'history', 'down', 'towards', 'zero', ',', 'just', 'doing', 'the', 'things', 'that', 'we', 'know', 'or', '.', '</s>', '</s>']\n",
      "(8400 70%) 0.5997\n",
      "['And', 'I', 'said', ',', '&quot;', 'What', '&apos;s', 'your', 'Movember', 'story', '?', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', 'differently', 'differently', 'to', 'to', 'to', '.']\n",
      "(8500 70%) 0.6211\n",
      "['One', '<unk>', 'of', 'this', 'is', 'in', 'the', 'United', 'Nations', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8600 71%) 0.6382\n",
      "['Actually', 'actually', '&apos;s', 'actually', 'a', 'instructions', 'that', 'you', 'only', 'use', 'once', 'a', 'you', 'you', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>']\n",
      "(8700 72%) 0.6532\n",
      "['What', 'it', 'is', 'doing', ',', ',', 'we', 'find', 'a', '<unk>', 'a', 'path', 'collision', '--', 'we', 'find', 'a', 'way', 'collision', 'between', 'us', '--', 'and', 'what', '&apos;s', '</s>']\n",
      "(8800 73%) 0.6788\n",
      "['So', ',', 'I', 'was', 'about', 'nine', ',', 'my', 'my', 'uncle', 'had', 'just', 'come', 'up', 'from', 'Germany', ',', 'and', 'we', 'had', 'the', 'Catholic', 'priest', 'over', ',', '</s>']\n",
      "(8900 74%) 0.6889\n",
      "['Many', 'think', 'think', 'that', 'West', 'on', 'the', 'map', ',', 'longer', ',', ',', 'but', 'they', '<unk>', 'are', 'that', '&apos;re', 'it', 'Using', 'maps', 'because', 'the', 'past', 'and', '</s>']\n",
      "(9000 75%) 0.7056\n",
      "['You', '&apos;ve', 'have', 'wondered', 'what', 'this', 'if', 'I', '&apos;ve', 'you', 'patient', '<unk>', 'little', 'too', '.', '</s>', 'drives', '?', '.', '.', '.', '.', '.', '.', '</s>', '</s>']\n",
      "(9100 75%) 0.7312\n",
      "['It', 'would', 'be', 'me', 'same', 'window', ',', 'I', 'put', 'my', 'thumb', 'up', 'the', 'blocked', 'that', 'spotlight', 'that', 'I', 'right', 'right', 'in', 'the', 'eye', ',', 'I', '</s>']\n",
      "(9200 76%) 0.7467\n",
      "['<unk>', 'Mohamed', ':', 'So', 'as', 'you', 'know', ',', 'the', 'in', 'the', 'game', 'war', ',', 'the', 'most', 'affected', 'by', 'women', 'and', '&#93;', 'the', 'women', 'and', 'children', '</s>']\n",
      "(9300 77%) 0.7248\n",
      "['These', 'targets', 'have', 'been', 'achieved', 'in', 'India', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9400 78%) 0.5757\n",
      "['But', 'it', 'that', 'meant', 'that', 'a', 'have', 'to', 'to', 'be', 'think', 'about', 'the', 'far', 'of', 'halls', 'they', 'put', 'together', '.', 'There', 'are', 'always', 'few', 'halls', '</s>']\n",
      "(9500 79%) 0.5909\n",
      "['And', 'this', '&apos;s', '&quot;', 'me', '.', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(9600 80%) 0.6131\n",
      "['<unk>', 'alcohol', ',', 'it', 'seems', 'for', 'it', 'it', 'for', 'your', 'marriage', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9700 80%) 0.6333\n",
      "['And', 'in', 'the', 'last', 'months', 'that', 'it', 'project', 'that', 'been', 'held', ',', 'it', 'became', 'been', 'downloaded', 'over', 'half', 'a', 'million', 'times', '.', '</s>', 'for', '.', '.']\n",
      "(9800 81%) 0.6455\n",
      "['How', 'can', 'one', 'explain', 'what', 'can', 'can', 'at', 'night', 'in', 'a', 'lab', ',', 'every', 'one', 'night', '?', '</s>', 'in', '?', '?', '?', '?', '.', '</s>', '</s>']\n",
      "(9900 82%) 0.6666\n",
      "['I', 'had', 'to', 'learn', 'it', 'language', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10000 83%) 0.6920\n",
      "['So', ',', 'variety', 'is', 'important', '.', 'Have', 'you', 'ever', 'rented', 'a', 'video', 'or', 'a', 'film', 'that', 'you', '&apos;ve', 'seen', 'seen', '?', '</s>', '&apos;s', 'done', 'this', '</s>']\n",
      "(10100 84%) 0.7011\n",
      "['I', 'am', 'speaking', 'of', 'Sweden', 'because', 'Sweden', 'has', 'about', 'little', 'bit', 'of', 'a', 'similar', 'law', 'to', 'the', 'United', 'States', '.', '</s>', '.', '.', '.', '.', '.']\n",
      "(10200 85%) 0.7151\n",
      "['And', 'when', 'we', 'can', 'these', 'things', ',', 'we', 'can', 'do', 'we', 'become', 'this', 'this', 'into', 'all', '.', '</s>', 'civilization', 'that', '.', '.', '.', '.', '.', '</s>']\n",
      "(10300 85%) 0.7258\n",
      "['&quot;', 'On', 'June', '20th', ',', '&quot;', 'if', 'you', 'can', 'read', 'it', ',', '&quot;', '2011', '.', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10400 86%) 0.5998\n",
      "['Is', 'it', 'the', 'exposure', '?', 'We', '&apos;ve', 'professors', 'from', 'MIT', ',', 'Berkeley', ',', 'Stanford', ',', 'Indian', 'Institute', 'of', 'Science', 'who', 'come', 'and', 'teach', 'our', 'children', '</s>']\n",
      "(10500 87%) 0.5679\n",
      "['Angela', '<unk>', ':', 'Using', 'nature', 'to', 'grow', 'batteries', '</s>', '</s>', '?', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(10600 88%) 0.5852\n",
      "['It', 'is', 'a', 'private', 'of', 'action', '.', '</s>', 'is', 'a', 'activity', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10700 89%) 0.6019\n",
      "['This', 'is', 'why', 'you', 'have', 'a', '--', 'new', 'paradigm', 'shift', ',', 'where', 'no', 'next', 'people', 'who', 'could', 'not', 'be', 'of', 'the', 'in', 'a', 'car', ',', '</s>']\n",
      "(10800 90%) 0.6243\n",
      "['I', 'I', '&apos;ve', 'thinking', 'about', 'and', 'over', 'here', 'my', 'mind', ',', 'what', 'did', 'happened', '?', 'Where', 'did', 'it', 'go', '?', '?', 'Why', 'didn', '&apos;t', 'I', '</s>']\n",
      "(10900 90%) 0.6397\n",
      "['Just', 'Morgana', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '.']\n",
      "(11000 91%) 0.6538\n",
      "['But', 'actually', ',', 'plastics', 'are', 'several', 'times', 'more', 'valuable', 'than', 'steel', '.', '</s>', 'stage', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(11100 92%) 0.6823\n",
      "['And', 'when', 'we', 'fight', 'that', 'along', 'of', 'the', ',', ',', 'we', 'fight', 'say', 'that', 'our', 'fight', 'is', 'not', ',', 'each', 'other', ',', 'but', 'country', 'is', '</s>']\n",
      "(11200 93%) 0.6935\n",
      "['Let', 'us', 'take', 'take', 'one', 'example', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11300 94%) 0.7130\n",
      "['Now', ',', 'some', 'states', 'are', 'where', 'working', 'learning', 'if', 'sleep', 'if', 'you', 'have', 'a', 'degree', 'in', 'biology', ',', 'if', 'if', 'degree', 'in', '<unk>', ',', 'you', '</s>']\n",
      "(11400 95%) 0.6529\n",
      "['This', 'little', 'baby', 'has', 'a', 'lot', 'of', '<unk>', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11500 95%) 0.5561\n",
      "['When', 'I', 'was', 'nine', ',', 'I', 'saw', 'my', 'General', '&quot;', '<unk>', ',', '&quot;', 'and', 'I', 'to', 'myself', ',', '&quot;', 'No', ',', 'thank', 'you', '.', '&quot;', '</s>']\n",
      "(11600 96%) 0.5681\n",
      "['A', 'new', 'liberal', 'arts', 'that', 'has', 'support', 'this', '<unk>', 'curriculum', 'has', 'begun', 'to', 'emerge', '.', '</s>', '?', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(11700 97%) 0.5855\n",
      "['On', 'again', 'and', 'again', ',', 'the', '<unk>', ',', 'the', 'is', '<unk>', 'was', 'the', 'greatest', '<unk>', 'in', 'the', 'world', ',', 'John', 'describes', 'the', 'evils', 'of', 'the', '</s>']\n",
      "(11800 98%) 0.5998\n",
      "['But', 'were', 'to', 'lot', 'to', 'be', 'there', '.', 'I', 'only', 'have', '13', 'minutes', '.', 'So', ',', 'you', 'of', 'you', 'may', 'also', 'the', 'desperately', '<unk>', 'to', '</s>']\n",
      "(11900 99%) 0.6125\n",
      "['Many', 'of', 'you', 'may', 'probably', 'possible', ',', 'we', 'surely', 'we', 'can', 'still', 'stop', 'it', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(12000 100%) 0.6324\n",
      "['I', 'believed', 'I', 'could', 'keep', 'him', 'up', 'person', ',', 'and', 'I', 'had', 'be', 'embarrassed', 'to', 'say', 'that', 'if', 'I', 'didn', '&apos;t', 'heard', 'it', 'many', 'people', '</s>']\n"
     ]
    }
   ],
   "source": [
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_noatten_lr0.001_step36000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_noatten_lr0.001_step36000_decoder\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
