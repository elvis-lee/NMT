{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt.model import EncoderLSTM, DecoderLSTM, DotAttenDecoderLSTM\n",
    "from nmt.train import train, trainIters\n",
    "from nmt.infer import infer\n",
    "from nmt.eval import eval\n",
    "from nmt.utils import BatchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "train_source = 'train.vi'\n",
    "train_target = 'train.en'\n",
    "train_source_dir = os.path.join(data_dir, train_source)\n",
    "train_target_dir = os.path.join(data_dir, train_target)\n",
    "\n",
    "test_source = 'tst2012.vi'\n",
    "test_target = 'tst2012.en'\n",
    "test_source_dir = os.path.join(data_dir, test_source)\n",
    "test_target_dir = os.path.join(data_dir, test_target)\n",
    "\n",
    "vocab_source = 'vocab.vi'\n",
    "vocab_target = 'vocab.en'\n",
    "vocab_source_dir = os.path.join(data_dir, vocab_source)\n",
    "vocab_target_dir = os.path.join(data_dir, vocab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in source training set: 133317\n",
      "Total number of sentences in target training set: 133317\n",
      "Total number of sentences in source testing set: 1553\n",
      "Total number of sentences in target testing set: 1553\n"
     ]
    }
   ],
   "source": [
    "# load training sets\n",
    "with open(train_source_dir) as f_source:\n",
    "    sentences_source = f_source.readlines()\n",
    "with open(train_target_dir) as f_target:\n",
    "    sentences_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source training set: {}\".format(len(sentences_source)))\n",
    "print(\"Total number of sentences in target training set: {}\".format(len(sentences_target)))\n",
    "\n",
    "# load testing sets\n",
    "with open(test_source_dir) as f_source:\n",
    "    test_source = f_source.readlines()\n",
    "with open(test_target_dir) as f_target:\n",
    "    test_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source testing set: {}\".format(len(test_source)))\n",
    "print(\"Total number of sentences in target testing set: {}\".format(len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the longest sentence in sentences_source: 3199\n",
      "The longest sentence: \n",
      "Thula Mama , Thula Mama , Thula Mama , Thula Mama . Trong kí ức tuổi thơ con , qua những giọt lệ nhoè mắt bà , con thấy chân lý trong nụ cười của bà , con thấy chân lý trong nụ cười của bà , xuyên thấu màn đêm u tối trong sự vô tri của con . Ôi , có một người bà đang nằm nghỉ bà ốm đau và trái tim bà rơi lệ . Băn khoăn , băn khoăn , băn khoăn , băn khoăn liệu thế giới này đang đi về đâu . Lẽ nào chuyện trẻ nhỏ phải tự xoay xở lấy là đúng ? Không , không , không , không , không , không . Lẽ nào phiền muộn dồn hết lên mái đầu người phụ nữ già là đúng ? Những người vô danh bất hạnh . Thula Mama Mama , Thula Mama . Thula Mama Mama . Thula Mama , Thula Mama , Thula Mama Mama , Thula Mama . Ngày mai sẽ tốt đẹp hơn . Ngày mai trèo đèo lội suối sẽ dễ hơn , bà ơi . Thula Mama , Thula Mama . Tôi có nên tan vào bài hát này như người đàn ông hát nhạc blues hay một người hát rong . Và rồi từ rất xa , không phải trong câu lạc bộ nhạc blues nào hết , tôi hát , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi . Bây giờ tôi có nên ngừng hát về tình yêu khi kí ức tôi đã nhuộm đầy máu ? Chị em ơi , ồ tại sao có khi ta lại tưởng lầm mụn nhọt là ung thư ? Thế thì , ai lại đi nói , giờ đây không còn bài thơ tình nào nữa ? Tôi muốn hát một bản tình ca cho người phụ nữ có thai đã dám nhảy qua hàng rào và vẫn sinh ra em bé khoẻ mạnh . Nhẹ nhàng thôi , tôi đi vào tia nắng của nụ cười sẽ đốt bùng lên bản tình ca của tôi , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời . Ooh , tôi chưa từng cố chạy trốn những bài ca , tôi nghe tiếng gọi da diết , mạnh mẽ hơn bom đạn kẻ thù . Bài ca rửa sạch cuộc đời ta và những cơn mưa dòng máu ta . Bài ca của tôi về tình yêu và bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi muốn mọi người cùng hát với tôi nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời -- mọi người cùng hát với tôi đi -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi không nghe thấy tiếng các bạn -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi biết bạn hát to hơn được mà -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- hát nữa , hát nữa nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , vâng , bài ca của tôi về tình yêu -- các bạn hát to hơn được nữa mà -- bài ca của tôi về cuộc đời , chính nó , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- cứ hát đi , hát đi , hát lên đi -- bài ca của tôi về tình yêu . Oh yeah . Bài ca -- một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the longest sentence after sentence truncation\n",
    "max = 0\n",
    "for s in sentences_source:\n",
    "    if len(s) > max:\n",
    "        max = len(s)\n",
    "        max_s = s\n",
    "print(\"Number of words in the longest sentence in sentences_source: {}\".format(max))\n",
    "print(\"The longest sentence: \\n{}\".format(max_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sentences by maximum length\n",
    "sentences_source = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_source))\n",
    "sentences_target = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_target))\n",
    "test_source = list(map(lambda src:src.split()[:MAX_LENGTH], test_source))\n",
    "test_target = list(map(lambda src:src.split()[:MAX_LENGTH], test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133166\n",
      "133166\n",
      "1553\n",
      "1553\n"
     ]
    }
   ],
   "source": [
    "# Delete empty sentences in source and target\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del sentences_source[i]\n",
    "        del sentences_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(sentences_source))\n",
    "print(len(sentences_target))\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del test_source[i]\n",
    "        del test_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(test_source))\n",
    "print(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nummber of words in source vocabulary: 7709\n",
      "Total nummber of words in target vocabulary: 17191\n"
     ]
    }
   ],
   "source": [
    "# load vocabularies\n",
    "\n",
    "# build index2word\n",
    "with open(vocab_source_dir) as f_vocab_source:\n",
    "    #index2word_source = f_vocab_source.readlines()\n",
    "    index2word_source = [line.rstrip() for line in f_vocab_source]\n",
    "with open(vocab_target_dir) as f_vocab_target:\n",
    "    #index2word_target = f_vocab_target.readlines()\n",
    "    index2word_target = [line.rstrip() for line in f_vocab_target]\n",
    "\n",
    "# build word2index\n",
    "word2index_source = {}\n",
    "for idx, word in enumerate(index2word_source):\n",
    "    word2index_source[word] = idx\n",
    "word2index_target = {}\n",
    "for idx, word in enumerate(index2word_target):\n",
    "    word2index_target[word] = idx\n",
    "    \n",
    "# check vocabularies size    \n",
    "source_vocab_size = len(index2word_source)\n",
    "target_vocab_size = len(index2word_target)\n",
    "print(\"Total nummber of words in source vocabulary: {}\".format(len(index2word_source)))\n",
    "print(\"Total nummber of words in target vocabulary: {}\".format(len(index2word_target)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# encoder & decoder init\n",
    "input_size = source_vocab_size\n",
    "output_size = target_vocab_size+1 # +1 is a wordaround for ignore_index field of NLLLoss\n",
    "hidden_size = 512\n",
    "dropout = 0.2\n",
    "attention_vector_size = 256\n",
    "\n",
    "# train & test & infer\n",
    "PAD_token = target_vocab_size # this padding token is ignored for loss calculation\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(step:100 0%) loss_train:6.3025, loss_test:5.5701\n",
      "(step:200 1%) loss_train:5.5158, loss_test:5.2500\n",
      "(step:300 2%) loss_train:5.2158, loss_test:5.1197\n",
      "(step:400 3%) loss_train:5.0678, loss_test:4.7552\n",
      "(step:500 4%) loss_train:4.9536, loss_test:4.6778\n",
      "(step:600 5%) loss_train:4.8649, loss_test:4.5479\n",
      "(step:700 5%) loss_train:4.7914, loss_test:4.4007\n",
      "(step:800 6%) loss_train:4.7088, loss_test:4.6770\n",
      "(step:900 7%) loss_train:4.6630, loss_test:4.5016\n",
      "(step:1000 8%) loss_train:4.5913, loss_test:4.3532\n",
      "epoch: 1\n",
      "bleu_test:0.028601475483475764\n",
      "(step:1100 9%) loss_train:4.5222, loss_test:4.4070\n",
      "(step:1200 10%) loss_train:4.4513, loss_test:4.2249\n",
      "(step:1300 10%) loss_train:4.4209, loss_test:4.1630\n",
      "(step:1400 11%) loss_train:4.3760, loss_test:4.2359\n",
      "(step:1500 12%) loss_train:4.3463, loss_test:4.1040\n",
      "(step:1600 13%) loss_train:4.3002, loss_test:4.0809\n",
      "(step:1700 14%) loss_train:4.2553, loss_test:4.1449\n",
      "(step:1800 15%) loss_train:4.2302, loss_test:4.1167\n",
      "(step:1900 15%) loss_train:4.1846, loss_test:4.0797\n",
      "(step:2000 16%) loss_train:4.1721, loss_test:4.0184\n",
      "epoch: 2\n",
      "bleu_test:0.047921589291844306\n",
      "(step:2100 17%) loss_train:4.1248, loss_test:3.9917\n",
      "(step:2200 18%) loss_train:4.0225, loss_test:3.7917\n",
      "(step:2300 19%) loss_train:4.0076, loss_test:3.8905\n",
      "(step:2400 20%) loss_train:4.0000, loss_test:3.8655\n",
      "(step:2500 20%) loss_train:3.9711, loss_test:3.7774\n",
      "(step:2600 21%) loss_train:3.9585, loss_test:3.8265\n",
      "(step:2700 22%) loss_train:3.9467, loss_test:3.9019\n",
      "(step:2800 23%) loss_train:3.9274, loss_test:3.6887\n",
      "(step:2900 24%) loss_train:3.9056, loss_test:3.7999\n",
      "(step:3000 25%) loss_train:3.8858, loss_test:3.7070\n",
      "(step:3100 25%) loss_train:3.8674, loss_test:3.7624\n",
      "epoch: 3\n",
      "bleu_test:0.06207828016441097\n",
      "(step:3200 26%) loss_train:3.7448, loss_test:3.7126\n",
      "(step:3300 27%) loss_train:3.7266, loss_test:3.7227\n",
      "(step:3400 28%) loss_train:3.7283, loss_test:3.5711\n",
      "(step:3500 29%) loss_train:3.6992, loss_test:3.7659\n",
      "(step:3600 30%) loss_train:3.6973, loss_test:3.7727\n",
      "(step:3700 30%) loss_train:3.6900, loss_test:3.6019\n",
      "(step:3800 31%) loss_train:3.6861, loss_test:3.6695\n",
      "(step:3900 32%) loss_train:3.6794, loss_test:3.6366\n",
      "(step:4000 33%) loss_train:3.6571, loss_test:3.5987\n",
      "(step:4100 34%) loss_train:3.6523, loss_test:3.5638\n",
      "epoch: 4\n",
      "bleu_test:0.07103866908835937\n",
      "(step:4200 35%) loss_train:3.5847, loss_test:3.5886\n",
      "(step:4300 35%) loss_train:3.4805, loss_test:3.4834\n",
      "(step:4400 36%) loss_train:3.5023, loss_test:3.5607\n",
      "(step:4500 37%) loss_train:3.4929, loss_test:3.6098\n",
      "(step:4600 38%) loss_train:3.4854, loss_test:3.5404\n",
      "(step:4700 39%) loss_train:3.4885, loss_test:3.5793\n",
      "(step:4800 40%) loss_train:3.4753, loss_test:3.4980\n",
      "(step:4900 40%) loss_train:3.4827, loss_test:3.5026\n",
      "(step:5000 41%) loss_train:3.4884, loss_test:3.5211\n",
      "(step:5100 42%) loss_train:3.4654, loss_test:3.5307\n",
      "(step:5200 43%) loss_train:3.4773, loss_test:3.5609\n",
      "epoch: 5\n",
      "bleu_test:0.07877203272932866\n",
      "(step:5300 44%) loss_train:3.2927, loss_test:3.4761\n",
      "(step:5400 45%) loss_train:3.3089, loss_test:3.3968\n",
      "(step:5500 45%) loss_train:3.3278, loss_test:3.4333\n",
      "(step:5600 46%) loss_train:3.3171, loss_test:3.5236\n",
      "(step:5700 47%) loss_train:3.3132, loss_test:3.3456\n",
      "(step:5800 48%) loss_train:3.3243, loss_test:3.5460\n",
      "(step:5900 49%) loss_train:3.3136, loss_test:3.4180\n",
      "(step:6000 50%) loss_train:3.3212, loss_test:3.4179\n",
      "(step:6100 50%) loss_train:3.3149, loss_test:3.4205\n",
      "(step:6200 51%) loss_train:3.3179, loss_test:3.4705\n",
      "epoch: 6\n",
      "bleu_test:0.08035998020946049\n",
      "(step:6300 52%) loss_train:3.2023, loss_test:3.3779\n",
      "(step:6400 53%) loss_train:3.1448, loss_test:3.4076\n",
      "(step:6500 54%) loss_train:3.1554, loss_test:3.3127\n",
      "(step:6600 55%) loss_train:3.1641, loss_test:3.2790\n",
      "(step:6700 55%) loss_train:3.1713, loss_test:3.5361\n",
      "(step:6800 56%) loss_train:3.1843, loss_test:3.2845\n",
      "(step:6900 57%) loss_train:3.1897, loss_test:3.4221\n",
      "(step:7000 58%) loss_train:3.1891, loss_test:3.4399\n",
      "(step:7100 59%) loss_train:3.1848, loss_test:3.4367\n",
      "(step:7200 60%) loss_train:3.1914, loss_test:3.3519\n",
      "epoch: 7\n",
      "bleu_test:0.08334179647257703\n",
      "(step:7300 60%) loss_train:3.1504, loss_test:3.3973\n",
      "(step:7400 61%) loss_train:2.9958, loss_test:3.3130\n",
      "(step:7500 62%) loss_train:3.0432, loss_test:3.3675\n",
      "(step:7600 63%) loss_train:3.0368, loss_test:3.3966\n",
      "(step:7700 64%) loss_train:3.0462, loss_test:3.4163\n",
      "(step:7800 65%) loss_train:3.0643, loss_test:3.2078\n",
      "(step:7900 65%) loss_train:3.0685, loss_test:3.5455\n",
      "(step:8000 66%) loss_train:3.0638, loss_test:3.3187\n",
      "(step:8100 67%) loss_train:3.0752, loss_test:3.3941\n",
      "(step:8200 68%) loss_train:3.0827, loss_test:3.4748\n",
      "(step:8300 69%) loss_train:3.0751, loss_test:3.2016\n",
      "epoch: 8\n",
      "bleu_test:0.08467538621237165\n",
      "(step:8400 70%) loss_train:2.9233, loss_test:3.2547\n",
      "(step:8500 70%) loss_train:2.9014, loss_test:3.4825\n",
      "(step:8600 71%) loss_train:2.9233, loss_test:3.2868\n",
      "(step:8700 72%) loss_train:2.9483, loss_test:3.4119\n",
      "(step:8800 73%) loss_train:2.9590, loss_test:3.4341\n",
      "(step:8900 74%) loss_train:2.9636, loss_test:3.4804\n",
      "(step:9000 75%) loss_train:2.9666, loss_test:3.3003\n",
      "(step:9100 75%) loss_train:2.9737, loss_test:3.2414\n",
      "(step:9200 76%) loss_train:2.9692, loss_test:3.2412\n",
      "(step:9300 77%) loss_train:2.9914, loss_test:3.3239\n",
      "epoch: 9\n",
      "bleu_test:0.0885434499965076\n",
      "(step:9400 78%) loss_train:2.9035, loss_test:3.2616\n",
      "(step:9500 79%) loss_train:2.8020, loss_test:3.2679\n",
      "(step:9600 80%) loss_train:2.8316, loss_test:3.3818\n",
      "(step:9700 80%) loss_train:2.8414, loss_test:3.3201\n",
      "(step:9800 81%) loss_train:2.8482, loss_test:3.2127\n",
      "(step:9900 82%) loss_train:2.8740, loss_test:3.4374\n",
      "(step:10000 83%) loss_train:2.8686, loss_test:3.4369\n",
      "(step:10100 84%) loss_train:2.8870, loss_test:3.3314\n",
      "(step:10200 85%) loss_train:2.8929, loss_test:3.2892\n",
      "(step:10300 85%) loss_train:2.9199, loss_test:3.2022\n",
      "(step:10400 86%) loss_train:2.8929, loss_test:3.3625\n",
      "epoch: 10\n",
      "bleu_test:0.08878896274943318\n",
      "(step:10500 87%) loss_train:2.7049, loss_test:3.3754\n",
      "(step:10600 88%) loss_train:2.7277, loss_test:3.3460\n",
      "(step:10700 89%) loss_train:2.7516, loss_test:3.3412\n",
      "(step:10800 90%) loss_train:2.7593, loss_test:3.3339\n",
      "(step:10900 90%) loss_train:2.7857, loss_test:3.3536\n",
      "(step:11000 91%) loss_train:2.7973, loss_test:3.4309\n",
      "(step:11100 92%) loss_train:2.8108, loss_test:3.4749\n",
      "(step:11200 93%) loss_train:2.8175, loss_test:3.3032\n",
      "(step:11300 94%) loss_train:2.8321, loss_test:3.3783\n",
      "(step:11400 95%) loss_train:2.8252, loss_test:3.1901\n",
      "epoch: 11\n",
      "bleu_test:0.09159693090407864\n",
      "(step:11500 95%) loss_train:2.7074, loss_test:3.4059\n",
      "(step:11600 96%) loss_train:2.6524, loss_test:3.1546\n",
      "(step:11700 97%) loss_train:2.6769, loss_test:3.3879\n",
      "(step:11800 98%) loss_train:2.6793, loss_test:3.3310\n",
      "(step:11900 99%) loss_train:2.7062, loss_test:3.3639\n",
      "(step:12000 100%) loss_train:2.7174, loss_test:3.1905\n"
     ]
    }
   ],
   "source": [
    "batch_generator_train = BatchGenerator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target, EOS_token, device)\n",
    "batch_generator_test = BatchGenerator(batch_size, test_source, test_target, word2index_source, word2index_target, EOS_token, device)\n",
    "\n",
    "encoder1 = EncoderLSTM(input_size, hidden_size, dropout=dropout).to(device)\n",
    "decoder1 = DotAttenDecoderLSTM(hidden_size, output_size, attention_vector_size, dropout=dropout).to(device) \n",
    "\n",
    "bleu_params = {}\n",
    "bleu_params['sentences_source'] = test_source\n",
    "bleu_params['sentences_ref'] = test_target\n",
    "bleu_params['max_length'] = MAX_LENGTH\n",
    "bleu_params['word2index_source'] = word2index_source\n",
    "bleu_params['word2index_target'] = word2index_target\n",
    "bleu_params['index2word_target'] = index2word_target\n",
    "bleu_params['EOS_token'] = EOS_token\n",
    "\n",
    "plot_losses_train, plot_losses_test, plot_bleu = trainIters(batch_generator_train, batch_generator_test, encoder1, decoder1, 12000, batch_size, device, SOS_token, PAD_token, print_every=100, step_every_epoch = 1040, learning_rate=0.001, bleu_params=bleu_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
