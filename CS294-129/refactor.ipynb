{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt.model import EncoderLSTM, DecoderLSTM, DotAttenDecoderLSTM\n",
    "from nmt.train import train, trainIters\n",
    "from nmt.infer import infer\n",
    "from nmt.eval import eval\n",
    "from nmt.utils import BatchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "train_source = 'train.vi'\n",
    "train_target = 'train.en'\n",
    "train_source_dir = os.path.join(data_dir, train_source)\n",
    "train_target_dir = os.path.join(data_dir, train_target)\n",
    "\n",
    "test_source = 'tst2012.vi'\n",
    "test_target = 'tst2012.en'\n",
    "test_source_dir = os.path.join(data_dir, test_source)\n",
    "test_target_dir = os.path.join(data_dir, test_target)\n",
    "\n",
    "vocab_source = 'vocab.vi'\n",
    "vocab_target = 'vocab.en'\n",
    "vocab_source_dir = os.path.join(data_dir, vocab_source)\n",
    "vocab_target_dir = os.path.join(data_dir, vocab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in source training set: 133317\n",
      "Total number of sentences in target training set: 133317\n",
      "Total number of sentences in source testing set: 1553\n",
      "Total number of sentences in target testing set: 1553\n"
     ]
    }
   ],
   "source": [
    "# load training sets\n",
    "with open(train_source_dir) as f_source:\n",
    "    sentences_source = f_source.readlines()\n",
    "with open(train_target_dir) as f_target:\n",
    "    sentences_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source training set: {}\".format(len(sentences_source)))\n",
    "print(\"Total number of sentences in target training set: {}\".format(len(sentences_target)))\n",
    "\n",
    "# load testing sets\n",
    "with open(test_source_dir) as f_source:\n",
    "    test_source = f_source.readlines()\n",
    "with open(test_target_dir) as f_target:\n",
    "    test_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source testing set: {}\".format(len(test_source)))\n",
    "print(\"Total number of sentences in target testing set: {}\".format(len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the longest sentence in sentences_source: 3199\n",
      "The longest sentence: \n",
      "Thula Mama , Thula Mama , Thula Mama , Thula Mama . Trong kí ức tuổi thơ con , qua những giọt lệ nhoè mắt bà , con thấy chân lý trong nụ cười của bà , con thấy chân lý trong nụ cười của bà , xuyên thấu màn đêm u tối trong sự vô tri của con . Ôi , có một người bà đang nằm nghỉ bà ốm đau và trái tim bà rơi lệ . Băn khoăn , băn khoăn , băn khoăn , băn khoăn liệu thế giới này đang đi về đâu . Lẽ nào chuyện trẻ nhỏ phải tự xoay xở lấy là đúng ? Không , không , không , không , không , không . Lẽ nào phiền muộn dồn hết lên mái đầu người phụ nữ già là đúng ? Những người vô danh bất hạnh . Thula Mama Mama , Thula Mama . Thula Mama Mama . Thula Mama , Thula Mama , Thula Mama Mama , Thula Mama . Ngày mai sẽ tốt đẹp hơn . Ngày mai trèo đèo lội suối sẽ dễ hơn , bà ơi . Thula Mama , Thula Mama . Tôi có nên tan vào bài hát này như người đàn ông hát nhạc blues hay một người hát rong . Và rồi từ rất xa , không phải trong câu lạc bộ nhạc blues nào hết , tôi hát , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi . Bây giờ tôi có nên ngừng hát về tình yêu khi kí ức tôi đã nhuộm đầy máu ? Chị em ơi , ồ tại sao có khi ta lại tưởng lầm mụn nhọt là ung thư ? Thế thì , ai lại đi nói , giờ đây không còn bài thơ tình nào nữa ? Tôi muốn hát một bản tình ca cho người phụ nữ có thai đã dám nhảy qua hàng rào và vẫn sinh ra em bé khoẻ mạnh . Nhẹ nhàng thôi , tôi đi vào tia nắng của nụ cười sẽ đốt bùng lên bản tình ca của tôi , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời . Ooh , tôi chưa từng cố chạy trốn những bài ca , tôi nghe tiếng gọi da diết , mạnh mẽ hơn bom đạn kẻ thù . Bài ca rửa sạch cuộc đời ta và những cơn mưa dòng máu ta . Bài ca của tôi về tình yêu và bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi muốn mọi người cùng hát với tôi nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời -- mọi người cùng hát với tôi đi -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi không nghe thấy tiếng các bạn -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi biết bạn hát to hơn được mà -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- hát nữa , hát nữa nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , vâng , bài ca của tôi về tình yêu -- các bạn hát to hơn được nữa mà -- bài ca của tôi về cuộc đời , chính nó , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- cứ hát đi , hát đi , hát lên đi -- bài ca của tôi về tình yêu . Oh yeah . Bài ca -- một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the longest sentence after sentence truncation\n",
    "max = 0\n",
    "for s in sentences_source:\n",
    "    if len(s) > max:\n",
    "        max = len(s)\n",
    "        max_s = s\n",
    "print(\"Number of words in the longest sentence in sentences_source: {}\".format(max))\n",
    "print(\"The longest sentence: \\n{}\".format(max_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sentences by maximum length\n",
    "sentences_source = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_source))\n",
    "sentences_target = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_target))\n",
    "test_source = list(map(lambda src:src.split()[:MAX_LENGTH], test_source))\n",
    "test_target = list(map(lambda src:src.split()[:MAX_LENGTH], test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133166\n",
      "133166\n",
      "1553\n",
      "1553\n"
     ]
    }
   ],
   "source": [
    "# Delete empty sentences in source and target\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del sentences_source[i]\n",
    "        del sentences_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(sentences_source))\n",
    "print(len(sentences_target))\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del test_source[i]\n",
    "        del test_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(test_source))\n",
    "print(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nummber of words in source vocabulary: 7709\n",
      "Total nummber of words in target vocabulary: 17191\n"
     ]
    }
   ],
   "source": [
    "# load vocabularies\n",
    "\n",
    "# build index2word\n",
    "with open(vocab_source_dir) as f_vocab_source:\n",
    "    #index2word_source = f_vocab_source.readlines()\n",
    "    index2word_source = [line.rstrip() for line in f_vocab_source]\n",
    "with open(vocab_target_dir) as f_vocab_target:\n",
    "    #index2word_target = f_vocab_target.readlines()\n",
    "    index2word_target = [line.rstrip() for line in f_vocab_target]\n",
    "\n",
    "# build word2index\n",
    "word2index_source = {}\n",
    "for idx, word in enumerate(index2word_source):\n",
    "    word2index_source[word] = idx\n",
    "word2index_target = {}\n",
    "for idx, word in enumerate(index2word_target):\n",
    "    word2index_target[word] = idx\n",
    "    \n",
    "# check vocabularies size    \n",
    "source_vocab_size = len(index2word_source)\n",
    "target_vocab_size = len(index2word_target)\n",
    "print(\"Total nummber of words in source vocabulary: {}\".format(len(index2word_source)))\n",
    "print(\"Total nummber of words in target vocabulary: {}\".format(len(index2word_target)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# encoder & decoder init\n",
    "input_size = source_vocab_size\n",
    "output_size = target_vocab_size+1 # +1 is a wordaround for ignore_index field of NLLLoss\n",
    "hidden_size = 512\n",
    "dropout = 0.2\n",
    "attention_vector_size = 256\n",
    "init_weight = 0.1\n",
    "\n",
    "# train & test & infer\n",
    "PAD_token = target_vocab_size # this padding token is ignored for loss calculation\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad has been used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-069b9354725e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_generator_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2index_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2index_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOS_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdecoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDotAttenDecoderLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_vector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/workspace/NMT/CS294-129/nmt/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, num_layers, num_directions, dropout, forget_bias, init_weight)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh_l0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforget_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih_l0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh_l0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad has been used in an in-place operation."
     ]
    }
   ],
   "source": [
    "batch_generator_train = BatchGenerator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target, EOS_token, device)\n",
    "batch_generator_test = BatchGenerator(batch_size, test_source, test_target, word2index_source, word2index_target, EOS_token, device)\n",
    "\n",
    "encoder1 = EncoderLSTM(input_size, hidden_size, dropout=dropout, init_weight=init_weight).to(device)\n",
    "decoder1 = DotAttenDecoderLSTM(hidden_size, output_size, attention_vector_size, dropout=dropout, init_weight=init_weight).to(device) \n",
    "\n",
    "bleu_params = {}\n",
    "bleu_params['sentences_source'] = test_source\n",
    "bleu_params['sentences_ref'] = test_target\n",
    "bleu_params['max_length'] = MAX_LENGTH\n",
    "bleu_params['word2index_source'] = word2index_source\n",
    "bleu_params['word2index_target'] = word2index_target\n",
    "bleu_params['index2word_target'] = index2word_target\n",
    "bleu_params['EOS_token'] = EOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(encoder1.lstm.weight_ih_l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_train, plot_losses_test, plot_bleu = trainIters(batch_generator_train, batch_generator_test, encoder1, decoder1, 6000, batch_size, device, SOS_token, PAD_token, print_every=1, step_every_epoch = 1000, learning_rate=0.001, bleu_params=bleu_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(plot_losses_train)\n",
    "plt.plot(plot_losses_test)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
