{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmt.model import EncoderLSTM, DecoderLSTM, DotAttenDecoderLSTM\n",
    "from nmt.train import train, trainIters\n",
    "from nmt.infer import infer\n",
    "from nmt.eval import eval\n",
    "from nmt.utils import BatchGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "train_source = 'train.vi'\n",
    "train_target = 'train.en'\n",
    "train_source_dir = os.path.join(data_dir, train_source)\n",
    "train_target_dir = os.path.join(data_dir, train_target)\n",
    "\n",
    "test_source = 'tst2012.vi'\n",
    "test_target = 'tst2012.en'\n",
    "test_source_dir = os.path.join(data_dir, test_source)\n",
    "test_target_dir = os.path.join(data_dir, test_target)\n",
    "\n",
    "vocab_source = 'vocab.vi'\n",
    "vocab_target = 'vocab.en'\n",
    "vocab_source_dir = os.path.join(data_dir, vocab_source)\n",
    "vocab_target_dir = os.path.join(data_dir, vocab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in source training set: 133317\n",
      "Total number of sentences in target training set: 133317\n",
      "Total number of sentences in source testing set: 1553\n",
      "Total number of sentences in target testing set: 1553\n"
     ]
    }
   ],
   "source": [
    "# load training sets\n",
    "with open(train_source_dir) as f_source:\n",
    "    sentences_source = f_source.readlines()\n",
    "with open(train_target_dir) as f_target:\n",
    "    sentences_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source training set: {}\".format(len(sentences_source)))\n",
    "print(\"Total number of sentences in target training set: {}\".format(len(sentences_target)))\n",
    "\n",
    "# load testing sets\n",
    "with open(test_source_dir) as f_source:\n",
    "    test_source = f_source.readlines()\n",
    "with open(test_target_dir) as f_target:\n",
    "    test_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source testing set: {}\".format(len(test_source)))\n",
    "print(\"Total number of sentences in target testing set: {}\".format(len(test_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the longest sentence in sentences_source: 3199\n",
      "The longest sentence: \n",
      "Thula Mama , Thula Mama , Thula Mama , Thula Mama . Trong kí ức tuổi thơ con , qua những giọt lệ nhoè mắt bà , con thấy chân lý trong nụ cười của bà , con thấy chân lý trong nụ cười của bà , xuyên thấu màn đêm u tối trong sự vô tri của con . Ôi , có một người bà đang nằm nghỉ bà ốm đau và trái tim bà rơi lệ . Băn khoăn , băn khoăn , băn khoăn , băn khoăn liệu thế giới này đang đi về đâu . Lẽ nào chuyện trẻ nhỏ phải tự xoay xở lấy là đúng ? Không , không , không , không , không , không . Lẽ nào phiền muộn dồn hết lên mái đầu người phụ nữ già là đúng ? Những người vô danh bất hạnh . Thula Mama Mama , Thula Mama . Thula Mama Mama . Thula Mama , Thula Mama , Thula Mama Mama , Thula Mama . Ngày mai sẽ tốt đẹp hơn . Ngày mai trèo đèo lội suối sẽ dễ hơn , bà ơi . Thula Mama , Thula Mama . Tôi có nên tan vào bài hát này như người đàn ông hát nhạc blues hay một người hát rong . Và rồi từ rất xa , không phải trong câu lạc bộ nhạc blues nào hết , tôi hát , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi . Bây giờ tôi có nên ngừng hát về tình yêu khi kí ức tôi đã nhuộm đầy máu ? Chị em ơi , ồ tại sao có khi ta lại tưởng lầm mụn nhọt là ung thư ? Thế thì , ai lại đi nói , giờ đây không còn bài thơ tình nào nữa ? Tôi muốn hát một bản tình ca cho người phụ nữ có thai đã dám nhảy qua hàng rào và vẫn sinh ra em bé khoẻ mạnh . Nhẹ nhàng thôi , tôi đi vào tia nắng của nụ cười sẽ đốt bùng lên bản tình ca của tôi , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời . Ooh , tôi chưa từng cố chạy trốn những bài ca , tôi nghe tiếng gọi da diết , mạnh mẽ hơn bom đạn kẻ thù . Bài ca rửa sạch cuộc đời ta và những cơn mưa dòng máu ta . Bài ca của tôi về tình yêu và bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi muốn mọi người cùng hát với tôi nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời -- mọi người cùng hát với tôi đi -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi không nghe thấy tiếng các bạn -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi biết bạn hát to hơn được mà -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- hát nữa , hát nữa nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , vâng , bài ca của tôi về tình yêu -- các bạn hát to hơn được nữa mà -- bài ca của tôi về cuộc đời , chính nó , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- cứ hát đi , hát đi , hát lên đi -- bài ca của tôi về tình yêu . Oh yeah . Bài ca -- một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the longest sentence after sentence truncation\n",
    "max = 0\n",
    "for s in sentences_source:\n",
    "    if len(s) > max:\n",
    "        max = len(s)\n",
    "        max_s = s\n",
    "print(\"Number of words in the longest sentence in sentences_source: {}\".format(max))\n",
    "print(\"The longest sentence: \\n{}\".format(max_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sentences by maximum length\n",
    "sentences_source = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_source))\n",
    "sentences_target = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_target))\n",
    "test_source = list(map(lambda src:src.split()[:MAX_LENGTH], test_source))\n",
    "test_target = list(map(lambda src:src.split()[:MAX_LENGTH], test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133166\n",
      "133166\n",
      "1553\n",
      "1553\n"
     ]
    }
   ],
   "source": [
    "# Delete empty sentences in source and target\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del sentences_source[i]\n",
    "        del sentences_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(sentences_source))\n",
    "print(len(sentences_target))\n",
    "\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del test_source[i]\n",
    "        del test_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(test_source))\n",
    "print(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nummber of words in source vocabulary: 7709\n",
      "Total nummber of words in target vocabulary: 17191\n"
     ]
    }
   ],
   "source": [
    "# load vocabularies\n",
    "\n",
    "# build index2word\n",
    "with open(vocab_source_dir) as f_vocab_source:\n",
    "    #index2word_source = f_vocab_source.readlines()\n",
    "    index2word_source = [line.rstrip() for line in f_vocab_source]\n",
    "with open(vocab_target_dir) as f_vocab_target:\n",
    "    #index2word_target = f_vocab_target.readlines()\n",
    "    index2word_target = [line.rstrip() for line in f_vocab_target]\n",
    "\n",
    "# build word2index\n",
    "word2index_source = {}\n",
    "for idx, word in enumerate(index2word_source):\n",
    "    word2index_source[word] = idx\n",
    "word2index_target = {}\n",
    "for idx, word in enumerate(index2word_target):\n",
    "    word2index_target[word] = idx\n",
    "    \n",
    "# check vocabularies size    \n",
    "source_vocab_size = len(index2word_source)\n",
    "target_vocab_size = len(index2word_target)\n",
    "print(\"Total nummber of words in source vocabulary: {}\".format(len(index2word_source)))\n",
    "print(\"Total nummber of words in target vocabulary: {}\".format(len(index2word_target)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# encoder & decoder init\n",
    "input_size = source_vocab_size\n",
    "output_size = target_vocab_size+1 # +1 is a wordaround for ignore_index field of NLLLoss\n",
    "hidden_size = 512\n",
    "dropout = 0.2\n",
    "attention_vector_size = 256\n",
    "\n",
    "# train & test & infer\n",
    "PAD_token = target_vocab_size # this padding token is ignored for loss calculation\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(step:1 0%) loss_train:9.7684, loss_test:9.7280\n",
      "(step:2 0%) loss_train:9.7365, loss_test:9.7340\n",
      "(step:3 0%) loss_train:9.6792, loss_test:9.7489\n",
      "(step:4 0%) loss_train:9.7484, loss_test:9.7070\n",
      "(step:5 0%) loss_train:9.7353, loss_test:9.6756\n",
      "(step:6 0%) loss_train:9.7048, loss_test:9.6608\n",
      "(step:7 0%) loss_train:9.5761, loss_test:9.5357\n",
      "(step:8 0%) loss_train:9.4614, loss_test:9.4030\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9dd206c8d378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mbleu_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EOS_token'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEOS_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_losses_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_losses_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_bleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_generator_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_generator_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOS_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_every_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1040\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbleu_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/workspace/NMT/CS294-129/nmt/train.py\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(batch_generator_train, batch_generator_test, encoder, decoder, n_iters, batch_size, device, SOS_token, PAD_token, print_every, plot_every, step_every_epoch, learning_rate, bleu_params)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0msource_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOS_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/workspace/NMT/CS294-129/nmt/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size, device, SOS_token, PAD_token)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/pytorch/lib/python3.5/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_generator_train = BatchGenerator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target, EOS_token, device)\n",
    "batch_generator_test = BatchGenerator(batch_size, test_source, test_target, word2index_source, word2index_target, EOS_token, device)\n",
    "\n",
    "encoder1 = EncoderLSTM(input_size, hidden_size, dropout=dropout).to(device)\n",
    "decoder1 = DotAttenDecoderLSTM(hidden_size, output_size, attention_vector_size, dropout=dropout).to(device) \n",
    "\n",
    "bleu_params = {}\n",
    "bleu_params['sentences_source'] = test_source\n",
    "bleu_params['sentences_ref'] = test_target\n",
    "bleu_params['max_length'] = MAX_LENGTH\n",
    "bleu_params['word2index_source'] = word2index_source\n",
    "bleu_params['word2index_target'] = word2index_target\n",
    "bleu_params['index2word_target'] = index2word_target\n",
    "bleu_params['EOS_token'] = EOS_token\n",
    "\n",
    "plot_losses_train, plot_losses_test, plot_bleu = trainIters(batch_generator_train, batch_generator_test, encoder1, decoder1, 12000, batch_size, device, SOS_token, PAD_token, print_every=1, step_every_epoch = 1040, learning_rate=0.001, bleu_params=bleu_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
