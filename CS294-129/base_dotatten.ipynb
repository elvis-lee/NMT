{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "MAX_LENGTH = 25\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\") # to be removed\n",
    "print(device)\n",
    "\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "hidden_size = 512\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "attention_vector_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data path\n",
    "data_dir = os.path.join('datasets', 'nmt_data_vi')\n",
    "train_source = 'train.vi'\n",
    "train_target = 'train.en'\n",
    "train_source_dir = os.path.join(data_dir, train_source)\n",
    "train_target_dir = os.path.join(data_dir, train_target)\n",
    "vocab_source = 'vocab.vi'\n",
    "vocab_target = 'vocab.en'\n",
    "vocab_source_dir = os.path.join(data_dir, vocab_source)\n",
    "vocab_target_dir = os.path.join(data_dir, vocab_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences in source training set: 133317\n",
      "Total number of sentences in target training set: 133317\n"
     ]
    }
   ],
   "source": [
    "# load training sets\n",
    "with open(train_source_dir) as f_source:\n",
    "    sentences_source = f_source.readlines()\n",
    "with open(train_target_dir) as f_target:\n",
    "    sentences_target = f_target.readlines()\n",
    "\n",
    "# check the total number of sentencs in training sets    \n",
    "print(\"Total number of sentences in source training set: {}\".format(len(sentences_source)))\n",
    "print(\"Total number of sentences in target training set: {}\".format(len(sentences_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the longest sentence in sentences_source: 3199\n",
      "The longest sentence: \n",
      "Thula Mama , Thula Mama , Thula Mama , Thula Mama . Trong kí ức tuổi thơ con , qua những giọt lệ nhoè mắt bà , con thấy chân lý trong nụ cười của bà , con thấy chân lý trong nụ cười của bà , xuyên thấu màn đêm u tối trong sự vô tri của con . Ôi , có một người bà đang nằm nghỉ bà ốm đau và trái tim bà rơi lệ . Băn khoăn , băn khoăn , băn khoăn , băn khoăn liệu thế giới này đang đi về đâu . Lẽ nào chuyện trẻ nhỏ phải tự xoay xở lấy là đúng ? Không , không , không , không , không , không . Lẽ nào phiền muộn dồn hết lên mái đầu người phụ nữ già là đúng ? Những người vô danh bất hạnh . Thula Mama Mama , Thula Mama . Thula Mama Mama . Thula Mama , Thula Mama , Thula Mama Mama , Thula Mama . Ngày mai sẽ tốt đẹp hơn . Ngày mai trèo đèo lội suối sẽ dễ hơn , bà ơi . Thula Mama , Thula Mama . Tôi có nên tan vào bài hát này như người đàn ông hát nhạc blues hay một người hát rong . Và rồi từ rất xa , không phải trong câu lạc bộ nhạc blues nào hết , tôi hát , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi , bé ơi . Bây giờ tôi có nên ngừng hát về tình yêu khi kí ức tôi đã nhuộm đầy máu ? Chị em ơi , ồ tại sao có khi ta lại tưởng lầm mụn nhọt là ung thư ? Thế thì , ai lại đi nói , giờ đây không còn bài thơ tình nào nữa ? Tôi muốn hát một bản tình ca cho người phụ nữ có thai đã dám nhảy qua hàng rào và vẫn sinh ra em bé khoẻ mạnh . Nhẹ nhàng thôi , tôi đi vào tia nắng của nụ cười sẽ đốt bùng lên bản tình ca của tôi , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời . Ooh , tôi chưa từng cố chạy trốn những bài ca , tôi nghe tiếng gọi da diết , mạnh mẽ hơn bom đạn kẻ thù . Bài ca rửa sạch cuộc đời ta và những cơn mưa dòng máu ta . Bài ca của tôi về tình yêu và bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi muốn mọi người cùng hát với tôi nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời -- mọi người cùng hát với tôi đi -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi không nghe thấy tiếng các bạn -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- tôi biết bạn hát to hơn được mà -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- hát nữa , hát nữa nào -- bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu , vâng , bài ca của tôi về tình yêu -- các bạn hát to hơn được nữa mà -- bài ca của tôi về cuộc đời , chính nó , bài ca của tôi về tình yêu , bài ca của tôi về cuộc đời , bài ca của tôi về tình yêu -- cứ hát đi , hát đi , hát lên đi -- bài ca của tôi về tình yêu . Oh yeah . Bài ca -- một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Hát lên . Một bản tình ca , bài ca của tôi về cuộc đời . Hát nào . Một bản tình ca , bài ca của tôi cuộc đời . Một bản tình ca , bài ca của tôi về cuộc đời . Hát lên .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the longest sentence after sentence truncation\n",
    "max = 0\n",
    "for s in sentences_source:\n",
    "    if len(s) > max:\n",
    "        max = len(s)\n",
    "        max_s = s\n",
    "print(\"Number of words in the longest sentence in sentences_source: {}\".format(max))\n",
    "print(\"The longest sentence: \\n{}\".format(max_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sentences by maximum length\n",
    "sentences_source = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_source))\n",
    "sentences_target = list(map(lambda src:src.split()[:MAX_LENGTH], sentences_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133166\n",
      "133166\n"
     ]
    }
   ],
   "source": [
    "# Delete empty sentences in source and target\n",
    "i = 0\n",
    "while i < len(sentences_source):\n",
    "    if sentences_source[i]==[] or sentences_target[i]==[]:\n",
    "        del sentences_source[i]\n",
    "        del sentences_target[i]\n",
    "        i -= 1\n",
    "    i += 1\n",
    "print(len(sentences_source))\n",
    "print(len(sentences_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nummber of words in source vocabulary: 7709\n",
      "Total nummber of words in target vocabulary: 17191\n"
     ]
    }
   ],
   "source": [
    "# load vocabularies\n",
    "\n",
    "# build index2word\n",
    "with open(vocab_source_dir) as f_vocab_source:\n",
    "    #index2word_source = f_vocab_source.readlines()\n",
    "    index2word_source = [line.rstrip() for line in f_vocab_source]\n",
    "with open(vocab_target_dir) as f_vocab_target:\n",
    "    #index2word_target = f_vocab_target.readlines()\n",
    "    index2word_target = [line.rstrip() for line in f_vocab_target]\n",
    "\n",
    "# build word2index\n",
    "word2index_source = {}\n",
    "for idx, word in enumerate(index2word_source):\n",
    "    word2index_source[word] = idx\n",
    "word2index_target = {}\n",
    "for idx, word in enumerate(index2word_target):\n",
    "    word2index_target[word] = idx\n",
    "    \n",
    "# check vocabularies size    \n",
    "source_vocab_size = len(index2word_source)\n",
    "target_vocab_size = len(index2word_target)\n",
    "print(\"Total nummber of words in source vocabulary: {}\".format(len(index2word_source)))\n",
    "print(\"Total nummber of words in target vocabulary: {}\".format(len(index2word_target)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funtions to convert sentence in natural language to list of word indexes\n",
    "def sen2idx(sentence, word2index):\n",
    "    return [word2index.get(word, 0) for word in sentence] # assume that 0 is for <unk>\n",
    "\n",
    "def sen2tensor(sentence, word2index):\n",
    "    idxes = sen2idx(sentence, word2index)\n",
    "    idxes.append(EOS_token)\n",
    "    return torch.tensor(idxes, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token to be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = target_vocab_size # padding value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator():\n",
    "    def __init__(self, batch_size, sentences_source, sentences_target, word2index_source, word2index_target):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences_source = sentences_source\n",
    "        self.sentences_target = sentences_target\n",
    "        self.word2index_source = word2index_source\n",
    "        self.word2index_target = word2index_target\n",
    "        self.num_sentence = len(sentences_source)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.consumed = 0\n",
    "        self.permutation = np.random.permutation(self.num_sentence)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        # generate id in one batch\n",
    "        if self.consumed + self.batch_size > self.num_sentence:\n",
    "            self.reset()\n",
    "        sample_id = self.permutation[self.consumed:self.consumed + self.batch_size]\n",
    "        self.consumed += self.batch_size\n",
    "\n",
    "        #generate a source batch\n",
    "        sentences_source_tensor = [sen2tensor(self.sentences_source[id], self.word2index_source) for id in sample_id]\n",
    "\n",
    "        len_array_source = [len(st) for st in sentences_source_tensor]\n",
    "        reorder_idx_source = np.argsort(len_array_source, kind='mergesort')\n",
    "        reorder_idx_source = np.argsort(np.flip(reorder_idx_source, 0)) #index to restore unsorted order\n",
    "\n",
    "        sentences_source_tensor.sort(key=len)\n",
    "        sentences_source_tensor.reverse()\n",
    "        sentences_source_packed = pack_sequence(sentences_source_tensor)\n",
    "\n",
    "        #generate a target batch\n",
    "        sentences_target_tensor = [sen2tensor(self.sentences_target[id], self.word2index_target) for id in sample_id]\n",
    "\n",
    "        len_array_target = [len(st) for st in sentences_target_tensor]\n",
    "        reorder_idx_target = np.argsort(len_array_target, kind='mergesort')\n",
    "        reorder_idx_target = np.argsort(np.flip(reorder_idx_target, 0)) #index to restore unsorted order\n",
    "\n",
    "        sentences_target_tensor.sort(key=len)\n",
    "        sentences_target_tensor.reverse()\n",
    "        sentences_target_packed = pack_sequence(sentences_target_tensor)\n",
    "\n",
    "        return (sentences_source_packed, reorder_idx_source), (sentences_target_packed, reorder_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be depreciated\n",
    "\n",
    "# def sentences2tensor(sentences, word2index):\n",
    "#     sentences_tensor = [sen2tensor(s, word2index) for s in sentences]\n",
    "#     sentences_tensor.sort(key=len, reverse=True)\n",
    "#     output = pad_sequence(sentences_tensor, batch_first=True)\n",
    "#     return output\n",
    "\n",
    "def batch_generator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target):\n",
    "    #output: two PackedSequence object, two indexes to reorder sentences in a batch\n",
    "    \n",
    "    # generate id in one batch\n",
    "    total = len(sentences_source)\n",
    "    sample_id = np.random.choice(total, batch_size, replace=False)\n",
    "    \n",
    "    #generate a source batch\n",
    "    sentences_source_tensor = [sen2tensor(sentences_source[id], word2index_source) for id in sample_id]\n",
    "    \n",
    "    len_array_source = [len(st) for st in sentences_source_tensor]\n",
    "    reorder_idx_source = np.argsort(len_array_source, kind='mergesort')\n",
    "    reorder_idx_source = np.argsort(np.flip(reorder_idx_source, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_source_tensor.sort(key=len)\n",
    "    sentences_source_tensor.reverse()\n",
    "    sentences_source_packed = pack_sequence(sentences_source_tensor)\n",
    "    \n",
    "    #generate a target batch\n",
    "    sentences_target_tensor = [sen2tensor(sentences_target[id], word2index_target) for id in sample_id]\n",
    "    \n",
    "    len_array_target = [len(st) for st in sentences_target_tensor]\n",
    "    reorder_idx_target = np.argsort(len_array_target, kind='mergesort')\n",
    "    reorder_idx_target = np.argsort(np.flip(reorder_idx_target, 0)) #index to restore unsorted order\n",
    "    \n",
    "    sentences_target_tensor.sort(key=len)\n",
    "    sentences_target_tensor.reverse()\n",
    "    sentences_target_packed = pack_sequence(sentences_target_tensor)\n",
    "    \n",
    "    return (sentences_source_packed, reorder_idx_source), (sentences_target_packed, reorder_idx_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([   20,     8,  1352,     2,     2], device='cuda:0'), batch_sizes=tensor([ 2,  2,  1]))\n",
      "[1 0]\n",
      "(tensor([[   20,  1352,     2],\n",
      "        [    8,     2,     0]], device='cuda:0'), tensor([ 3,  2]))\n"
     ]
    }
   ],
   "source": [
    "# test batch_generator\n",
    "a = ['I','am','a','boy']\n",
    "b = ['a']\n",
    "c = ['the','goat']\n",
    "\n",
    "sentences_test = [a,b,c]\n",
    "sentences_test2 = [c,b,a]\n",
    "\n",
    "BG_test = BatchGenerator(2, sentences_test, sentences_test2, word2index_target, word2index_target)\n",
    "\n",
    "(output_test, reorder_idx_test), (output_test2, reorder_idx_test2) = BG_test.get_batch()\n",
    "print(output_test)\n",
    "print(reorder_idx_test)\n",
    "print(pad_packed_sequence(output_test, batch_first=True))\n",
    "\n",
    "del a, b, c, sentences_test, sentences_test2, output_test, reorder_idx_test, output_test2, reorder_idx_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to resume order of sentences in a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order recovery is necessary because sentences have to be sorted in descending order of sentence length to be packed as a PackedSequence object. PackedSequence object helps to deal with inputs with variable length in NMT setting. LSTM, RNN can accept PackedSequence objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_order(input, idx):\n",
    "    # input \n",
    "    #   input: Tensor: (batch_size, seq_length)\n",
    "    #   idx: Tensor or ndarray: (batch_size)\n",
    "    # output\n",
    "    #   out: Tensor with reordered sentences in batch: (batch_size, seq_length) \n",
    "    \n",
    "    if isinstance(idx, (np.ndarray)):\n",
    "        idx = torch.from_numpy(idx)\n",
    "        if device == torch.device(\"cuda\"):\n",
    "            idx = idx.cuda()\n",
    "    out = torch.index_select(input, 0, idx)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  8,  9],\n",
      "        [ 4,  5,  6],\n",
      "        [ 1,  2,  3]], device='cuda:0')\n",
      "tensor([[ 7,  8,  9],\n",
      "        [ 4,  5,  6],\n",
      "        [ 1,  2,  3]], device='cuda:0')\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# test resume_order\n",
    "input_test = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],device=device)\n",
    "idx_test = np.array([2,1,0])\n",
    "print(resume_order(input_test, idx_test))\n",
    "input_test = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],device=device)\n",
    "idx_test = torch.tensor([2,1,0], device=device)\n",
    "print(resume_order(input_test, idx_test))\n",
    "print(idx_test.device)\n",
    "del input_test, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Encoder and Decoder classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1, num_directions=1, dropout=0):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "    def forward(self, input_tuple, prev_h, prev_c):\n",
    "        # input\n",
    "        # input size: (batch_size, seq_length)\n",
    "        # prev_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # prec_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # output\n",
    "        # h_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        # c_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "        (sentences_packed, reorder_idx) = input_tuple\n",
    "        sentences_tensor, sentences_length = pad_packed_sequence(sentences_packed, batch_first=True, padding_value=0)\n",
    "        #sentences_tensor: (batch_size, seq_length)\n",
    "        input_embedded = self.embedding(sentences_tensor) # (batch_size, seq_length, hidden_size)\n",
    "        input_embedded_packed = pack_padded_sequence(input_embedded, sentences_length, batch_first=True)\n",
    "        output, (h_n, c_n) = self.lstm(input_embedded_packed, (prev_h, prev_c))\n",
    "        return output, h_n, c_n\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers and num_directions for encoder must be 0 when this decoder is used\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, prev_h, prev_c, encoder_output=None):\n",
    "        input_embedded = self.embedding(input)\n",
    "        h, c = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "        output = self.softmax(self.out(h))\n",
    "        return output, h, c\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttenDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, batch_size):\n",
    "        super(DotAttenDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size*2, attention_vector_size)\n",
    "        self.out2 = nn.Linear(attention_vector_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, prev_h, prev_c, encoder_output):\n",
    "        # encoder_output: PackedSequence to be converted to (batch_size, seq_length, hidden_size*num_directions)\n",
    "        input_embedded = self.embedding(input)\n",
    "        h, c = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "        \n",
    "        # trick: use padding value = -inf to do variable length attention correctly \n",
    "        encoder_output, _ = pad_packed_sequence(encoder_output, batch_first=True, padding_value=0)\n",
    "        #print(encoder_output) # to be removed\n",
    "        scores = torch.matmul(encoder_output, h.unsqueeze(-1)) # (batch_size, seq_length, 1)\n",
    "        scores[scores==0] = -10e10\n",
    "        #print(scores) # to be removed\n",
    "        scores = F.softmax(scores, dim=1)\n",
    "        context_vector = torch.matmul(torch.transpose(encoder_output, 1, 2), scores).squeeze(-1) # (batch_size, hidden_size)\n",
    "        attention_vector = F.tanh(self.out(torch.cat((context_vector, h), -1))) # (batch_size, attention_vector_size)\n",
    "        output = self.softmax(self.out2(attention_vector))\n",
    "        return output, h, c\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This DecoderLSTM can't do teacher forcing in training\n",
    "\n",
    "# class DecoderLSTM(nn.Module):\n",
    "#     def __init__(self, hidden_size, output_size, batch_size, num_layers=1, num_directions=1, dropout=0):\n",
    "#         super(DecoderLSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.num_directions = num_directions\n",
    "#         self.dropout = dropout\n",
    "        \n",
    "#         self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "#         self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "#         self.out = nn.Linear(hidden_size*num_directions, output_size)\n",
    "#         self.softmax = nn.LogSoftmax(dim=2)\n",
    "#     def forward(self, input, prev_h, prev_c):\n",
    "#         # input \n",
    "#         # input size: (batch_size, seq_length)\n",
    "#         # prev_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # prec_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # output\n",
    "#         # h_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # c_n size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "#         # output size: (batch_size, seq_length, output_size)\n",
    "#         input_embedded = self.embedding(input)\n",
    "#         output, (h_n, c_n) = self.lstm(input_embedded, (prev_h, prev_c))\n",
    "#         output =self.softmax(self.out(h))\n",
    "#         return output, h_n, c_n\n",
    "#     def initHidden(self, batch_size):\n",
    "#         return torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer,batch_size, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden_h = encoder.initHidden()\n",
    "    encoder_hidden_c = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_token, size_average=False)\n",
    "    \n",
    "    (sentences_source_packed, reorder_idx_source) = source_tuple\n",
    "    (sentences_target_packed, reorder_idx_target) = target_tuple\n",
    "    sentences_target_tensor, sentences_target_length = pad_packed_sequence(sentences_target_packed, batch_first=True, padding_value=PAD_token)\n",
    "    sentences_target_tensor = resume_order(sentences_target_tensor, reorder_idx_target)\n",
    "\n",
    "    target_length = sentences_target_tensor.size(1)\n",
    "    \n",
    "    # encoder_output size: (batch_size, seq_length, hidden_size*num_directions)\n",
    "    # encoder_hidden_h size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    # encoder_hidden_c size: (num_layers*num_directions, batch_size, hidden_size)\n",
    "    encoder_output, encoder_hidden_h, encoder_hidden_c = encoder(source_tuple, encoder_hidden_h, encoder_hidden_c)\n",
    "    \n",
    "    \n",
    "    decoder_input = torch.full((batch_size,), SOS_token, dtype=torch.long, device=device)\n",
    "    decoder_hidden_c = resume_order(encoder_hidden_c[0], reorder_idx_source)\n",
    "    decoder_hidden_h = resume_order(encoder_hidden_h[0], reorder_idx_source)\n",
    "    \n",
    "    to_print = []\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden_h, decoder_hidden_c = decoder(decoder_input, decoder_hidden_h, decoder_hidden_c, encoder_output)\n",
    "        loss += criterion(decoder_output, sentences_target_tensor[:,di])\n",
    "        decoder_input = sentences_target_tensor[:,di]\n",
    "        \n",
    "        decoder_output_np = np.argmax(decoder_output.detach().cpu().numpy(), 1)[0]\n",
    "        to_print.append(index2word_target[decoder_output_np])\n",
    "    \n",
    "    denominator = torch.sum(sentences_target_length).float()\n",
    "    if device == torch.device(\"cuda\"):\n",
    "        denominator = denominator.cuda()\n",
    "    loss = loss / denominator\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item(), to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(batch_generator, encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.1):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), learning_rate)\n",
    "    \n",
    "    for iter in range(1, n_iters+1):\n",
    "        source_tuple, target_tuple = batch_generator.get_batch()\n",
    "        loss, to_print = train(source_tuple, target_tuple, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter%print_every ==0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(%d %d%%) %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
    "            print(to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100 0%) 6.2771\n",
      "['And', ',', ',', 'the', 'the', ',', '</s>', '.', '</s>', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(200 1%) 5.4307\n",
      "['And', 'we', '<unk>', '<unk>', ',', 'the', '<unk>', ',', 'the', ',', ',', 'a', 'the', '&apos;s', '<unk>', 'to', 'the', ',', '<unk>', '.', '</s>', ',', 'the', ',', '.', '</s>']\n",
      "(300 2%) 5.1344\n",
      "['And', 'I', 'of', 'the', 'of', 'I', '<unk>', ',', 'a', 'a', ',', 'the', '<unk>', ',', 'and', 'the', ',', ',', 'and', '&apos;s', 'a', 'we', '&apos;m', ',', '&quot;', '</s>']\n",
      "(400 3%) 4.9771\n",
      "['And', 'the', ',', 'the', 'the', ',', 'I', 'you', '&apos;m', 'a', 'to', 'be', ',', 'the', 'we', 'is', 'is', 'a', 'the', 'and', '&apos;m', 'a', 'to', '<unk>', ',', '</s>']\n",
      "(500 4%) 4.8653\n",
      "['And', ',', 'I', ',', '<unk>', '<unk>', ',', 'the', ',', 'be', 'a', 'the', ',', ',', 'and', 'I', 'have', 'be', 'to', 'the', '<unk>', 'of', 'the', '.', '</s>', '</s>']\n",
      "(600 5%) 4.7503\n",
      "['It', '&apos;s', '&apos;t', 'a', '<unk>', ',', '.', 'it', '&apos;s', 'a', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(700 5%) 4.6733\n",
      "['And', 'the', 'of', 'the', 'own', ',', ',', 'the', '<unk>', ',', ',', 'to', ',', 'a', ',', 'and', 'we', 'it', 'the', ',', 'we', 'of', 'the', 'are', 'a', '</s>']\n",
      "(800 6%) 4.5503\n",
      "['And', 'I', 'I', 'was', 'to', 'to', 'I', 'years', ',', 'I', 'I', 'was', ',', '&quot;', 'I', ',', 'I', 'me', 'be', 'the', '&quot;', 'can', 'not', 'a', '.', '</s>']\n",
      "(900 7%) 4.4886\n",
      "['We', '<unk>', 'is', ',', ',', 'I', ',', 'I', '&apos;s', 'be', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1000 8%) 4.4181\n",
      "['Thank', 'it', 'it', 'you', '.', 'much', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1100 9%) 4.2844\n",
      "['So', 'what', 'you', 'have', 'the', 'the', 'the', 'percent', 'of', 'the', 'the', 'the', 'percent', '?', '</s>', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n",
      "(1200 10%) 4.2291\n",
      "['We', 'have', 'been', 'a', '<unk>', 'of', 'and', 'the', 'lot', '<unk>', 'of', 'the', 'the', 'world', 'of', 'the', '.', '</s>', ',', 'them', 'them', '.', '.', '.', '.', '</s>']\n",
      "(1300 10%) 4.1732\n",
      "['<unk>', 'didn', '&apos;t', '.', '.', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1400 11%) 4.1368\n",
      "['And', '<unk>', 'we', 'the', ',', '<unk>', ',', 'be', ',', ',', 'the', 'own', '.', '</s>', '.', 'us', 'us', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '.']\n",
      "(1500 12%) 4.0828\n",
      "['The', '&apos;s', 'the', '<unk>', 'more', '<unk>', '<unk>', ',', ',', 'and', 'is', '<unk>', ',', 'of', '<unk>', 'and', 'in', 'the', 'to', 'get', 'the', ',', '<unk>', 'of', 'and', '</s>']\n",
      "(1600 13%) 4.0456\n",
      "['Now', 'the', ',', 'you', 'know', 'a', '<unk>', 'of', 'of', '&apos;s', 'the', 'the', 'to', 'a', 'in', 'much', 'to', 'and', 'to', 'a', '<unk>', 'of', ',', 'and', '<unk>', '</s>']\n",
      "(1700 14%) 4.0290\n",
      "['Now', ',', 'we', '&apos;s', 'look', 'at', 'the', ',', 'a', '<unk>', 'of', '.', '<unk>', '.', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1800 15%) 3.9718\n",
      "['<unk>', '<unk>', 'to', 'not', 'to', '<unk>', '<unk>', '&quot;', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1900 15%) 3.9470\n",
      "['We', '&apos;ve', 'this', 'lot', 'of', 'this', ',', ',', ',', 'and', 'we', '&apos;s', 'very', 'to', 'us', 'to', 'do', 'this', 'be', 'in', 'lot', 'of', 'time', 'minds', ',', '</s>']\n",
      "(2000 16%) 3.9157\n",
      "['And', 'in', 'the', 'of', '<unk>', 'of', 'the', ',', ',', 'the', '<unk>', ',', 'we', 'can', 'use', 'the', '<unk>', 'time', 'of', 'we', 'years', 'of', '.', '</s>', ',', '</s>']\n",
      "(2100 17%) 3.8532\n",
      "['It', '&apos;s', '&apos;t', 'a', 'to', 'me', 'to', 'be', 'me', '<unk>', 'and', ',', 'and', 'I', 'I', 'who', 'were', 'me', ',', 'and', '&apos;s', '&apos;t', 'a', 'same', '<unk>', '</s>']\n",
      "(2200 18%) 3.7065\n",
      "['We', 'have', 'have', 'a', 'kinds', ',', 'the', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2300 19%) 3.6806\n",
      "['And', 'the', 'first', 'we', 'call', 'is', 'is', 'is', '&apos;s', 'called', '<unk>', '.', '.', '</s>', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2400 20%) 3.6688\n",
      "['This', 'is', 'a', 'new', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(2500 20%) 3.6698\n",
      "['A', '<unk>', 'woman', '<unk>', ',', '<unk>', 'in', ',', 'in', 'and', 'was', 'to', 'go', 'the', 'from', 'the', '<unk>', 'of', 'to', 'and', 'the', 'was', 'a', '<unk>', 'the', '</s>']\n",
      "(2600 21%) 3.6470\n",
      "['And', 'I', 'know', 'like', 'to', 'know', 'about', 'you', 'people', 'in', 'in', 'and', 'you', 'you', 'are', 'the', 'who', 'have', 'in', 'in', '</s>', ',', 'in', 'in', 'in', '</s>']\n",
      "(2700 22%) 3.6489\n",
      "['But', 'the', 'the', '<unk>', 'of', 'food', ',', 'don', 'going', 'about', ',', 'and', '&apos;s', 'be', 'more', 'more', 'and', 'more', 'to', 'understand', 'it', 'things', 'and', 'the', 'and', '</s>']\n",
      "(2800 23%) 3.6278\n",
      "['I', '&apos;m', 'tell', 'you', 'friends', 'with', 'you', 'who', ',', '&quot;', 'You', ',', '.', '&quot;', '</s>', ',', ',', '?', '?', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2900 24%) 3.6034\n",
      "['If', 'we', 'have', 'a', 'way', 'system', 'of', 'how', 'future', 'of', 'we', 'need', 'have', 'able', 'to', 'do', 'the', 'the', ',', ',', 'we', 'of', 'the', ',', 'the', '</s>']\n",
      "(3000 25%) 3.6027\n",
      "['<unk>', 'all', 'of', 'these', 'people', ',', 'all', 'all', 'of', 'the', 'different', 'of', 'are', 'all', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3100 25%) 3.5856\n",
      "['If', 'you', 'have', 'in', 'in', 'a', 'of', 'the', 'two', 'that', 'you', '<unk>', 'half', 'of', 'can', 'you', ',', 'there', ',', 'you', 'can', '&apos;t', 'the', '<unk>', '.', '</s>']\n",
      "(3200 26%) 3.3903\n",
      "['We', '&apos;ve', 'the', 'million', 'a', 'and', 'day', 'and', 'we', '&apos;s', 'not', 'a', '<unk>', 'thing', ',', 'We', '&apos;s', 'no', 'money', '.', '</s>', ',', ',', ',', ',', '</s>']\n",
      "(3300 27%) 3.3439\n",
      "['Now', ',', 'in', 'the', 'ideas', ',', ',', 'the', ',', 'we', 'can', 'think', 'that', 'we', '<unk>', 'of', 'this', 'is', 'really', 'of', 'the', 'most', 'that', 'we', 'are', '</s>']\n",
      "(3400 28%) 3.3553\n",
      "['But', 'in', 'this', 'time', 'I', 'I', 'was', 'in', ',', ',', 'I', 'was', 'in', 'a', 'lot', 'famous', 'day', ',', 'the', 'country', ',', 'and', 'in', 'the', ',', '</s>']\n",
      "(3500 29%) 3.3449\n",
      "['You', 'know', ',', 'there', '&apos;s', 'be', 'some', 'few', 'of', 'different', 'and', '<unk>', 'and', 'the', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(3600 30%) 3.3578\n",
      "['It', 'have', 'the', 'in', '.', 'the', 'same', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '.']\n",
      "(3700 30%) 3.3575\n",
      "['What', '&apos;s', 'to', 'space', 'space', '?', 'a', '?', '?', '</s>', '?', 'a', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '</s>']\n",
      "(3800 31%) 3.3476\n",
      "['We', '&apos;re', 'to', 'get', 'the', '<unk>', '<unk>', 'in', 'the', 'times', 'of', '<unk>', '<unk>', '.', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(3900 32%) 3.3603\n",
      "['I', 'started', 'with', 'start', 'this', '<unk>', 'with', 'this', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4000 33%) 3.3444\n",
      "['<unk>', ':', ':', '<unk>', 'the', '<unk>', 'are', '</s>', 'the', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4100 34%) 3.3357\n",
      "['The', 'that', 'this', 'of', 'you', ',', ',', 'you', 'you', 'have', 'to', 'the', 'key', 'of', 'a', 'that', ',', 'some', 'of', 'them', 'is', 'be', 'out', 'the', 'new', '</s>']\n",
      "(4200 35%) 3.2209\n",
      "['Now', ',', 'what', 'we', 'did', 'doing', 'doing', 'do', 'is', 'take', 'we', 'D', 'is', '<unk>', 'out', '<unk>', '<unk>', ',', ',', 'then', 'asked', 'to', '<unk>', 'the', '<unk>', '</s>']\n",
      "(4300 35%) 3.0771\n",
      "['And', 'we', '&apos;ve', 'the', 'at', 'two', 'second', 'of', 'reasons', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(4400 36%) 3.0891\n",
      "['And', 'you', 'can', 'learn', 'that', ',', 'and', 'it', '&apos;s', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500 37%) 3.0872\n",
      "['He', 'said', ',', '&quot;', 'know', ',', 'the', '&apos;s', 'this', '<unk>', 'thing', ',', 'up', 'it', '&apos;s', 'a', 'to', 'be', 'up', 'the', 'bottom', 'of', 'the', ',', '</s>', '</s>']\n",
      "(4600 38%) 3.1024\n",
      "['<unk>', 'very', '<unk>', 'thing', '.', 'I', 'want', '&apos;t', 'want', 'whether', 'I', 'want', 'a', 'talk', 'to', 'you', 'some', 'say', 'lot', 'people', 'people', ',', 'the', ',', 'but', '</s>']\n",
      "(4700 39%) 3.1157\n",
      "['We', 'are', 'the', 'first', '<unk>', 'the', 'world', 'where', 'see', 'the', 'from', 'the', '<unk>', '<unk>', 'where', 'they', 'were', '<unk>', 'how', '</s>', ',', '--', '--', ',', ',', '</s>']\n",
      "(4800 40%) 3.1327\n",
      "['And', 'I', 'think', 'we', 'we', 'can', 'find', 'we', 'we', 'can', 'have', 'able', 'to', 'have', 'more', 'of', 'the', '<unk>', 'and', 'and', 'of', 'the', 'brain', ',', 'we', '</s>']\n",
      "(4900 40%) 3.1206\n",
      "['In', ',', 'the', ',', 'the', 'the', 'as', 'the', '<unk>', 'has', 'have', 'really', 'very', 'to', 'get', 'a', 'different', 'look', 'like', 'the', ',', 'a', ',', 'and', 'that', '</s>']\n",
      "(5000 41%) 3.1371\n",
      "['To', 'that', ',', 'the', 'the', 'the', 'the', 'people', ',', 'the', '<unk>', 'of', 'in', ',', 'the', ',', 'the', '.', '<unk>', '.', '</s>', ',', ',', '</s>', '</s>', '</s>']\n",
      "(5100 42%) 3.1185\n",
      "['And', 'the', '<unk>', '<unk>', 'is', 'of', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5200 43%) 3.1265\n",
      "['The', 'societal', 'difference', 'in', 'the', 'society', 'of', 'the', 'in', '<unk>', '<unk>', 'care', 'the', 'United', 'countries', 'country', 'is', 'the', 'many', 'the', 'women', 'is', 'the', 'media', '.', '</s>']\n",
      "(5300 44%) 2.8320\n",
      "['So', 'when', 'we', 'comes', 'to', 'to', 'the', 'own', ',', ',', 'we', 'have', 'a', 'billion', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(5400 45%) 2.8478\n",
      "['However', ',', 'if', '&apos;s', 'important', 'to', 'be', 'the', 'in', 'the', 'U.S.', 'States', '.', '</s>', ',', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(5500 45%) 2.8836\n",
      "['And', 'other', 'you', 'can', 'is', 'be', 'very', 'complicated', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(5600 46%) 2.8853\n",
      "['But', 'yet', ',', '&apos;s', 'just', '<unk>', '<unk>', 'information', 'in', 'we', '&apos;ve', 'in', '</s>', '</s>', 'here', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5700 47%) 2.8982\n",
      "['But', 'there', 'are', 'have', 'science', 'ideas', 'to', 'science', '.', 'science', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5800 48%) 2.9072\n",
      "['This', '&apos;s', 'called', '<unk>', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5900 49%) 2.9317\n",
      "['And', 'they', '<unk>', 'away', 'the', 'home', ',', 'the', ',', 'they', 'were', 'their', 'be', 'us', 'own', '.', '</s>', '.', 'them', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6000 50%) 2.9505\n",
      "['In', 'in', 'truth', 'is', 'the', 'is', 'that', 'the', 'fact', 'of', 'information', 'that', 'you', 'have', 'have', 'think', 'about', 'should', 'in', 'a', 'public', 'system', '.', 'not', 'the', '</s>']\n",
      "(6100 50%) 2.9322\n",
      "['Now', ',', 'the', 'two', '<unk>', ',', 'the', ',', 'the', 'the', ',', 'the', ',', 'the', 'pictures', '<unk>', ',', ',', 'and', 'much', 'it', 'that', 'you', 'were', '.', '</s>']\n",
      "(6200 51%) 2.9453\n",
      "['There', 'is', 'in', ',', 'which', '<unk>', '<unk>', ',', 'which', 'the', '<unk>', 'of', 'the', 'to', 'the', 'cell', 'to', 'the', 'cell', '.', 'the', '<unk>', ',', '.', '</s>', '</s>']\n",
      "(6300 52%) 2.7509\n",
      "['It', 'it', 'into', 'to', 'me', 'friend', ',', 'and', 'my', 'five', 'my', 'minutes', 'of', 'had', 'a', '<unk>', '<unk>', 'industry', 'of', 'of', 'the', '<unk>', '<unk>', 'hospital', '.', '</s>']\n",
      "(6400 53%) 2.6434\n",
      "['I', '&apos;m', 'talking', 'about', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(6500 54%) 2.6785\n",
      "['There', '&apos;s', 'two', 'two', '<unk>', ':', '<unk>', '<unk>', 'and', '<unk>', '<unk>', '.', '</s>', ',', ',', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6600 55%) 2.6926\n",
      "['What', '&apos;s', 'interesting', 'cool', '.', 'And', 'then', 'there', '<unk>', 'is', 'onto', '.', '</s>', ',', ',', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6700 55%) 2.7133\n",
      "['We', 'need', 'have', 'three', 'billion', 'people', 'in', 'the', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(6800 56%) 2.7345\n",
      "['We', '&apos;re', 'up', 'as', 'the', 'same', 'and', ',', 'feel', 'like', 'need', 'our', '.', '</s>', ',', 'us', ',', ',', '.', '.', '</s>', '</s>', '</s>', '</s>', '.', '.']\n",
      "(6900 57%) 2.7556\n",
      "['They', '&apos;re', 'the', '<unk>', 'of', 'the', 'roof', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>']\n",
      "(7000 58%) 2.7527\n",
      "['And', 'the', 'important', 'question', 'is', ',', 'is', 'a', 'social', 'social', 'really', 'matter', '?', 'What', 'it', 'matter', '?', 'it', '&apos;s', 'a', 'more', '?', '</s>', ',', 'a', '</s>']\n",
      "(7100 59%) 2.7763\n",
      "['So', 'if', 'you', '&apos;re', 'to', 'the', 'sound', ',', 'can', 'listen', 'hear', 'all', '&apos;s', 'these', 'sounds', 'like', ',', '.', '</s>', '</s>', '</s>', '.', '</s>', '.', '.', '.']\n",
      "(7200 60%) 2.7761\n",
      "['Sylvia', ',', 'Sylvia', 'Browne', 'is', 'a', 'one', 'one', '.', '</s>', 'here', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7300 60%) 2.7100\n",
      "['They', 'these', 'these', 'answered', 'my', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7400 61%) 2.4536\n",
      "['Have', 'you', 'ever', 'ever', 'a', '<unk>', 'to', '?', '?', 'time', '?', '</s>', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n",
      "(7500 62%) 2.5004\n",
      "['So', 'the', 'approach', 'we', 'that', 'generation', 'is', 'to', 'create', 'the', 'movement', 'and', 'are', 'the', 'lives', 'and', 'life', 'and', 'innovation', ',', 'and', 'the', 'same', 'of', 'we', '</s>']\n",
      "(7600 63%) 2.5259\n",
      "['And', 'this', 'coral', 'came', 'to', 'a', 'White', 'House', 'to', 'a', '<unk>', 'conference', '.', '</s>', ',', 'a', ',', ',', ',', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7700 64%) 2.5435\n",
      "['The', 'best', 'principle', 'is', 'is', 'is', 'when', 'you', 'is', 'to', 'the', 'it', '&apos;s', 'to', 'a', 'single', 'of', '100', '.', '</s>', '.', 'a', '.', '.', '.', '</s>']\n",
      "(7800 65%) 2.5670\n",
      "['So', 'how', 'do', 'we', 'do', 'it', 'work', 'work', '?', '</s>', ',', 'it', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n",
      "(7900 65%) 2.5747\n",
      "['What', 'do', 'you', 'do', '?', 'And', 'course', 'you', 'can', '.', 'the', '<unk>', '.', '</s>', ',', ',', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(8000 66%) 2.5869\n",
      "['As', 'since', 'we', 'live', 'in', 'a', ',', 'we', 'day', 'we', 'live', 'to', 'energy', ',', 'we', 'would', 'it', 'that', 'the', ',', 'and', 'we', 'we', 'the', 'the', '</s>']\n",
      "(8100 67%) 2.6080\n",
      "['That', '&apos;s', 'even', 'more', 'important', 'for', 'we', 'are', 'to', 'the', ',', 'we', 'not', 'to', 'be', 'their', 'humanity', 'humanity', '.', 'our', 'like', 'this', '<unk>', 'of', 'our', '</s>']\n",
      "(8200 68%) 2.6197\n",
      "['In', 'the', 'year', '20', 'years', ',', 'it', '&apos;s', 'changing', 'changed', 'the', 'way', 'we', 'we', 'work', ',', 'work', ',', 'and', 'that', 'way', 'that', 'we', 'do', 'it', '</s>']\n",
      "(8300 69%) 2.6255\n",
      "['In', 'the', 'last', 'few', 'years', ',', 'I', 'don', '&apos;t', 'had', 'a', 'much', 'more', 'to', 'think', 'think', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8400 70%) 2.3632\n",
      "['If', 'we', 'were', 'on', 'a', '12', 'to', 'hold', 'the', ',', 'I', 'would', 'not', 'that', '&apos;re', 'never', 'know', 'there', '.', '</s>', ',', ',', ',', '</s>', '</s>', '</s>']\n",
      "(8500 70%) 2.3229\n",
      "['So', 'imagine', 'imagine', 'if', 'you', 'can', 'go', 'and', 'and', 'do', 'at', 'a', 'company', 'owner', 'of', 'the', 'company', '.', '</s>', '.', ',', ',', ',', '.', '.', '.']\n",
      "(8600 71%) 2.3453\n",
      "['How', 'should', 'I', 'do', 'with', '</s>', '</s>', ',', ',', '.', '.', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n",
      "(8700 72%) 2.3768\n",
      "['And', 'we', 'were', 'very', 'when', 'they', 'decided', 'a', 'chance', 'to', 'a', 'new', 'of', '<unk>', '<unk>', 'investigators', 'and', 'decided', 'them', 'to', 'see', 'the', 'people', 'in', 'guns', '</s>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8800 73%) 2.3950\n",
      "['I', '&apos;s', 'that', 'me', 'that', 'we', '&apos;re', 'all', 'wrong', 'out', 'the', 'wrong', 'direction', '.', '</s>', 'important', '<unk>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(8900 74%) 2.4184\n",
      "['Can', 'you', 'have', 'a', 'sense', 'of', '<unk>', 'mistakes', 'from', 'it', '?', '</s>', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n",
      "(9000 75%) 2.4539\n",
      "['I', 'feel', 'like', 'bit', 'like', 'I', 'talk', 'I', 'I', 'this', 'because', 'And', ',', 'and', 'they', 'forth', 'people', 'were', 'looking', 'back', '.', 'a', 'feet', '.', '.', '</s>']\n",
      "(9100 75%) 2.4501\n",
      "['The', 'by', 'fact', 'way', ',', 'they', '&apos;re', 'true', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(9200 76%) 2.4852\n",
      "['The', 'result', 'conclusion', 'of', 'the', 'mission', 'would', 'have', 'in', 'the', 'the', 'of', '<unk>', '<unk>', 'by', 'social', 'million', 'social', 'of', '<unk>', 'scientists', 'and', '<unk>', '.', '</s>', '</s>']\n",
      "(9300 77%) 2.4769\n",
      "['And', 'this', 'whole', 'has', 'a', 'largest', 'data', '<unk>', 'data', 'from', 'from', 'the', '<unk>', '.', '.', '</s>', '</s>', ',', ',', ',', '.', '.', '.', '.', '.', '.']\n",
      "(9400 78%) 2.3475\n",
      "['And', 'actually', 'what', 'something', 'she', 'called', '&quot;', '<unk>', '<unk>', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(9500 79%) 2.1624\n",
      "['I', 'want', ',', ',', '<unk>', ',', '<unk>', 'are', 'being', ',', 'of', 'too', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(9600 80%) 2.1926\n",
      "['Melinda', ',', 'when', 'on', ',', 'this', '&apos;re', 'done', 'doing', 'working', '.', 'work', '.', '</s>', 'here', ',', ',', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(9700 80%) 2.2278\n",
      "['I', '&apos;m', 'not', 'interested', 'in', 'the', 'the', '<unk>', ',', 'the', 'competition', '.', '</s>', '.', ',', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(9800 81%) 2.2618\n",
      "['All', 'right', ',', 'So', 'what', 'you', 'see', 'seeing', 'of', 'here', 'here', 'is', 'a', '<unk>', 'of', 'the', 'technology', ',', 'which', 'course', ',', 'technological', 'and', 'Internet', '.', '</s>']\n",
      "(9900 82%) 2.2891\n",
      "['No', ',', 'it', 'a', 'that', 'on', 'hard', ',', 'it', 'device', ',', 'but', 'it', '&apos;s', 'not', 'have', 'that', '&apos;s', 'something', 'a', 'that', 'on', ',', 'the', 'end', '</s>']\n",
      "(10000 83%) 2.2920\n",
      "['I', 'I', 'took', 'upstairs', ',', 'I', 'a', 'my', 'home', '.', '</s>', '.', ',', '.', ',', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(10100 84%) 2.3123\n",
      "['I', 'felt', 'like', 'I', 'was', 'a', 'a', 'experiment', 'experiment', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(10200 85%) 2.3389\n",
      "['This', 'is', 'the', 'Earth', '.', '</s>', 'here', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10300 85%) 2.3443\n",
      "['Similarly', ',', 'in', ',', 'in', 'the', '&apos;s', 'society', ',', 'are', 'so', 'who', 'are', 'the', 'species', ',', 'and', 'in', 'the', 'way', 'culture', '.', 'and', 'they', 'are', '</s>']\n",
      "(10400 86%) 2.3516\n",
      "['Over', 'over', '100', 'years', ',', 'researchers', 'and', 'the', ',', 'were', 'the', 'same', 'that', 'the', 'all', 'development', 'in', 'the', ',', 'we', 'matter', '<unk>', 'has', 'were', 'be', '</s>']\n",
      "(10500 87%) 2.0235\n",
      "['Watson', '&apos;s', 'still', 'quite', '.', '</s>', 'here', ',', ',', ',', ',', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(10600 88%) 2.0465\n",
      "['The', 'only', 'reason', 'that', '<unk>', 'buy', 'is', 'is', 'buy', 'is', 'because', 'they', 'buy', 'buy', 'buy', 'money', '<unk>', '.', '.', '</s>', ',', '<unk>', ',', ',', ',', '</s>']\n",
      "(10700 89%) 2.0911\n",
      "['You', 'ask', ',', ',', 'and', 'you', 'said', 'say', ',', '&quot;', 'Well', ',', 'you', 'is', 'a', 'worst', 'mistake', 'thing', 'of', 'force', 'that', '.', '&quot;', '</s>', '.', '.']\n",
      "(10800 90%) 2.1204\n",
      "['And', 'so', '&apos;s', 'our', 'in', 'the', 'national', 'of', 'of', 'the', 'studies', 'of', 'democracy', '.', '</s>', 'in', 'in', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(10900 90%) 2.1365\n",
      "['After', 'was', 'when', 'I', 'wrote', 'my', 'school', ',', 'and', 'I', 'don', '&apos;t', 'really', 'in', 'the', 'York', 'City', 'the', 'of', '<unk>', 'because', '<unk>', ',', 'I', 'think', '</s>']\n",
      "(11000 91%) 2.1575\n",
      "['And', 'when', 'when', 'when', 'I', 'knew', 'that', 'I', 'people', 'what', 'I', 'decided', 'that', 'the', 'project', 'changed', 'the', 'has', 'to', 'the', 'the', 'the', 'culture', 'year', '.', '</s>']\n",
      "(11100 92%) 2.1756\n",
      "['And', 'up', ',', 'I', 'was', 'always', 'asking', 'by', 'the', 'people', 'changed', '.', '</s>', ',', 'me', 'me', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(11200 93%) 2.2019\n",
      "['But', 'many', 'are', 'many', 'other', 'of', 'this', 'life', ',', 'is', 'not', 'away', '.', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11300 94%) 2.2124\n",
      "['All', 'I', 'know', 'is', 'how', 'bad', 'felt', 'to', 'be', 'a', 'one', 'in', 'a', '.', 'it', '.', '</s>', ',', ',', '.', '</s>', '.', '</s>', '.', '.', '.']\n",
      "(11400 95%) 2.2327\n",
      "['Women', 'have', 'to', 'be', 'the', '<unk>', 'surgeries', 'to', 'get', 'the', 'the', 'the', 'most', 'lymph', 'nodes', '.', '</s>', ',', ',', '.', ',', ',', '.', ',', ',', '</s>']\n",
      "(11500 95%) 2.0311\n",
      "['It', 'changes', 'the', 'time', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(11600 96%) 1.9234\n",
      "['We', 'have', 'the', '<unk>', 'porridge', 'pot', 'in', 'the', '<unk>', 'of', '<unk>', 'and', '<unk>', '.', '</s>', '.', '<unk>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(11700 97%) 1.9411\n",
      "['Come', 'here', '.', 'Come', 'here', '.', 'Come', 'here', '.', '</s>', 'here', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(11800 98%) 1.9797\n",
      "['It', '&apos;s', 'called', 'the', '<unk>', 'the', '<unk>', 'adage', 'that', 'I', 'read', 'out', 'the', '<unk>', ',', '&quot;', 'If', 'it', 'is', '<unk>', '.', '&quot;', '</s>', ',', ',', '</s>']\n",
      "(11900 99%) 2.0118\n",
      "['<unk>', ':', 'And', 'it', '&apos;s', 'on', 'to', 'show', 'you', 'all', 'who', 'things', 'like', 'of', 'cardboard', ',', 'a', '<unk>', 'for', 'a', '<unk>', 'like', 'the', 'basement', ',', '</s>']\n",
      "(12000 100%) 2.0373\n",
      "['Would', 'you', 'of', 'you', 'who', 'what', 'you', 'missed', 'to', 'be', 'in', 'you', 'were', 'a', '?', '</s>', ',', 'a', ',', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '?']\n"
     ]
    }
   ],
   "source": [
    "batch_generator1 = BatchGenerator(batch_size, sentences_source, sentences_target, word2index_source, word2index_target)\n",
    "encoder1 = EncoderLSTM(source_vocab_size, hidden_size, batch_size).to(device)\n",
    "decoder1 = DotAttenDecoderLSTM(hidden_size, target_vocab_size+1, batch_size).to(device) # +1 is a wordaround for ignore_index field of NLLLoss\n",
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step12000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step12000_decoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100 0%) 2.1021\n",
      "['Why', 'are', 'they', 'sitting', 'down', 'the', '?', '</s>', ',', '.', ',', ',', ',', ',', '?', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(200 1%) 2.1175\n",
      "['This', '<unk>', '<unk>', ',', ',', 'a', '<unk>', '<unk>', 'system', 'that', 'uses', '<unk>', 'to', 'create', 'and', 'generate', 'water', 'in', 'a', 'to', '</s>', ',', '<unk>', ',', ',', '</s>']\n",
      "(300 2%) 2.1242\n",
      "['<unk>', '<unk>', '<unk>', ':', 'The', '<unk>', 'against', 'fight', 'the', '</s>', 'of', 'the', '</s>', '</s>', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(400 3%) 2.1384\n",
      "['The', 'argument', 'to', 'the', 'book', ',', 'and', 'it', 'reason', 'I', 'use', 'it', 'book', 'it', 'book', ',', 'is', 'it', 'it', '&apos;s', 'least', 'has', 'to', 'the', 'problem', '</s>']\n",
      "(500 4%) 2.0753\n",
      "['Now', 'of', 'because', 'I', '&apos;m', '&apos;t', 'do', 'it', 'of', 'this', ',', 'because', 'one', '<unk>', 'years', 'in', 'the', ':', 'The', 'average', 'highest', '<unk>', '<unk>', 'is', 'cheap', '</s>']\n",
      "(600 5%) 1.8083\n",
      "['Other', 'researchers', 'could', 'want', 'to', 'fly', 'the', 'drone', 'with', 'with', 'plane', 'of', 'a', '<unk>', 'to', 'fly', 'and', 'to', 'the', 'or', 'even', 'who', 'are', 'be', '</s>', '</s>']\n",
      "(700 5%) 1.8488\n",
      "['You', 'don', '&apos;t', 'know', 'know', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(800 6%) 1.8806\n",
      "['The', 'cab', 'Donald', 'Brown', 'has', 'tried', 'to', 'go', 'them', 'all', 'the', 'and', 'they', 'use', 'from', 'the', ',', 'and', 'and', '<unk>', '<unk>', ',', 'the', 'way', 'down', '</s>']\n",
      "(900 7%) 1.9125\n",
      "['We', '&apos;re', 'going', 'to', 'use', 'it', 'in', 'and', 'around', 'the', '.', '</s>', 'we', 'people', 'in', 'in', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1000 8%) 1.9329\n",
      "['Thomas', '<unk>', ':', 'The', 'the', '<unk>', '</s>', '</s>', '</s>', '</s>', '<unk>', '<unk>', '<unk>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1100 9%) 1.9566\n",
      "['These', 'is', 'the', '<unk>', 'that', 'the', 'light', '.', 'and', 'open', '.', 'door', '.', '</s>', 'here', ',', '.', ',', ',', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1200 10%) 1.9763\n",
      "['I', '&apos;m', 'not', 'going', 'the', '<unk>', '&apos;', 'building', 'in', 'this', '.', '.', '</s>', 'here', 'County', ',', 'in', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1300 10%) 1.9851\n",
      "['They', 'only', 'thought', 'that', 'say', 'going', 'is', 'that', 'we', 'takes', 'a', 'big', 'price', 'that', 'get', 'us', 'a', '.', '</s>', 'we', 'it', 'in', '.', '.', '.', '.']\n",
      "(1400 11%) 2.0092\n",
      "['Now', ',', 'in', 'the', 'summer', ',', 'there', 'were', 'three', 'kinds', 'amazing', 'industrial', 'of', 'in', 'industrial', 'new', 'design', 'was', 'made', 'from', 'a', 'ground', 'and', 'the', 'the', '</s>']\n",
      "(1500 12%) 2.0277\n",
      "['<unk>', 'technology', 'of', 'part', 'of', 'this', 'technology', 'is', '&apos;t', 'the', 'to', 'the', 'United', 'States', '.', '</s>', 'here', '.', 'in', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1600 13%) 1.7659\n",
      "['Do', '&apos;ve', 'this', '<unk>', '<unk>', 'for', 'you', 'say', ',', '&quot;', 'Hey', ',', 'this', '&apos;s', 'Mena', '<unk>', '?', '&quot;', '</s>', ',', '</s>', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(1700 14%) 1.7184\n",
      "['But', 'we', 'we', 'should', 'change', 'the', 'question', '.', '</s>', 'important', ',', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1800 15%) 1.7567\n",
      "['He', 'said', ',', '&quot;', 'I', '&apos;d', 'got', 'some', 'good', 'news', '.', '</s>', '.', ',', '.', '.', '.', '.', '.', '.', '.', '</s>', '.', '</s>', '</s>', '</s>']\n",
      "(1900 15%) 1.7878\n",
      "['In', 'this', 'cell', ',', ',', 'you', 'only', 'noticed', 'a', 'of', 'the', 'most', 'fundamental', 'basic', 'that', 'body', 'has', 'its', 'the', '<unk>', '.', '</s>', ',', 'a', ',', '</s>']\n",
      "(2000 16%) 1.8182\n",
      "['So', 'go', 'feet', ',', 'except', 'for', 'the', ',', 'radio', ',', 'radio', ',', '<unk>', '.', '</s>', '</s>', ',', ',', 'water', 'water', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2100 17%) 1.8318\n",
      "['But', 'the', 'are', 'blue', 'flowers', 'green', 'the', 'spaces', '.', '</s>', '.', 'water', '.', 'land', ',', '<unk>', '<unk>', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(2200 18%) 1.8626\n",
      "['But', '&apos;s', 'actually', 'not', 'very', 'complicated', '.', '</s>', '</s>', 'a', '.', '.', ',', ',', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2300 19%) 1.8862\n",
      "['It', 'is', 'a', 'different', 'different', 'from', 'all', 'people', 'channels', '.', '</s>', 'other', ',', 'it', ',', ',', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2400 20%) 1.9151\n",
      "['It', '!', 'up', '.', '</s>', 'there', '.', '.', ',', ',', ',', '.', '.', '.', '.', '.', '<unk>', '<unk>', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2500 20%) 1.9161\n",
      "['The', 'international', 'have', 'post-conflict', 'aid', '.', '</s>', 'I', '<unk>', '.', ',', ',', ',', ',', ',', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2600 21%) 1.7835\n",
      "['Here', '&apos;s', 'three', 'visions', 'of', 'a', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', ',', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2700 22%) 1.6079\n",
      "['At', 'the', 'the', 'hero', 'rats', 'have', 'been', 'thousands', 'of', 'lives', '.', '</s>', ',', '<unk>', ',', ',', ',', '</s>', '</s>', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2800 23%) 1.6523\n",
      "['You', 'have', 'a', 'of', 'a', 'volcano', '.', 'There', 'are', 'some', 'volcano', 'jam', 'of', 'a', 'volcano', 'there', '.', 'You', 'have', 'a', 'worms', '.', '</s>', '.', ',', '.']\n",
      "(2900 24%) 1.6791\n",
      "['<unk>', 'emerged', 'from', 'a', 'media', ',', 'unemployment', ',', 'and', 'the', '<unk>', 'of', 'the', 'waste', 'from', 'the', 'system', 'system', 'the', 'system', 'collection', 'system', '.', '</s>', ',', '</s>']\n",
      "(3000 25%) 1.7073\n",
      "['What', '&apos;s', 'different', 'about', 'it', '?', '&quot;', '</s>', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3100 25%) 1.7418\n",
      "['We', '&apos;ve', 'this', 'with', 'science', 'technologists', 'from', 'India', 'and', 'Pakistan', '--', 'the', 'about', 'the', 'of', 'the', '.', '</s>', 'important', 'a', ',', ',', ',', ',', ',', '</s>']\n",
      "(3200 26%) 1.7599\n",
      "['That', '&apos;s', 'what', 'I', 'want', 'like', 'to', 'do', '.', '</s>', '</s>', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3300 27%) 1.7805\n",
      "['It', 'represents', 'of', 'the', 'products', '&apos;s', 'your', 'brand', 'company', 'to', 'the', 'rest', ',', 'and', '<unk>', 'the', 'money', 'money', 'that', '</s>', ',', 'them', ',', ',', 'plastic', '</s>']\n",
      "(3400 28%) 1.7976\n",
      "['So', 'this', 'is', 'the', '<unk>', 'Opera', 'theater', '.', 'We', 'did', 'it', 'in', 'two', 'months', '.', '</s>', ',', ',', ',', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3500 29%) 1.8150\n",
      "['It', '&apos;s', 'a', 'wonderful', 'new', 'poem', 'to', 'you', 'life', '.', 'the', '&apos;s', 'wonderful', '.', 'be', 'the', 'gift', '.', '</s>', 'you', 'you', 'you', 'for', '</s>', '</s>', '</s>']\n",
      "(3600 30%) 1.8266\n",
      "['I', 'started', 'started', 'to', 'notice', 'that', 'by', 'interesting', 'from', 'Matthew', '<unk>', '.', '</s>', '</s>', ',', '.', ',', 'radiation', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3700 30%) 1.5053\n",
      "['So', 'the', 'challenge', 'is', ',', 'how', 'are', 'we', 'going', 'to', 'get', 'these', 'and', 'reach', 'these', 'things', '?', '</s>', 'what', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3800 31%) 1.5403\n",
      "['I', '&apos;ve', 'spent', 'a', 'lot', 'of', 'time', 'in', 'in', 'the', 'last', 'two', 'years', '.', '</s>', 'here', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3900 32%) 1.5757\n",
      "['That', '&apos;s', 'the', 'question', '.', '</s>', '</s>', '.', ',', ',', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000 33%) 1.6121\n",
      "['Yeah', '&apos;re', 'Yeah', ',', 'it', '&apos;s', 'all', '.', 'the', 'way', '.', '</s>', '.', 'them', 'him', ',', ',', ',', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4100 34%) 1.6466\n",
      "['Well', ',', 'you', 'see', ',', 'is', 'just', 'one', 'preview', 'of', 'what', 'is', 'in', 'mind', 'for', 'us', 'as', '.', '</s>', 'often', 'a', ',', '.', '.', '</s>', '</s>']\n",
      "(4200 35%) 1.6660\n",
      "['The', 'had', 'a', 'Ph.D.', 'to', ',', 'It', 'was', 'about', 'way', 'in', 'the', 'rule', 'system', ',', 'because', 'the', 'only', 'model', 'government', 'declared', 'to', 'get', 'an', 'bomb', '</s>']\n",
      "(4300 35%) 1.6802\n",
      "['He', 'just', 'wants', 'to', 'go', 'to', 'the', 'front', 'of', '.', '</s>', ',', 'them', '.', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4400 36%) 1.7012\n",
      "['<unk>', '<unk>', 'consumption', 'rate', 'fell', '.', 'zero', '.', '</s>', 'again', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4500 37%) 1.7209\n",
      "['And', 'I', '&apos;m', 'say', 'you', ',', '&quot;', 'Why', 'are', 'you', 'go', 'to', '<unk>', '?', '</s>', '</s>', '</s>', '?', '?', '?', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4600 38%) 1.7403\n",
      "['We', 'can', 'plant', 'all', 'in', 'the', 'oceans', '.', '</s>', 'here', '.', '.', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4700 39%) 1.5591\n",
      "['You', 'can', 'agree', 'with', 'me', 'that', 'empty', 'doesn', '&apos;t', 'penetrate', 'through', 'the', '.', '</s>', 'on', '<unk>', 'air', 'radiation', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4800 40%) 1.4473\n",
      "['The', '<unk>', 'of', 'this', 'interaction', 'is', 'so', 'and', ',', 'so', 'I', 'need', 'a', 'to', 'four', ',', '.', 'my', 'of', 'me', '.', '</s>', '</s>', ',', ',', '</s>']\n",
      "(4900 40%) 1.4757\n",
      "['And', 'the', 'the', 'reason', 'you', 'have', 'two', 'main', 'you', 'when', 'of', 'one', 'is', 'from', 'when', 'when', 'see', ',', 'one', 'of', 'the', 'things', 'is', 'a', '<unk>', '</s>']\n",
      "(5000 41%) 1.5201\n",
      "['Singer', '/', '<unk>', '<unk>', 'Reader', 'performs', '&quot;', 'Kiteflyer', '&apos;s', 'Hill', ',', '&quot;', 'after', 'tender', 'look', 'at', 'and', 'the', 'time', 'place', '.', 'The', 'a', 'Dolby', 'on', '</s>']\n",
      "(5100 42%) 1.5387\n",
      "['They', 'they', 'allow', 'the', 'pollination', ',', 'the', '<unk>', 'that', 'absorb', 'and', 'butterflies', 'and', 'birds', 'platforms', 'be', 'together', 'into', 'the', 'circulation', '?', '</s>', ',', 'a', ',', '</s>']\n",
      "(5200 43%) 1.5734\n",
      "['While', 'some', 'of', 'the', 'properties', 'are', 'be', ',', ',', 'what', 'of', 'involved', ',', 'and', 'they', 'of', 'them', 'have', 'hardwired', 'into', 'their', 'brains', '.', '</s>', 'working', '</s>']\n",
      "(5300 44%) 1.5848\n",
      "['But', 'that', '&apos;s', 'not', 'mean', 'that', 'we', '&apos;re', 'abandon', 'up', 'the', 'in', 'the', 'pandemic', 'against', 'cancer', '.', '</s>', '?', '<unk>', ',', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5400 45%) 1.6278\n",
      "['Because', 'if', 'you', 'look', 'at', 'at', 'the', 'same', '&apos;s', 'shell', ',', 'it', '&apos;s', 'lots', 'of', 'little', 'bumps', 'on', 'the', 'limb', '.', '</s>', '.', 'a', ',', ',']\n",
      "(5500 45%) 1.6303\n",
      "['<unk>', 'is', 'the', 'is', 'a', 'great', 'fun', 'expression', 'of', 'beautiful', 'comedy', '.', '</s>', 'great', '<unk>', '.', 'history', ',', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5600 46%) 1.6492\n",
      "['And', 'to', 'make', 'it', 'with', 'them', 'as', 'order', 'to', 'foster', 'a', 'sustainable', 'way', '.', '</s>', '</s>', 'a', 'a', 'wealth', '<unk>', '<unk>', '<unk>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5700 47%) 1.6075\n",
      "['Do', 'we', 'have', 'a', 'flourishing', 'civil', 'society', ',', 'a', 'vigorous', 'rule', 'of', 'law', 'and', 'the', 'security', '?', '<unk>', ',', 'So', 'we', 'do', 'in', 'the', 'is', '</s>']\n",
      "(5800 48%) 1.3686\n",
      "['This', 'is', 'Harriet', '<unk>', '<unk>', '.', '</s>', '.', '<unk>', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5900 49%) 1.3876\n",
      "['In', 'be', 'a', 'game', 'that', 'the', 'days', ',', '&apos;re', 'to', 'draw', 'fun', 'imagination', 'to', 'tell', 'that', 'you', 'were', 'really', 'mad', '&quot;', '<unk>', '&quot;', '.', '&quot;', '</s>']\n",
      "(6000 50%) 1.4231\n",
      "['So', 'I', '&apos;m', 'going', 'to', 'show', 'to', 'you', '.', '</s>', 'here', 'you', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6100 50%) 1.4583\n",
      "['So', 'so', 'of', 'it', 'needle', 'as', 'a', 'child', '.', '</s>', '?', '<unk>', '.', ',', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6200 51%) 1.4745\n",
      "['We', 'can', '&apos;t', 'control', 'control', 'it', 'for', 'it', '&apos;re', 'subconscious', 'to', 'us', '.', '</s>', '</s>', '.', 'in', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6300 52%) 1.5101\n",
      "['What', 'Kepler', 'does', 'is', 'search', 'the', 'long', 'of', 'ways', ',', 'and', 'we', '&apos;ll', 'will', 'up', 'with', 'look', 'out', 'the', ',', 'confirm', 'as', 'planets', '.', '</s>', '</s>']\n",
      "(6400 53%) 1.5234\n",
      "['We', 'need', 'to', 'believe', 'in', 'ourselves', '.', '</s>', 'other', '<unk>', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6500 54%) 1.5532\n",
      "['People', 'have', 'a', 'pretty', '<unk>', 'talk', ',', 'they', 'will', 'think', 'say', ',', 'and', 'that', 'can', 'see', 'it', 'that', 'they', 'they', 'you', 'things', 'give', '&apos;ve', 'you', '</s>']\n",
      "(6600 55%) 1.5666\n",
      "['This', 'is', 'the', ',', 'the', 'first', 'cloned', 'horse', '.', '</s>', 'still', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6700 55%) 1.5987\n",
      "['How', 'do', 'I', 'have', 'three', 'three', 'problems', 'in', 'I', 'know', 'like', 'to', 'take', '?', '</s>', 'here', 'that', 'a', '</s>', '</s>', '</s>', '</s>', '.', '</s>', '</s>', '</s>']\n",
      "(6800 56%) 1.3450\n",
      "['But', 'they', 'detained', 'me', '.', '</s>', 'I', 'him', 'me', 'me', ',', ',', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6900 57%) 1.3202\n",
      "['So', 'let', 'this', 'creature', ',', ',', 'it', 'this', 'he', 'does', 'trying', 'to', 'do', 'is', 'take', 'up', 'a', 'piece', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(7000 58%) 1.3456\n",
      "['It', 'it', 'it', '&apos;s', 'about', '35', 'percent', '.', '</s>', 'here', '.', 'in', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(7100 59%) 1.3848\n",
      "['We', '&apos;ve', 'been', 'a', 'long', 'way', '.', 'We', '&apos;re', 'doing', 'a', '.', '</s>', 'doing', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(7200 60%) 1.4059\n",
      "['There', 'were', 'been', 'a', 'lot', 'of', 'talking', 'about', 'some', 'of', 'the', 'time', 'of', 'that', 'few', '.', '.', '</s>', '</s>', 'a', ',', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(7300 60%) 1.4215\n",
      "['We', 'don', '&apos;t', 'want', 'to', 'be', 'our', 'about', 'different', ',', 'but', 'we', 'want', 'to', 'be', 'and', 'other', 'and', 'need', 'each', 'other', '.', '</s>', 'other', 'demand', '.']\n",
      "(7400 61%) 1.4409\n",
      "['You', 'can', 'give', 'them', 'a', 'look', 'press', 'conversation', 'they', 'would', 'say', 'not', 'understand', ',', '</s>', ',', 'them', ',', 'them', ',', ',', '</s>', '</s>', 'for', 'for', '</s>']\n",
      "(7500 62%) 1.4740\n",
      "['And', 'the', 'goat', 'was', '99', '120', '.', '</s>', 'there', '.', '.', ',', ',', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(7600 63%) 1.4915\n",
      "['And', 'I', 'thought', ',', 'after', 'the', ',', ',', ',', 'the', 'is', 'the', 'the', '--', 'are', 'pray', '.', '</s>', 'the', 'the', 'us', 'us', '</s>', '.', '.', '.']\n",
      "(7700 64%) 1.5037\n",
      "['So', ',', 'the', 'middle', ',', 'you', 'see', 'the', 'about', 'the', 'sunflower', 'is', 'facing', ',', 'and', 'you', 'mark', 'it', 'on', 'the', '<unk>', 'area', 'in', 'the', 'middle', '</s>']\n",
      "(7800 65%) 1.4039\n",
      "['Temple', '<unk>', ':', 'The', 'world', 'needs', 'all', 'kinds', 'of', 'minds', '</s>', '</s>', 'the', '.', '<unk>', '<unk>', 'motorbike', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7900 65%) 1.2386\n",
      "['These', 'are', 'trials', 'in', 'the', 'field', 'of', '<unk>', '.', '</s>', '</s>', '.', ',', ',', ',', ',', '<unk>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(8000 66%) 1.2687\n",
      "['And', 'we', 'can', 'aggregate', 'them', 'together', 'and', 'make', 'these', 'thousands', 'and', 'thousands', 'of', 'animals', 'big', '<unk>', 'molecules', 'and', 'same', 'things', 'of', 'carbon', 'and', '<unk>', ',', '</s>']\n",
      "(8100 67%) 1.3018\n",
      "['And', 'they', 'people', 'who', 'said', ',', '&quot;', 'Okay', ',', 'I', '.', 'I', '&apos;ll', 'going', 'sure', 'at', '&quot;', ',', '&quot;', 'they', '&apos;re', 'going', 'as', 'long', 'efficacy', '</s>']\n",
      "(8200 68%) 1.3253\n",
      "['Five', 'I', 'moved', ',', 'to', 'the', ',', 'and', 'I', 'want', 'to', 'see', 'at', 'Japan', '.', '</s>', 'in', ',', ',', ',', 'water', 'water', 'water', 'water', '</s>', '</s>']\n",
      "(8300 69%) 1.3455\n",
      "['The', 'next', 'slide', 'is', 'going', 'to', 'be', '<unk>', '.', '</s>', 'here', ',', ',', ',', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(8400 70%) 1.3749\n",
      "['Am', 'I', 'a', 'one', 'that', 'you', 'choose', 'to', 'leave', 'by', 'your', 'son', 'or', 'your', 'brother', 'or', 'your', 'sister', 'or', 'your', 'brother', '&apos;s', 'your', 'grandparents', ',', '</s>']\n",
      "(8500 70%) 1.4114\n",
      "['And', 'I', 'think', 'that', 'science', 'research', ',', 'economists', 'enough', ',', 'really', 'been', '<unk>', 'what', 'help', 'to', 'save', 'study', 'studying', 'lung', 'illnesses', 'and', 'also', 'kidney', 'illnesses', '</s>']\n",
      "(8600 71%) 1.4192\n",
      "['That', '&apos;s', 'actually', 'just', '.', '</s>', '.', 'a', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(8700 72%) 1.4323\n",
      "['It', '&apos;s', 'like', 'like', 'the', 'worthy', 'up', 'and', 'sacrificed', 'in', 'the', 'name', 'of', 'the', 'god', 'of', '<unk>', ',', 'or', 'the', 'than', 'a', '<unk>', 'of', 'your', '</s>']\n",
      "(8800 73%) 1.4515\n",
      "['It', 'was', 'a', 'like', 'a', '<unk>', '<unk>', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '.']\n",
      "(8900 74%) 1.1676\n",
      "['<unk>', 'attractive', 'people', 'are', 'hot', '.', '</s>', '.', '.', ',', ',', ',', ',', 'for', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(9000 75%) 1.1921\n",
      "['The', 'problem', 'is', 'that', 'media', 'media', 'doesn', '&apos;t', 'know', 'it', '.', '</s>', 'them', 'them', ',', ',', ',', ',', ',', ',', '.', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(9100 75%) 1.2331\n",
      "['What', 'are', 'of', 'you', 'are', 'the', '<unk>', 'seen', '?', '</s>', '</s>', 'a', ',', ',', '</s>', '</s>', 'discovery', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(9200 76%) 1.2598\n",
      "['And', 'think', 'we', 'have', 'to', 'build', 'smart', 'homes', 'and', 'make', 'smart', 'stuff', 'in', 'it', '.', '</s>', ',', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>']\n",
      "(9300 77%) 1.2828\n",
      "['Also', ',', 'two', 'women', 'in', '<unk>', '<unk>', ':', 'Wangari', 'Maathai', ',', 'the', 'Nobel', '<unk>', 'from', 'North', 'who', '<unk>', 'planted', '<unk>', 'million', 'people', ',', 'By', 'by', '</s>']\n",
      "(9400 78%) 1.3114\n",
      "['We', 'did', 'a', 'lot', 'of', 'research', 'people', 'these', 'who', 'are', 'successful', 'and', 'we', 'on', 'to', 'lists', 'of', 'altered', 'components', 'that', '</s>', 'other', 'a', ',', ',', '</s>']\n",
      "(9500 79%) 1.3339\n",
      "['Six', 'hundred', 'million', 'million', 'people', 'were', 'lifted', 'out', 'of', 'poverty', '.', '</s>', 'still', ',', ',', ',', 'than', '</s>', '</s>', '.', '.', '<unk>', '<unk>', '</s>', '</s>', '</s>']\n",
      "(9600 80%) 1.3507\n",
      "['It', '&apos;s', 'actually', '--', 'I', 'never', 'not', 'not', 'just', 'the', 'the', 'official', 'trying', 'to', 'find', 'the', 'answer', 'answer', '.', '</s>', 'at', '<unk>', ',', ',', '</s>', '</s>']\n",
      "(9700 80%) 1.3580\n",
      "['Now', 'I', 'got', 'to', 'the', 'point', 'where', 'I', 'could', 'able', 'to', 'perceive', '360', 'colors', ',', 'particularly', 'like', 'the', 'vision', '.', '</s>', '</s>', ',', '--', '</s>', '</s>']\n",
      "(9800 81%) 1.3835\n",
      "['So', 'she', 'was', 'been', 'that', 'whole', 'thing', ',', 'but', 'she', '&apos;s', 'going', 'beautifully', 'to', 'her', 'life', 'life', '.', '</s>', '.', '<unk>', ',', ',', '</s>', '</s>', '</s>']\n",
      "(9900 82%) 1.2263\n",
      "['That', '&apos;s', 'me', 'at', 'the', 'desk', '.', '</s>', 'doing', '.', '.', '.', ',', ',', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(10000 83%) 1.1227\n",
      "['Third', 'step', 'is', 'cognitive', 'dissonance', '.', '</s>', 'learning', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10100 84%) 1.1724\n",
      "['There', '&apos;s', 'a', 'lot', 'of', 'people', 'work', 'out', 'there', 'doing', 'are', 'done', 'the', 'wrong', 'problem', '.', 'and', 'it', 'makes', 'a', 'a', 'problem', 'worse', '.', '</s>', 'correlation']\n",
      "(10200 85%) 1.1882\n",
      "['You', 'will', 'have', 'energy', 'vitality', 'and', 'energy', '.', '</s>', 'there', '<unk>', 'injuries', ',', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(10300 85%) 1.2139\n",
      "['You', 'can', 'buy', '100', 'Ph.D.', 'students', ',', 'this', 'umbrella', 'of', 'this', 'presentation', ',', 'because', 'I', '<unk>', 'stroke', 'tall', 'white', 'fungi', ',', 'which', 'to', '<unk>', ',', '</s>']\n",
      "(10400 86%) 1.2333\n",
      "['But', 'it', 'seems', 'like', 'we', 'didn', '&apos;t', 'have', 'a', 'other', '<unk>', '<unk>', 'having', 'margin', 'of', 'some', 'of', 'the', '<unk>', 'were', 'been', 'and', 'be', 'frequently', 'I', '</s>']\n",
      "(10500 87%) 1.2668\n",
      "['Because', 'it', 'is', 'like', 'a', 'national', 'issue', ',', 'it', 'it', '&apos;s', 'not', 'a', '</s>', 'poor', 'water', 'water', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>']\n",
      "(10600 88%) 1.2940\n",
      "['But', 'I', 'was', 'a', 'lot', '.', 'fun', '.', '</s>', 'at', '.', '.', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(10700 89%) 1.3137\n",
      "['In', 'fact', ',', 'the', 'United', 'Nations', 'estimates', 'is', 'there', '&apos;s', 'about', '85', 'billion', 'pounds', 'a', 'year', 'from', 'waste', 'waste', 'that', 'are', 'discarded', 'around', 'the', 'world', '</s>']\n",
      "(10800 90%) 1.3252\n",
      "['That', 'that', '&apos;s', 'what', 'we', 'found', 'seen', 'in', 'the', '.', 'and', 'that', 'is', 'the', 'the', 'economy', 'of', 'journalism', 'is', '.', '</s>', 'far', 'part', '</s>', '</s>', '</s>']\n",
      "(10900 90%) 1.2821\n",
      "['Right', '?', 'It', 'would', 'you', ',', 'for', 'right', ',', '</s>', ',', ',', ',', ',', ',', ',', '.', '.', ',', ',', ',', '.', '.', '.', '.', '</s>']\n",
      "(11000 91%) 1.0688\n",
      "['Now', 'I', 'I', 'worked', 'in', 'acute', 'care', '.', '</s>', 'working', 'the', 'surgery', 'Joe', 'medical', '</s>', '</s>', 'medical', 'medical', 'suicide', 'suicide', 'suicide', 'Emma', 'Emma', 'Emma', '</s>', '</s>']\n",
      "(11100 92%) 1.0958\n",
      "['They', 'are', 'also', 'available', 'in', 'of', 'three', 'different', 'types', ',', 'like', 'they', 'you', 'start', 'a', 'little', 'of', 'narrative', 'or', 'other', 'on', 'the', 'way', 'plain', '.', '</s>']\n",
      "(11200 93%) 1.1213\n",
      "['The', 'thing', 'that', 'can', 'happen', 'very', 'unlikely', 'very', 'important', 'very', 'important', 'is', 'sometimes', 'may', 'a', 'lot', 'of', 'people', 'to', 'have', 'have', 'been', 'from', 'the', 'on', '</s>']\n",
      "(11300 94%) 1.1500\n",
      "['Another', 'title', '<unk>', 'was', 'be', '<unk>', 'to', 'the', '<unk>', '&quot;', '<unk>', ',', '&quot;', 'which', 'has', 'there', 'a', 'Ben', 'of', 'the', 'year', 'before', 'the', 'Royal', 'Yorker', '</s>']\n",
      "(11400 95%) 1.1866\n",
      "['Do', 'you', 'blinding', '<unk>', 'by', 'and', 'the', 'on', 'the', 'other', '?', '</s>', 'the', 'someone', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '—']\n",
      "(11500 95%) 1.2033\n",
      "['If', 'If', '&apos;', 'philosophers', '&apos;', 'is', 'about', 'and', 'be', 'a', 'long', 'and', 'lofty', 'a', 'term', ',', '&quot;', 'this', 'said', ',', '&quot;', 'from', '.', 'with', 'the', '</s>']\n",
      "(11600 96%) 1.2277\n",
      "['And', 'everything', 'they', 'they', 'call', 'to', 'call', 'it', '<unk>', '.', '.', '</s>', '.', ',', '.', ',', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(11700 97%) 1.2548\n",
      "['They', 'do', 'these', 'doing', 'with', '?', '</s>', '?', '--', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11800 98%) 1.2629\n",
      "['But', '&apos;s', 'as', 'show', 'you', 'how', 'we', 'we', 'in', 'coming', 'at', 'the', 'last', '<unk>', '.', '</s>', 'we', 'us', ',', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(11900 99%) 1.2872\n",
      "['When', 'I', 'asked', 'him', 'what', 'the', 'the', 'solution', 'was', 'the', 'challenge', 'was', 'be', ',', 'it', 'answer', 'was', 'pretty', 'hard', '.', '</s>', 'other', '--', '.', '</s>', '</s>']\n",
      "(12000 100%) 1.0733\n",
      "['I', 'said', 'say', 'yes', ',', 'that', 'important', ',', 'to', 'a', 'same', 'reason', '.', '</s>', 'doing', 'a', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step24000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step24000_decoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100 0%) 1.0708\n",
      "['But', 'if', 'the', 'Bible', 'became', '<unk>', 'and', 'began', 'to', 'predict', 'for', 'the', 'sacrifice', ',', 'the', '<unk>', 'would', 'be', 'the', '<unk>', '.', '</s>', ',', '<unk>', ',', '</s>']\n",
      "(200 1%) 1.0999\n",
      "['Now', 'when', 'we', '&apos;re', 'cutting', '<unk>', 'sharks', 'shark', ',', 'what', 'was', 'what', 'we', 'could', 'them', 'on', 'the', 'light', 'and', 'the', '<unk>', '--', 'and', 'in', 'the', '</s>']\n",
      "(300 2%) 1.1246\n",
      "['This', 'is', 'part', 'of', 'our', 'collective', 'narrative', '.', '</s>', 'it', 'it', '.', 'them', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '</s>']\n",
      "(400 3%) 1.1459\n",
      "['The', 'acid', 'protect', 'us', '.', 'Oh', ',', 'all', 'of', 'fun', ',', '<unk>', '?', '</s>', ',', ',', ',', ',', ',', '.', '.', '.', '.', '.', '.', '.']\n",
      "(500 4%) 1.1730\n",
      "['And', 'I', 'would', 'it', 'all', 'the', '&apos;ll', 'that', 'kind', 'of', 'change', '.', '</s>', '</s>', ',', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(600 5%) 1.1818\n",
      "['And', 'it', 'walks', 'away', '.', '&quot;', ',', 'this', 'is', 'DARwIn', 'IV', '.', '</s>', '</s>', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(700 5%) 1.2050\n",
      "['People', 'do', 'everything', 'person', 'to', 'whatever', 'he', 'needs', 'to', 'or', 'others', 'single', 'of', 'people', 'other', 'person', '&apos;s', 'every', 'other', 'person', 'or', '</s>', '.', '.', '.', '.']\n",
      "(800 6%) 1.2170\n",
      "['If', 'you', 'have', 'a', 'final', 'worldview', 'of', 'today', ',', 'you', 'might', 'have', 'to', 'lot', 'to', 'understand', 'why', '&apos;s', 'going', 'next', ',', 'the', 'future', '.', '</s>', '</s>']\n",
      "(900 7%) 1.2317\n",
      "['This', '<unk>', 'is', 'very', 'in', 'the', 'encryption', 'as', 'a', 'kind', 'of', 'the', 'signature', '.', '</s>', 'we', '<unk>', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(1000 8%) 1.1350\n",
      "['What', 'find', 'looking', 'for', 'the', 'of', 'crosses', '<unk>', '.', '</s>', 'at', '<unk>', '.', ',', 'tube', 'tube', 'tube', 'tube', 'tube', 'tube', 'tube', 'tube', 'tube', 'tube', '</s>', '</s>']\n",
      "(1100 9%) 0.9836\n",
      "['And', 'the', '<unk>', 'scenario', ':', '&quot;', 'I', 'thought', 'this', 'was', 'was', 'was', 'something', 'to', 'happen', 'and', 'something', 'else', 'happened', '.', '.', '&apos;', 'And', 'these', 'thing', '</s>']\n",
      "(1200 10%) 1.0139\n",
      "['And', 'we', 'know', 'how', 'to', 'eradicate', 'poverty', ',', 'we', 'need', 'need', 'to', 'earn', 'more', '?', '</s>', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(1300 10%) 1.0442\n",
      "['I', 'was', 'my', 'man', 'of', 'a', 'guy', 'who', 'joined', 'an', 'club', 'in', 'the', 'life', ',', 'an', 'head', 'Club', '.', '</s>', ',', 'a', ',', ',', ',', '</s>']\n",
      "(1400 11%) 1.0568\n",
      "['The', 'the', 'words', 'of', 'the', '<unk>', '<unk>', ',', '&quot;', 'the', 'idea', 'voice', 'is', 'the', 'essence', 'of', 'the', 'soul', '.', '&quot;', '</s>', ',', '.', '.', '.', '.']\n",
      "(1500 12%) 1.0915\n",
      "['Without', 'the', 'of', 'corruption', ',', 'you', 'cannot', '&apos;t', 'have', 'the', 'national', 'to', 'join', 'the', '.', '</s>', 'with', '<unk>', ',', ',', ',', '</s>', '</s>', '</s>', 'moral', '</s>']\n",
      "(1600 13%) 1.1149\n",
      "['But', 'of', 'course', ',', 'the', 'illnesses', 'can', 'kill', 'up', 'a', 'ways', 'ways', 'than', 'well', '.', 'They', 'average', 'obvious', 'example', 'they', 'suicide', '.', '</s>', 'in', ',', '</s>']\n",
      "(1700 14%) 1.1349\n",
      "['It', '&apos;s', '&apos;s', 'not', 'just', 'us', 'we', 'have', 'the', 'imagine', 'of', 'imagine', 'the', ',', 'a', 'different', ',', 'but', 'possibility', 'that', 'there', ',', 'but', 'we', 'need', '</s>']\n",
      "(1800 15%) 1.1539\n",
      "['And', 'that', '&apos;s', 'a', 'system', 'where', 'a', 'high', 'levels', '.', '</s>', 'doing', '.', 'in', '.', '.', '.', '.', '.', '.', '.', 'GDP', 'GDP', 'GDP', 'GDP', '</s>']\n",
      "(1900 15%) 1.1759\n",
      "['She', 'said', ',', '&quot;', 'Did', 'he', 'mean', 'that', 'sitting', '?', '&quot;', '</s>', '</s>', '.', '.', ',', ',', '.', 'me', 'me', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2000 16%) 1.1985\n",
      "['And', 'at', 'that', 'time', ',', 'there', 'was', '<unk>', 'piles', 'seafarers', 'being', 'held', 'hostage', ',', 'and', 'some', 'of', 'them', 'were', 'held', 'hostage', 'for', 'me', 'because', 'of', '</s>']\n",
      "(2100 17%) 0.9360\n",
      "['This', 'could', 'be', 'a', 'difficult', 'to', 'grasp', 'now', 'than', 'we', 'use', 'the', 'word', '&quot;', 'awesome', '&quot;', 'to', 'describe', 'the', 'book', 'app', 'or', 'a', 'viral', 'video', '</s>']\n",
      "(2200 18%) 0.9636\n",
      "['And', 'by', 'the', 'way', ',', 'she', 'was', 'particularly', 'very', 'politically', 'to', 'completely', 'very', 'to', 'change', 'color', 'simplest', ',', 'a', 'important', 'component', 'for', 'this', 'demonstration', '.', '</s>']\n",
      "(2300 19%) 0.9910\n",
      "['<unk>', 'of', 'grizzly', 'bears', 'are', 'pretty', 'familiar', '.', '</s>', 'here', '.', '.', ',', ',', ',', 'for', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2400 20%) 1.0196\n",
      "['They', 'were', 'years', 'on', 'every', 'day', 'time', 'before', 'they', 'had', 'there', '.', '</s>', 'done', ',', 'before', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2500 20%) 1.0468\n",
      "['So', ',', 'husbands', 'start', 'beating', 'wives', ',', 'mothers', 'and', 'fathers', 'left', 'their', 'children', ',', 'and', 'afterward', ',', 'they', 'feel', 'high', '.', '</s>', ',', '—', ',', '</s>']\n",
      "(2600 21%) 1.0698\n",
      "['The', 'real', 'thing', 'here', 'is', 'that', 'you', 'can', '&apos;t', 'see', 'all', 'are', 'the', 'researchers', 'here', 'this', 'picture', '.', '</s>', 'solving', '<unk>', '--', '--', '.', '.', '.']\n",
      "(2700 22%) 1.0930\n",
      "['And', 'this', '&apos;s', 'completely', 'greater', 'of', '38', 'million', 'variable', 'positions', '.', '</s>', '</s>', '.', '.', '.', '.', 'growth', 'tube', 'tube', 'tube', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2800 23%) 1.1070\n",
      "['Remember', 'Chris', 'Brown', '&apos;s', 'video', '&quot;', '<unk>', '&quot;', '?', '</s>', '</s>', ',', ',', '<unk>', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(2900 24%) 1.1304\n",
      "['A', 'high', 'gun', 'at', 'a', 'paper', 'feet', '&apos;s', 'distance', '.', '</s>', 'else', ',', '.', ',', ',', ',', ',', '<unk>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(3000 25%) 1.1498\n",
      "['So', 'what', 'you', 'see', 'is', 'which', 'precipice', ',', 'that', 'big', 'precipice', 'with', 'the', 'valley', ',', 'is', 'the', '2008', 'financial', 'crisis', '.', '</s>', ',', 'a', ',', '</s>']\n",
      "(3100 25%) 1.0056\n",
      "['That', '&apos;s', 'more', 'to', 'reveal', 'some', 'couple', 'of', 'attention', 'slides', 'for', 'the', 'of', 'you', 'going', 'more', '&apos;ll', 'show', 'you', 'know', 'when', 'you', '&apos;re', 'see', 'and', '</s>']\n",
      "(3200 26%) 0.9203\n",
      "['Now', ',', '57', 'times', '68', 'is', '<unk>', ',', 'plus', '<unk>', 'is', '<unk>', ',', 'plus', '&apos;s', '<unk>', 'plus', '<unk>', ',', '<unk>', 'plus', '<unk>', 'is', '<unk>', '.', '</s>']\n",
      "(3300 27%) 0.9496\n",
      "['The', 'ability', 'savanna', 'landscape', 'is', 'one', 'of', 'the', 'clearest', 'examples', 'where', 'human', 'beings', 'everywhere', 'find', 'beauty', 'in', 'nature', 'than', 'mine', '.', '</s>', 'at', 'a', ',', '</s>']\n",
      "(3400 28%) 0.9720\n",
      "['You', 'want', 'that', 'by', 'changing', 'your', 'word', '&quot;', 'you', '.', '&quot;', '</s>', '.', '.', '.', 'it', '.', 'people', 'people', 'people', '</s>', '</s>', '</s>', 'you', 'you', 'you']\n",
      "(3500 29%) 1.0017\n",
      "['Everybody', 'people', 'and', 'behold', ',', 'people', 'started', 'uploading', 'their', 'videos', '.', '</s>', 'we', 'them', ',', ',', 'electronics', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'in', 'in', 'in']\n",
      "(3600 30%) 1.0133\n",
      "['And', '&apos;s', 'out', 'that', 'every', 'lot', 'of', 'times', 'ideas', 'have', 'been', 'important', 'me', 'periods', ',', 'I', '&apos;m', 'it', 'the', '&quot;', '<unk>', 'hunch', '.', '&quot;', '</s>', '</s>']\n",
      "(3700 30%) 1.0396\n",
      "['I', 'don', '&apos;t', 'do', 'to', 'do', 'serve', 'anymore', ',', 'only', 'documenting', '.', '</s>', 'doing', '.', '.', ',', 'radiation', 'radiation', 'radiation', 'electronics', '—', '—', 'visions', '</s>', '</s>']\n",
      "(3800 31%) 1.0594\n",
      "['Sometimes', 'long', 'had', 'to', 'be', 'it', 'in', 'a', 'submersible', 'because', 'quite', 'sometime', ',', 'because', 'the', 'images', 'of', 'of', 'that', 'animal', 'is', 'the', 'knot', ',', 'which', '</s>']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900 32%) 1.0933\n",
      "['One', 'is', '--', 'never', '.', '.', '.', 'Never', 'been', 'late', '.', '</s>', 'at', ',', ',', ',', '--', '—', '—', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4000 33%) 1.1049\n",
      "['I', 'call', 'it', 'the', '<unk>', 'rule', '.', '<unk>', 'rules', '.', '</s>', '</s>', '--', '—', '.', '—', '—', '<unk>', '<unk>', '<unk>', 'television', 'television', '</s>', '</s>', '</s>', '</s>']\n",
      "(4100 34%) 1.0658\n",
      "['He', '&apos;s', 'saying', 'to', 'me', ',', '&quot;', 'Electricity', '.', '&apos;', 'Was', 'he', 'an', '<unk>', '?', '&quot;', '&quot;', 'No', '.', '&quot;', '</s>', '.', '.', '.', '</s>', '</s>']\n",
      "(4200 35%) 0.8792\n",
      "['They', 'only', 'only', 'find', 'it', 'out', 'by', 'the', 'into', 'hands', 'on', 'it', 'and', 'playing', 'with', 'it', '.', '</s>', 'it', 'it', '.', '.', '.', '</s>', '</s>', '</s>']\n",
      "(4300 35%) 0.9018\n",
      "['So', 'they', 'missed', 'that', 'as', 'a', 'tool', ',', 'and', 'then', 'they', 'went', 'got', 'this', 'recruited', 'now', 'software', 'researchers', 'from', 'around', 'the', 'world', 'that', 'are', 'able', '</s>']\n",
      "(4400 36%) 0.9247\n",
      "['He', '&apos;s', 'probably', 'the', 'most', 'popular', 'social', 'entrepreneur', 'in', 'education', 'in', 'the', 'world', '.', '</s>', 'doing', 'people', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4500 37%) 0.9549\n",
      "['Now', ',', 'for', 'my', 'innocent', 'mind', ',', 'dead', 'implies', 'incapable', 'of', 'communicating', '.', '</s>', ',', 'me', '.', '--', ',', '</s>', '</s>', '</s>', '</s>', '</s>', 'recognition', 'recognition']\n",
      "(4600 38%) 0.9743\n",
      "['And', '&apos;s', 'what', 'turned', 'gone', '.', '</s>', 'far', '<unk>', 'injuries', ',', ',', ',', ',', ',', ',', ',', ',', 'my', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4700 39%) 1.0009\n",
      "['When', ',', 'when', 'I', 'took', 'care', 'of', 'my', '200', 'years', ',', 'I', 'realized', 'that', 'much', 'painful', '.', '</s>', 'working', 'you', 'numbers', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(4800 40%) 1.0174\n",
      "['I', 'don', '&apos;t', 'think', 'so', '.', '</s>', 'here', ',', ',', ',', ',', ',', 'electronics', 'electronics', 'electronics', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', 'hardware', 'hardware', '</s>']\n",
      "(4900 40%) 1.0455\n",
      "['Let', 'me', 'give', 'you', 'some', 'few', 'suggestions', 'to', 'get', 'what', '&apos;s', 'happening', '.', '</s>', 'doing', '<unk>', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(5000 41%) 1.0622\n",
      "['Are', 'they', 'have', 'a', 'resources', 'to', 'be', 'up', 'to', 'a', 'serious', 'pandemic', '?', '</s>', '</s>', '?', '?', '?', '?', '?', '?', '.', '</s>', '</s>', '</s>', '</s>']\n",
      "(5100 42%) 1.0760\n",
      "['It', '&apos;s', 'like', 'the', '&apos;s', 'them', 'good', 'boost', 'on', 'a', '<unk>', 'game', ',', 'or', 'they', 'they', '&apos;ve', 'in', 'a', 'of', 'letters', 'into', 'their', 'car', 'and', '</s>']\n",
      "(5200 43%) 0.8896\n",
      "['It', 'is', 'the', '<unk>', 'called', '<unk>', '<unk>', '.', '</s>', '</s>', ',', ',', ',', '<unk>', '<unk>', '</s>', '<unk>', '<unk>', 'deficit', 'deficit', '<unk>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5300 44%) 0.8551\n",
      "['<unk>', 'improvement', '.', '</s>', '.', '.', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'recognition', 'recognition', 'recognition', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5400 45%) 0.8801\n",
      "['But', 'look', 'what', 'happened', 'in', 'the', 'after', 'the', 'plague', ':', '<unk>', 'wages', ',', 'land', 'reform', ',', 'which', 'innovation', ',', 'birth', 'of', 'the', 'larger', 'of', ';', '</s>']\n",
      "(5500 45%) 0.9045\n",
      "['And', 'they', 'get', 'much', 'more', 'areas', '.', '</s>', '.', '<unk>', '.', '.', ',', ',', ',', ',', ',', ',', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(5600 46%) 0.9369\n",
      "['It', 'is', 'expected', 'to', 'double', ',', 'triple', 'or', 'maybe', '<unk>', 'years', '.', 'time', 'for', 'the', 'foreseeable', 'future', '.', '</s>', 'the', 'four', ',', ',', ',', ',', '</s>']\n",
      "(5700 47%) 0.9623\n",
      "['And', 'maybe', 'maybe', 'it', 'can', 'do', 'is', 'manipulate', 'them', 'with', 'both', 'of', 'my', 'fingers', 'at', 'the', 'same', 'time', '.', '</s>', '.', ',', ',', '.', '.', '.']\n",
      "(5800 48%) 0.9894\n",
      "['That', '&apos;s', 'a', 'clearly', '.', '</s>', 'working', '<unk>', '.', '.', ',', ',', ',', '.', '.', ',', ',', '.', '.', 'intensity', 'intensity', 'intensity', 'intensity', '.', '.', '.']\n",
      "(5900 49%) 0.9983\n",
      "['And', 'instead', 'of', 'Sony', 'blocking', ',', 'they', 'allowed', 'the', '<unk>', 'to', 'occur', '.', '</s>', 'doing', '<unk>', '<unk>', '<unk>', '.', '</s>', '</s>', '</s>', '<unk>', '<unk>', '<unk>', 'suicide']\n",
      "(6000 50%) 1.0238\n",
      "['It', 'is', 'a', 'writing', 'system', '.', '</s>', 'alone', 'English', 'from', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '</s>', '</s>', '</s>', '</s>', '</s>', ',', ',']\n",
      "(6100 50%) 1.0402\n",
      "['And', 'have', 'a', 'newspaper', '<unk>', '.', '</s>', '.', ',', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6200 51%) 0.9561\n",
      "['They', 'gave', 'for', 'me', 'the', 'biggest', 'LED', 'in', 'the', 'world', 'at', 'for', 'TEDx', 'in', 'Amsterdam', '.', '</s>', '.', '<unk>', '.', '.', '<unk>', '.', '.', '.', '</s>']\n",
      "(6300 52%) 0.8261\n",
      "['That', '&apos;s', 'my', 'journey', '.', '</s>', 'there', ',', '.', ',', ',', ',', '<unk>', '<unk>', '<unk>', '<unk>', ',', ',', 'tube', 'tube', '</s>', 'Emma', 'Emma', 'Emma', '.', '.']\n",
      "(6400 53%) 0.8390\n",
      "['And', 'you', 'weigh', 'these', 'options', 'out', '--', 'you', 'simply', 'that', 'simple', 'cost-benefit', 'analysis', ',', 'and', 'you', 'decide', 'whether', 'it', '&apos;s', 'worthwhile', 'to', 'commit', 'the', 'crime', '</s>']\n",
      "(6500 54%) 0.8684\n",
      "['Daniel', 'didn', '&apos;t', 'get', 'any', 'about', 'the', 'protests', 'in', 'Egypt', 'at', 'all', 'in', 'history', 'first', 'page', 'of', 'Google', 'results', '.', '</s>', 'interesting', '<unk>', ',', ',', '</s>']\n",
      "(6600 55%) 0.8914\n",
      "['The', 'big', 'trials', 'are', 'much', 'to', 'the', 'challenge', 'answer', '.', '</s>', 'here', 'them', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '<unk>', 'mindset', '</s>', 'background', 'background']\n",
      "(6700 55%) 0.9206\n",
      "['There', '&apos;s', 'only', 'one', 'way', 'to', 'do', 'calculating', 'and', 'it', 'was', 'by', 'hand', '.', '</s>', 'with', ',', ',', ',', ',', 'electronics', 'electronics', 'electronics', '<unk>', ',', ',']\n",
      "(6800 56%) 0.9379\n",
      "['Angeline', 'wants', 'to', 'be', 'a', '<unk>', 'could', 'she', 'can', 'fly', 'around', 'the', 'world', 'and', 'make', 'a', 'difference', '.', '</s>', 'different', 'the', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(6900 57%) 0.9580\n",
      "['Even', 'people', 'are', 'playing', 'this', 'for', 'terminal', 'diagnoses', 'like', '<unk>', ',', '</s>', 'there', '<unk>', 'this', ',', ',', ',', '--', '.', '.', '.', '.', '.', '.', '.']\n",
      "(7000 58%) 0.9773\n",
      "['And', 'you', 'think', 'of', 'a', 'Internet', 'as', 'a', '<unk>', 'of', 'system', 'system', '.', '</s>', '</s>', ',', '<unk>', '<unk>', '<unk>', '<unk>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(7100 59%) 0.9989\n",
      "['The', 'end', 'coming', 'went', 'in', 'the', '1960s', 'to', 'up', 'way', 'for', 'all', 'the', 'improvements', '<unk>', 'that', 'we', 'to', 'come', 'out', '</s>', 'we', 'the', ',', ',', '</s>']\n",
      "(7200 60%) 1.0187\n",
      "['Some', 'of', 'the', 'classes', 'and', 'fewer', 'have', 'reported', 'higher', 'solutions', 'scores', ',', 'many', 'much', 'awful', 'level', 'with', 'the', 'rise', ',', 'especially', 'with', 'the', 'other', ',', '</s>']\n",
      "(7300 60%) 0.7909\n",
      "['You', 'know', ',', 'you', 'are', 'been', '1.2', 'other', 'parts', 'of', 'the', 'brain', 'that', 'seem', 'to', 'have', 'different', 'kinds', '.', '</s>', 'other', 'them', ',', ',', 'demand', '<unk>']\n",
      "(7400 61%) 0.8151\n",
      "['<unk>', 'reefs', 'are', 'growing', 'slower', 'in', 'some', 'places', 'because', 'of', 'this', '.', '.', '</s>', 'still', 'the', 'in', '.', '.', '.', 'intensity', 'level', 'than', '.', '.', '.']\n",
      "(7500 62%) 0.8261\n",
      "['That', '&apos;s', 'place', ',', ',', 'in', 'the', 'earthquake', 'in', '1954', ',', 'this', 'marine', '<unk>', 'grew', 'lifted', 'up', 'out', 'of', 'the', 'ocean', 'with', 'quickly', ',', 'and', '</s>']\n",
      "(7600 63%) 0.8499\n",
      "['You', 'want', 'to', 'want', '.', 'You', 'want', 'to', 'fix', '.', '</s>', '.', ',', '.', '.', '.', '.', '</s>', '.', '.', '.', '.', '.', '</s>', '.', '.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7700 64%) 0.8708\n",
      "['So', 'in', 'this', 'world', '<unk>', 'world', ',', 'we', 'might', 'be', 'us', 'that', 'us', 'to', 'be', 'reasonably', 'in', 'more', 'bit', 'more', 'explicit', 'about', 'the', 'moral', 'judgments', '</s>']\n",
      "(7800 65%) 0.9038\n",
      "['And', 'the', 'question', 'question', 'about', '<unk>', 'was', ':', 'Did', 'she', 'marry', '?', '</s>', '</s>', ':', 'Emma', 'me', 'me', 'my', 'my', 'Emma', 'my', 'my', 'my', 'my', 'my']\n",
      "(7900 65%) 0.9281\n",
      "['In', 'the', '1920s', ',', 'the', 'average', '<unk>', '<unk>', 'and', 'French', 'elite', '<unk>', 'were', '<unk>', 'same', 'dangers', 'road', '.', '</s>', ',', 'your', ',', ',', ',', ',', '</s>']\n",
      "(8000 66%) 0.9421\n",
      "['It', 'can', 'transformational', '.', '</s>', 'changes', '<unk>', ',', ',', ',', ',', ',', ',', ',', ',', 'recognition', 'recognition', 'recognition', 'recognition', 'recognition', 'recognition', 'recognition', 'recognition', 'recognition', 'recognition', 'recognition']\n",
      "(8100 67%) 0.9671\n",
      "['Steven', '<unk>', ':', 'A', 'universal', 'translator', 'for', 'surgeons', '</s>', '</s>', 'the', '.', '.', '—', 'hands', 'hands', 'speech', 'speech', 'speech', '</s>', '</s>', '—', '</s>', '</s>', '.', '.']\n",
      "(8200 68%) 0.9835\n",
      "['So', 'in', 'morning', ',', 'in', 'my', 'limited', 'time', ',', 'I', 'wanted', 'to', 'show', 'the', 'physics', 'out', 'of', 'a', 'lot', 'of', 'the', 'by', 'putting', 'you', 'to', '</s>']\n",
      "(8300 69%) 0.8571\n",
      "['And', 'the', 'reason', 'for', 'this', 'is', 'that', 'pigs', 'are', 'actually', 'quite', 'close', 'to', 'human', 'beings', '.', 'just', 'the', 'collagen', 'is', 'as', 'well', '.', '</s>', '</s>', ',']\n",
      "(8400 70%) 0.7716\n",
      "['So', 'when', 'I', 'go', 'around', 'the', 'world', ',', 'I', 'am', 'going', 'predictions', 'of', 'what', 'I', 'will', 'look', ',', 'except', 'them', 'away', '.', '</s>', ',', 'the', '</s>']\n",
      "(8500 70%) 0.7971\n",
      "['And', 'there', '&apos;s', 'more', 'other', 'more', 'people', 'too', 'the', 'other', '.', '</s>', '?', '</s>', '</s>', '--', '</s>', 'mind', '</s>', '</s>', '</s>', 'art', 'art', '</s>', '</s>', '</s>']\n",
      "(8600 71%) 0.8099\n",
      "['Here', 'is', 'why', 'we', 'spend', 'bonuses', 'to', 'bankers', 'and', 'pay', 'in', 'all', 'kinds', 'of', 'ways', '.', '</s>', 'you', 'them', ',', 'us', 'us', 'our', 'awards', 'demand', '</s>']\n",
      "(8700 72%) 0.8443\n",
      "['<unk>', ':', 'me', '--', 'piece', 'since', 'you', 'have', 'it', 'here', '.', '</s>', 'bother', 'it', 'this', ',', ',', ',', ',', '</s>', '</s>', 'history', 'history', 'history', '<unk>', '<unk>']\n",
      "(8800 73%) 0.8606\n",
      "['Does', 'your', 'brain', '<unk>', 'to', 'send', 'the', '<unk>', 'in', 'that', '?', '</s>', 'it', 'a', '?', '?', '?', '?', '?', '</s>', '</s>', '?', '?', 'you', 'you', 'consciousness']\n",
      "(8900 74%) 0.8894\n",
      "['And', 'you', 'might', 'see', 'that', 'it', '&apos;re', 'synchronized', '.', '</s>', 'working', '<unk>', '.', ',', ',', ',', '--', ',', ',', ',', ',', ',', 'people', 'people', 'people', 'people']\n",
      "(9000 75%) 0.9055\n",
      "['So', 'we', '&apos;re', 'able', 'of', 'skeptical', 'that', 'maybe', 'this', 'was', 'a', 'company', 'oil', 'that', 'just', 'sort', 'of', 'forced', 'the', 'secret', 'around', '.', '</s>', '</s>', '--', '</s>']\n",
      "(9100 75%) 0.9263\n",
      "['From', 'that', 'age', 'I', 'I', 'was', 'terrified', 'of', 'swimming', '.', '</s>', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'my', 'my', 'my', 'my', 'my', 'my']\n",
      "(9200 76%) 0.9608\n",
      "['In', 'a', ',', 'again', 'maybe', ',', 'it', ',', 'it', '&apos;s', 'because', 'we', '&apos;m', 'a', 'tech', 'entrepreneur', ',', 'I', 'look', 'at', 'the', 'like', 'being', '.', '</s>', '</s>']\n",
      "(9300 77%) 0.9198\n",
      "['That', '&apos;s', 'the', '<unk>', 'being', '<unk>', 'onto', 'paper', ',', 'basically', '.', 'It', '&apos;s', 'round', 'last', 'running', '.', '</s>', 'here', ',', ',', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(9400 78%) 0.7481\n",
      "['We', 'we', 'wanted', 'to', 'understand', 'whether', 'could', 'we', 'change', 'this', '?', '</s>', '?', ',', ',', ',', '?', '</s>', '</s>', '?', 'space', ',', 'people', 'people', 'people', 'knowledge']\n",
      "(9500 79%) 0.7595\n",
      "['They', 'use', 'political', 'violence', 'to', '<unk>', ',', 'physical', 'violence', 'to', '<unk>', 'and', '<unk>', 'or', 'emotional', 'violence', 'to', '<unk>', '.', '</s>', 'unhappy', ',', ',', ',', ',', '</s>']\n",
      "(9600 80%) 0.7812\n",
      "['And', '&apos;s', 'though', 'animal', 'finds', 'a', 'mine', '.', '</s>', '</s>', '<unk>', ',', ',', 'blood', 'blood', '.', 'blood', '--', '--', '--', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '.']\n",
      "(9700 80%) 0.7934\n",
      "['And', 'it', '&apos;s', 'not', 'a', 'solid', 'building', ',', 'but', 'I', 'have', 'it', 'a', ',', 'and', 'we', '&apos;d', 'have', 'a', 'living', 'relationship', 'between', 'us', 'that', 'could', '</s>']\n",
      "(9800 81%) 0.8337\n",
      "['There', 'may', 'be', 'ruins', 'intimacy', '–', 'I', 'happen', 'to', 'think', 'and', 'healing', 'energy', 'and', 'erotic', 'energy', 'are', 'actually', 'the', 'forms', 'of', 'the', 'same', 'thing', '.', '</s>']\n",
      "(9900 82%) 0.8537\n",
      "['It', 'would', 'have', 'the', '<unk>', 'of', 'the', 'town', 'square', '.', '</s>', 'still', '.', '.', ',', '<unk>', '<unk>', '<unk>', '<unk>', 'Europe', 'Europe', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
      "(10000 83%) 0.8786\n",
      "['In', 'the', 'majority', 'of', 'bubble', 'universes', ',', 'the', 'Higgs', 'mass', 'is', 'be', 'limited', 'the', 'temperature', 'value', ',', 'like', 'to', 'a', 'cosmic', 'collapse', 'of', 'the', 'Higgs', '</s>']\n",
      "(10100 84%) 0.8973\n",
      "['But', 'I', 'I', 'started', 'to', 'look', 'at', 'again', '.', '</s>', ',', '<unk>', '.', ',', ',', ',', ',', '</s>', ',', 'intensity', '<unk>', '<unk>', '<unk>', '<unk>', 'hardware', 'hardware']\n",
      "(10200 85%) 0.9174\n",
      "['So', 'there', 'there', 'are', 'nowhere', 'far', 'time', 'of', 'very', 'signs', 'and', 'backwards', 'signs', '.', '</s>', '.', '.', '.', '.', '.', '.', '.', '.', '</s>', '.', '.']\n",
      "(10300 85%) 0.9281\n",
      "['And', 'somebody', 'someone', 'referred', 'me', 'to', 'a', '<unk>', 'community', 'clinic', '.', '</s>', '.', '.', ',', ',', ',', 'people', '</s>', 'suicide', '</s>', 'suicide', 'electronics', 'my', 'my', 'hardware']\n",
      "(10400 86%) 0.7616\n",
      "['My', 'mom', 'was', 'a', '<unk>', 'mom', 'who', 'sat', 'out', 'the', 'dinner', 'company', 'in', 'the', 'day', 'and', 'sold', 'a', 'on', 'night', ',', 'that', 'I', 'wasn', 'write', '</s>']\n",
      "(10500 87%) 0.7266\n",
      "['And', 'see', ',', 'I', 'bet', 'you', 'are', '.', '.', '</s>', 'doing', 'you', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "(10600 88%) 0.7543\n",
      "['And', 'we', 'did', 'video', 'clips', '--', 'you', 'can', 'see', 'it', 'if', 'you', 'can', 'to', 'YouTube', '/', '<unk>', '.', '</s>', '</s>', ',', ',', '</s>', '</s>', '</s>', '</s>']\n",
      "(10700 89%) 0.7789\n",
      "['And', 'I', 'try', 'to', 'figure', 'out', 'what', 'was', 'they', 'thinking', '.', '</s>', 'here', ',', '.', ',', ',', ',', '.', '.', '.', '.', '.', '.', ',', ',']\n",
      "(10800 90%) 0.7942\n",
      "['If', 'you', 'friends', 'have', 'a', 'reciprocal', 'transaction', ',', 'like', 'selling', 'a', 'car', ',', 'it', '&apos;s', 'much', 'known', 'that', 'we', 'can', 'be', 'a', 'source', 'of', 'waste', '</s>']\n",
      "(10900 90%) 0.8122\n",
      "['And', 'when', 'the', 'put', 'their', 'together', ',', 'it', 'incredible', 'thing', ':', 'something', 'incredible', 'incredible', 'happens', ',', 'and', 'all', 'of', 'this', 'sudden', 'I', 'still', 'a', '<unk>', '</s>']\n",
      "(11000 91%) 0.8444\n",
      "['Tinkering', 'School', 'is', 'a', 'place', 'where', 'kids', '<unk>', 'pick', 'up', 'and', 'and', '<unk>', 'and', 'other', '<unk>', 'objects', ',', 'and', 'can', 'trusted', '.', '</s>', 'else', ':', '</s>']\n",
      "(11100 92%) 0.8611\n",
      "['And', 'we', 'should', 'be', 'developing', 'things', ',', 'we', 'should', 'be', 'developing', 'packaging', 'for', 'ideas', 'and', 'elevate', 'people', '&apos;s', 'perceptions', 'and', 'information', 'for', 'the', 'things', 'that', '</s>']\n",
      "(11200 93%) 0.8878\n",
      "['So', 'there', 'were', 'a', 'little', 'of', 'inmates', 'in', 'a', 'prison', 'in', '<unk>', '.', '</s>', '</s>', ',', ',', ',', ',', ',', '</s>', '</s>', '</s>', '--', '--', '</s>']\n",
      "(11300 94%) 0.9005\n",
      "['And', 'was', 'also', 'a', 'shift', 'question', 'of', 'ambiguity', '.', '</s>', '</s>', '--', '?', '?', '?', '?', '?', '?', '.', '.', '.', '</s>', '</s>', '</s>', '</s>', '</s>']\n",
      "(11400 95%) 0.8308\n",
      "['And', 'then', 'you', 'shake', 'it', ',', 'and', 'it', 'starts', 'to', 'error', '<unk>', 'and', 'built', 'it', 'structure', 'on', 'its', 'own', '.', '</s>', 'using', 'it', 'structure', 'structure', ',']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11500 95%) 0.7105\n",
      "['And', 'it', 'can', 'break', 'drive', 'them', 'to', 'the', 'garbage', 'cans', 'of', 'the', 'cell', 'and', 'recycle', 'them', 'if', 'they', '&apos;re', 'damaged', '.', '</s>', '.', '.', ',', '.']\n",
      "(11600 96%) 0.7314\n",
      "['But', 'it', '&apos;s', 'also', 'a', 'big', '.', 'There', '&apos;s', 'a', 'lot', 'of', 'work', 'involved', 'went', 'into', 'that', 'after', 'that', '.', '</s>', '</s>', '</s>', '.', '</s>', '</s>']\n",
      "(11700 97%) 0.7445\n",
      "['Well', 'the', 'range', 'is', 'quite', 'a', 'bit', '.', '</s>', 'there', 'they', '.', 'them', 'themselves', 'themselves', ',', ',', ',', ',', ',', ',', ',', 'mine', '</s>', '</s>', '</s>']\n",
      "(11800 98%) 0.7642\n",
      "['Architecture', 'is', 'not', 'based', 'on', 'concrete', 'and', '<unk>', 'and', 'the', 'surface', 'of', 'the', 'soil', '.', '</s>', '</s>', 'them', '<unk>', '.', '.', '.', '.', '<unk>', '.', '.']\n",
      "(11900 99%) 0.7909\n",
      "['Alan', 'Russell', 'studies', 'regenerative', 'medicine', '--', 'a', 'colony', 'which', 'of', 'thinking', 'about', 'autism', 'and', 'injury', ',', 'using', 'a', 'drug', 'that', 'can', '<unk>', 'the', 'body', 'to', '</s>']\n",
      "(12000 100%) 0.8106\n",
      "['You', 'know', ',', 'these', 'are', 'two', 'you', 'we', '&apos;ve', 'days', 'physiological', 'reasons', 'to', 'expect', 'to', 'be', '<unk>', 'caused', 'by', 'suggesting', 'reasoning', '.', '</s>', 'other', ':', '</s>']\n"
     ]
    }
   ],
   "source": [
    "trainIters(batch_generator1, encoder1, decoder1, 12000, print_every=100, learning_rate = 0.001)\n",
    "torch.save(encoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step36000_encoder\"))\n",
    "torch.save(decoder1.state_dict(), os.path.join(\"model_ckpt\",\"model_maxlen25_dotatten_lr0.001_step36000_decoder\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
